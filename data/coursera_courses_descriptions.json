{"course_slug": ["machine-learning", "neural-networks-deep-learning", "what-is-datascience", "python-data-analysis", "convolutional-neural-networks", "deep-neural-network", "python-for-applied-data-science-ai", "python-data", "nlp-sequence-models", "sql-for-data-science", "excel-essentials", "machine-learning-with-python", "r-programming", "sql-data-science", "linear-algebra-machine-learning", "open-source-tools-for-data-science", "data-analysis-with-python", "aws-machine-learning", "machine-learning-projects", "data-scientists-tools", "gcp-big-data-ml-fundamentals", "data-science-methodology", "python-databases", "python-machine-learning", "probability-intro", "basic-statistics", "python-for-data-visualization", "natural-language-processing-tensorflow", "excel-intermediate-1", "introduction-to-ai", "convolutional-neural-networks-tensorflow", "wharton-customer-analytics", "tensorflow-sequences-time-series-and-prediction", "competitive-data-science", "data-structures", "ml-foundations", "fundamentals-of-reinforcement-learning", "decision-making", "data-visualization-tableau", "google-machine-learning", "python-plotting", "excel-data-analysis", "language-processing", "applied-data-science-capstone", "big-data-introduction", "intro-to-deep-learning", "uva-darden-market-analytics", "datasciencemathskills", "analytics-excel", "wharton-quantitative-modeling", "python-statistics-financial-analysis", "python-data-visualization", "clinical-research", "analytics-business-metrics", "excel-analysis", "search-engine-optimization", "bayesian-statistics", "data-cleaning", "python-text-mining", "excel-intermediate-2", "erasmus-econometrics", "understanding-visualization-data", "analytics-tableau", "sas-programming-basics", "practical-time-series-analysis", "bayesian-methods-in-machine-learning", "inferential-statistics-intro", "leveraging-unstructured-data-dataproc-gcp", "database-management", "statistical-inference", "ml-regression", "launching-machine-learning", "big-data-essentials", "wharton-people-analytics", "data-science-course", "excel-advanced", "computational-neuroscience", "probabilistic-graphical-models", "exploratory-data-analysis", "analytics-mysql", "data-analytics-business", "end-to-end-ml-tensorflow-gcp", "practical-machine-learning", "powerpoint-presentations", "advanced-excel", "serverless-machine-learning-gcp", "pca-machine-learning", "wharton-operations-analytics", "ai-with-ibm-watson", "regression-models", "gcp-exploring-preparing-data-bigquery", "process-mining", "clinical-data-management", "intro-tensorflow", "ml-classification", "python-programming", "unix", "hadoop", "datavisualization", "google-machine-learning-jp", "ds", "practical-rl", "forensic-accounting", "sample-based-learning-methods", "big-data-machine-learning", "dataviz-design", "statistical-inferences", "python-social-network-analysis", "ai", "building-resilient-streaming-systems-gcp", "introduction-statistics-data-analysis-public-health", "accounting-analytics", "linear-regression-model", "introduction-genomics", "foundations-big-data-analysis-sql", "wharton-introduction-spreadsheets-models", "machine-learning-duke", "computer-vision-basics", "data-analytics-for-lean-six-sigma", "guided-tour-machine-learning-finance", "summary-statistics", "big-data-graph-analytics", "digital-analytics", "qualitative-methods", "machine-learning-business-professionals", "bayesian", "mcmc-bayesian-statistics", "applying-data-analytics-business-in-finance", "big-data-management", "crash-course-in-causality", "advanced-machine-learning-signal-processing", "descriptive-statistics-statistical-distributions-business-application", "conversational-experiences-dialogflow", "probability-statistics", "social-economic-networks", "julia-programming", "ml-clustering-and-retrieval", "introduction-gis-mapping", "reproducible-research", "intro-business-analytics", "introductiontoprobability", "feature-engineering", "recommender-systems-introduction", "introduction-computer-vision-watson-opencv", "dataviz-dashboards", "inferential-statistical-analysis-python", "r-programming-environment", "dataviz-visual-analytics", "data-products", "text-retrieval", "machine-learning-big-data-apache-spark", "dwdesign", "functional-mri", "iiot-google-cloud-platform", "computervision-imageprocessing", "sas-statistics", "text-mining", "big-data-analysis", "wharton-risk-models", "python-genomics", "managing-data-analysis", "art-science-ml", "data-driven-astronomy", "big-data-integration-processing", "launching-machine-learning-jp", "building-ai-applications", "importance-of-listening", "data-wrangling-analysis-abtesting", "introduction-to-deep-learning-with-keras", "people-analytics", "supply-chain-analytics-essentials", "basic-data-processing-visualization-python", "inferential-statistics", "cloudera-big-data-analysis-sql-queries", "build-data-science-team", "sas-programming-advanced", "fundamentals-machine-learning-in-finance", "social-media-data-analytics", "introduction-clinical-data-science", "strategic-business-analytics", "wharton-decision-making-scenarios", "dna-sequencing", "fitting-statistical-models-data-python", "digital-manufacturing-design", "experimentation", "spatial-data-science", "reinforcement-learning-in-finance", "galaxy-project", "business-intelligence-tools", "deep-learning-business", "the-socio-technical-health-informatics-context", "datascimed", "business-analytics-decision-making", "python-analysis", "image-understanding-tensorflow-gcp", "predictive-modeling-analytics", "data-science-project", "linear-regression-business-statistics", "gcp-creating-bigquery-datasets-visualizing-insights", "statistical-genomics", "measuring-disease-epidemiology", "data-visualization", "responsible-data-analysis", "dataviz-project", "real-life-data-science", "hypothesis-testing-confidence-intervals", "business-data", "data-manipulation", "advanced-r", "genomic-tools", "mbse", "data-science-k-means-clustering-python", "causal-inference", "intro-tensorflow-es", "probabilistic-graphical-models-2-inference", "gcp-big-data-ml-fundamentals-jp", "predictive-analytics-data-mining", "python-representation", "real-time-streaming-big-data", "information-visualization-programming-d3js", "survival-analysis-r-public-health", "data-patterns", "information-theory", "excel-essentials-ar", "gcp-advanced-insights-bigquery", "enterprise-systems", "introduction-portfolio-construction-python", "data-management", "advanced-data-science-capstone", "designexperiments", "sas-programming-certification-review", "sequence-models-tensorflow-gcp", "intro-accounting-data-analytics-visual", "gcp-production-ml-systems", "linear-regression-r-public-health", "intro-practical-deep-learning", "hypothesis-testing-public-health", "machine-learning-sas", "recommendation-models-gcp", "cluster-analysis", "gis-data-acquisition-map-design", "data-in-database", "machine-learning-h2o", "data-insights-gcp-apply-ml", "data-collection-framework", "data-science-ethics", "data-analytics-accountancy-1", "python-data-processing", "mathematics-sport", "gis-1", "code-free-data-science", "mri-fundamentals", "geospatial", "deploying-machine-learning-models", "business-analytics", "bioconductor", "data-modeling-regression-analysis-business", "logistic-regression-r-public-health", "healthcare-data-literacy", "probabilistic-graphical-models-3-learning", "spatial-analysis-satellite-imagery-in-a-gis", "relational-database", "neurohacking", "clinical-data-models-and-data-quality-assessments", "meaningful-marketing-insights", "wharton-capstone-analytics", "prediction-control-function-approximation", "simple-regression-analysis-public-health", "business-analytics-executive-overview", "uva-darden-understanding-data-tools", "demand-analytics", "cloud-storage-big-data-analysis-sql", "wharton-business-financial-modeling-capstone", "intelligent-machining", "linear-models", "data-analysis-project-pwc", "applying-data-analytics-business-in-marketing", "material-informatics", "machine-learning-data-analysis", "stereovision-motion-tracking", "predictive-analytics", "advanced-methods-reinforcement-learning-finance", "python-visualization", "gis-capstone", "matrix-factorization", "mongodb-aggregation-framework", "multiple-regression-analysis-public-health", "data-visualization-science-communication", "gcp-big-data-ml-fundamentals-es", "collaborative-filtering", "the-data-science-of-health-informatics", "mobile-health-monitoring-systems", "study-designs-epidemiology", "network-biology", "machine-learning-applications-big-data", "data-analysis-tools", "business-intelligence-data-warehousing", "nosql-databases", "design-thinking-predictive-analytics-data-products", "automated-reasoning-sat", "dwrelational", "regression-modeling-practice", "clinical-research-biostatistics-wolfram", "matrix-methods", "web-data", "scala-capstone", "validity-bias-epidemiology", "advanced-manufacturing-enterprise", "health-informatics-professional", "healthcare-data", "case-studies-business-analytics-accenture", "recommender-metrics", "python-machine-learning-for-investment-management", "ecology", "supply-chain-analytics", "formal-concept-analysis", "text-mining-analytics", "digital-thread-components", "gis-mapping-spatial-analysis-capstone", "visual-recognition", "causal-effects", "business-statistics-analysis-capstone", "biological-diversity", "advanced-manufacturing-process-analysis", "computational-phenotyping", "functional-mri-2", "financial-regulation", "big-data-project", "information-visualization-applied-perception", "digital-footprint", "meaningful-predictive-modeling", "r-data-visualization", "google-machine-learning-es", "communicating-business-analytics-results", "executive-data-science-capstone", "sas-viya-rest-api-python-r", "healthcare-data-models", "biostatistics-2", "entrepreneurs-blockchain-technology", "understanding-china-history-part-1", "scientific-approach-innovation-management", "r-packages", "neural-networks-deep-learning-ar", "data-science-for-business-innovation", "advanced-portfolio-construction-python", "digital-thread-implementation", "healthcare-data-quality-governance", "linear-models-2", "causal-inference-2", "missing-data", "analytics-capstone", "information-visualization-advanced-techniques", "gcp-big-data-ml-fundamentals-br", "statistics-project", "data-intensive-applications", "cyber-security-manufacturing", "analytical-solutions-common-healthcare-problems", "bd2k-lincs", "geodesign", "visualization-for-data-journalism", "gcp-big-data-ml-fundamentals-fr", "reproducible-templates-analysis", "data-results", "building-resilient-streaming-systems-gcp-br", "global-statistics", "data-collection-analytics-project", "data-warehouse-bi-building", "intermediate-programming-capstone", "mining-medical-data", "big-data-introduction-ar", "data-analytics-accountancy-2", "serverless-machine-learning-gcp-es", "roadmap-success-digital-manufacturing-design", "google-machine-learning-br", "training-others-nursing-informatics", "genomic-data-science-project", "recommeder-systems-capstone", "google-machine-learning-fr", "understanding-china-history-part-2", "serverless-machine-learning-gcp-jp", "data-scientists-tools-ar", "leveraging-unstructured-data-dataproc-gcp-jp", "launching-machine-learning-fr", "social-science-study-chinese-society", "data-analytics-business-capstone", "strategic-business-analytics-capstone", "google-machine-learning-de", "infonomics-2", "gcp-big-data-ml-fundamentals-de", "building-resilient-streaming-systems-gcp-jp", "leveraging-unstructured-data-dataproc-gcp-es", "data-mining-project", "r-capstone", "analytics-business-metrics-ar", "data-analysis-capstone", "feature-engineering-es", "building-resilient-streaming-systems-gcp-es", "launching-machine-learning-br", "intro-tensorflow-jp", "launching-machine-learning-es", "building-resilient-streaming-systems-gcp-fr", "intro-tensorflow-fr", "art-science-ml-es", "art-science-ml-jp", "nursing-informatics-leaders", "gcp-exploring-preparing-data-bigquery-jp", "social-science-research-chinese-society", "serverless-machine-learning-gcp-br", "feature-engineering-jp", "feature-engineering-fr", "serverless-machine-learning-gcp-fr", "leveraging-unstructured-data-dataproc-gcp-fr", "gcp-advanced-insights-bigquery-jp", "gcp-creating-bigquery-datasets-visualizing-insights-jp", "serverless-machine-learning-gcp-de", "leveraging-unstructured-data-dataproc-gcp-br", "leveraging-unstructured-data-dataproc-gcp-de", "art-science-ml-de", "feature-engineering-br", "art-science-ml-fr", "datasci-capstone", "intro-tensorflow-br", "predictive-modeling-clinical-data", "deep-neural-networks-with-pytorch", "launching-machine-learning-de", "clinical-natural-language-processing", "building-resilient-streaming-systems-gcp-de", "data-insights-gcp-apply-ml-jp", "intro-tensorflow-de", "power-sample-size", "art-science-ml-br", "ai-deep-learning-capstone", "big-data-services", "building-deep-learning-models-with-tensorflow", "complete-reinforcement-learning-system", "advanced-clinical-data-science", "feature-engineering-de"], "course_description": ["Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\n\nThis course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas. Introduction Linear Regression with One Variable Linear Algebra Review Linear Regression with Multiple Variables Octave/Matlab Tutorial Logistic Regression Regularization Neural Networks: Representation Neural Networks: Learning Advice for Applying Machine Learning Machine Learning System Design Support Vector Machines Unsupervised Learning Dimensionality Reduction Anomaly Detection Recommender Systems Large Scale Machine Learning Application Example: Photo OCR Welcome to Machine Learning! In this module, we introduce the core idea of teaching a computer to learn concepts using data\u00e2\u0080\u0094without being explicitly programmed. The Course Wiki is under construction. Please visit the resources tab for the most complete and up-to-date information. Linear regression predicts a real-valued output based on an input value. We discuss the application of linear regression to housing price prediction, present the notion of a cost function, and introduce the gradient descent method for learning. This optional module provides a refresher on linear algebra concepts. Basic understanding of linear algebra is necessary for the rest of the course, especially as we begin to cover models with multiple variables. \nWhat if your input has more than one value? In this module, we show how linear regression can be extended to accommodate multiple input features. We also discuss best practices for implementing linear regression. This course includes programming assignments designed to help you understand how to implement the learning algorithms in practice. To complete the programming assignments, you will need to use Octave or MATLAB. This module introduces Octave/Matlab and shows you how to submit an assignment. Logistic regression is a method for classifying data into discrete outcomes. For example, we might use logistic regression to classify an email  as spam or not spam. In this module, we introduce the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.\n Machine learning models need to generalize well to new examples that the model has not seen in practice. In this module, we introduce regularization, which helps prevent models from overfitting the training data.  Neural networks is a model inspired by how the brain works. It is widely used today in many applications: when your phone interprets and understand your voice commands, it is likely that a neural network is helping to understand your speech; when you cash a check, the machines that automatically read the digits also use neural networks.  In this module, we introduce the backpropagation algorithm that is used to help learn parameters for a neural network. At the end of this module, you will be implementing your own neural network for digit recognition.\n Applying machine learning in practice is not always straightforward. In this module, we share best practices for applying machine learning in practice, and discuss the best ways to evaluate performance of the learned models.\n To optimize a machine learning algorithm, you\u00e2\u0080\u0099ll need to first understand where the biggest improvements can be made. In this module, we discuss how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data.\n Support vector machines, or SVMs, is a machine learning algorithm for classification. We introduce the idea and intuitions behind SVMs and discuss how to use it in practice.\n We use unsupervised learning to build models that help us understand our data better. We discuss the k-Means algorithm for clustering that enable us to learn groupings of unlabeled data points. In this module, we introduce Principal Components Analysis, and show how it can be used for data compression to speed up learning algorithms as well as for visualizations of complex datasets.\n Given a large number of data points, we may sometimes want to figure out which ones vary significantly from the average. For example, in manufacturing, we may want to detect defects or anomalies. We show how a dataset can be modeled using a Gaussian distribution, and how the model can be used for anomaly detection.\n When you buy a product online, most websites automatically recommend other products that you may like. Recommender systems look at patterns of activities between different users and different products to produce these recommendations. In this module, we introduce recommender algorithms such as the collaborative filtering algorithm and low-rank matrix factorization. Machine learning works best when there is an abundance of data to leverage for training. In this module, we discuss how to apply the machine learning algorithms with large datasets. Identifying and recognizing objects, words, and digits in an image is a challenging task. We discuss how a pipeline can be built to tackle this problem and how to analyze and improve the performance of such a system.\n", "If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new \"superpower\" that will let you build AI systems that just weren't possible a few years ago. \n\nIn this course, you will learn the foundations of deep learning. When you finish this class, you will:\n- Understand the major technology trends driving Deep Learning\n- Be able to build, train and apply fully connected deep neural networks \n- Know how to implement efficient (vectorized) neural networks \n- Understand the key parameters in a neural network's architecture \n\nThis course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. \n\nThis is the first course of the Deep Learning Specialization. Introduction to deep learning Neural Networks Basics Shallow neural networks Deep Neural Networks Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.   Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.  Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.  Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision. ", "The art of uncovering the insights and trends in data has been around since ancient times. The ancient Egyptians used census data to increase efficiency in tax collection and they accurately predicted the flooding of the Nile river every year. Since then, people working in data science have carved out a unique and distinct field for the work they do. This field is data science. In this course, we will meet some data science practitioners and we will get an overview of what data science is today.\n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Defining Data Science and What Data Scientists Do Data Science Topics Data Science in Business In this module, you will go over the course syllabus to learn what will be taught in this course. Also, you will hear from data science professionals to learn what data science is, what data scientists do, and what tools and algorithms data scientists use on a daily basis. Finally, you will be required to complete a reading assignment to learn why data science is considered the sexiest job in the 21st century. In this module, you will hear from Norman White, the Faculty Director of the Stern Centre for Research Computing, at New York University, as he talks about data science and what skills are required for anyone interested in pursuing a career in this field and as he gives advice to those who are looking to start a career in data science. Finally, you will be required to complete reading assignments to learn about the process of mining a given dataset and about regression analysis. In this module, you will learn about what companies need to do in order to start with data science. You will also learn about some of the qualities that differentiate data scientists from other professionals. In addition, you will learn about analytics and what important role data scientists play in this process, and about story-telling and the importance of an effective final deliverable. Finally, you will be required to apply what you learned about data science by answering open-ended questions.\n", "This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses. \n\nThis course should be taken before any of the other Applied Data Science with Python courses: Applied Plotting, Charting & Data Representation in Python, Applied Machine Learning in Python, Applied Text Mining in Python, Applied Social Network Analysis in Python. Week 1 Week 2 Week 3 Week 4 In this week you'll get an introduction to the field of data science, review common Python functionality and features which data scientists use, and be introduced to the Coursera Jupyter Notebook for the lectures. All of the course information on grading, prerequisites, and expectations are on the course syllabus, and you can find more information about the Jupyter Notebooks on our Course Resources page. In this week of the course you'll learn the fundamentals of one of the most important toolkits Python has for data cleaning and processing -- pandas. You'll learn how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. The module ends with a programming assignment and a discussion question. In this week you'll deepen your understanding of the python pandas library by learning how to merge DataFrames, generate summary tables, group data into logical pieces, and manipulate dates. We'll also refresh your understanding of scales of data, and discuss issues with creating metrics for analysis. The week ends with a more significant programming assignment. In this week of the course you'll be introduced to a variety of statistical techniques such a distributions, sampling and t-tests. The majority of the week will be dedicated to your course project, where you'll engage in a real-world data cleaning activity and provide evidence for (or against!) a given hypothesis. This project is suitable for a data science portfolio, and will test your knowledge of cleaning, merging, manipulating, and test for significance in data. The week ends with two discussions of science and the rise of the fourth paradigm -- data driven discovery.", "This course will teach you how to build convolutional neural networks and apply it to image data. Thanks to deep learning, computer vision is working far better than just two years ago, and this is enabling numerous exciting applications ranging from safe autonomous driving, to accurate face recognition, to automatic reading of radiology images. \n\nYou will:\n- Understand how to build a convolutional neural network, including recent variations such as residual networks.\n- Know how to apply convolutional networks to visual detection and recognition tasks.\n- Know to use neural style transfer to generate art.\n- Be able to apply these algorithms to a variety of image, video, and other 2D or 3D data.\n\nThis is the fourth course of the Deep Learning Specialization. Foundations of Convolutional Neural Networks Deep convolutional models: case studies Object detection Special applications: Face recognition & Neural style transfer Learn to implement the foundational layers of CNNs (pooling, convolutions) and to stack them properly in a deep network to solve multi-class image classification problems. Learn about the practical tricks and methods used in deep CNNs straight from the research papers.  Learn how to apply your knowledge of CNNs to one of the toughest but hottest field of computer vision: Object detection. Discover how CNNs can be applied to multiple fields, including art generation and face recognition. Implement your own algorithm to generate art and recognize faces!", "This course will teach you the \"magic\" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. \n\nAfter 3 weeks, you will: \n- Understand industry best-practices for building deep learning applications. \n- Be able to effectively use the common neural network \"tricks\", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, \n- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. \n- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance\n- Be able to implement a neural network in TensorFlow. \n\nThis is the second course of the Deep Learning Specialization. Practical aspects of Deep Learning Optimization algorithms Hyperparameter tuning, Batch Normalization and Programming Frameworks   ", "This introduction to Python will kickstart your learning of Python for data science, as well as programming in general. This beginner-friendly Python course will take you from zero to programming in Python in a matter of hours.\n\nModule 1 - Python Basics\no\tYour first program\no\tTypes\no\tExpressions and Variables\no\tString Operations\n\nModule 2 - Python Data Structures\no\tLists and Tuples\no\tSets\no\tDictionaries\n\nModule 3 - Python\u00c2\u00a0Programming Fundamentals\no\tConditions and Branching\no\tLoops\no\tFunctions\no\tObjects and Classes\n\nModule 4 - Working with Data in Python\no\tReading files with open\no\tWriting files with open\no\tLoading data with Pandas\no\tNumpy \n\nFinally, you will create a project to test your skills. Python Basics  Python Data Structures  Python Programming Fundamentals  Working with Data in Python  Analyzing US Economic Data and Building a Dashboard      ", "This course will introduce the core data structures of the Python programming language. We will move past the basics of procedural programming and explore how we can use the Python built-in data structures such as lists, dictionaries, and tuples to perform increasingly complex data analysis. This course will cover Chapters 6-10 of the textbook \u00e2\u0080\u009cPython for Everybody\u00e2\u0080\u009d.  This course covers Python 3. Chapter Six: Strings Unit: Installing and Using Python Chapter Seven: Files Chapter Eight: Lists Chapter Nine: Dictionaries Chapter Ten: Tuples Graduation In this class, we pick up where we left off in the previous class, starting in Chapter 6 of the textbook and covering Strings and moving into data structures.   The second week of this class is dedicated to getting Python installed if you want to actually run the applications on your desktop or laptop.  If you choose not to install Python, you can just skip to the third week and get a head start. In this module you will set things up so you can write Python programs.  We do not require installation of Python for this class.  You can write and test Python programs in the browser using the \"Python Code Playground\" in this lesson.  Please read the \"Using Python in this Class\" material for details. Up to now, we have been working with data that is read from the user or data in constants.   But real programs process much larger amounts of data by reading and writing files on the secondary storage on your computer.   In this chapter we start to write our first programs that read, scan, and process real data.   As we want to solve more complex problems in Python, we need more powerful variables.  Up to now we have been using simple variables to store numbers or strings where we have a single value in a variable.  Starting with lists we will store many values in a single variable using an indexing scheme to store, organize, and retrieve different values from within a single variable.  We call these multi-valued variables \"collections\" or \"data structures\". The Python dictionary is one of its most powerful data structures.  Instead of representing values in a linear list, dictionaries store data as key / value pairs.  Using key / value pairs gives us a simple in-memory \"database\" in a single Python variable. Tuples are our third and final basic Python data structure.  Tuples are a simple version of lists.  We often use tuples in conjunction with dictionaries to accomplish multi-step tasks like sorting or looping through all of the data in a dictionary. To celebrate your making it to the halfway point in our Python for Everybody Specialization, we welcome you to attend our online graduation ceremony.  It is not very long, and it features a Commencement speaker and very short commencement speech.", "This course will teach you how to build models for natural language, audio, and other sequence data. Thanks to deep learning, sequence algorithms are working far better than just two years ago, and this is enabling numerous exciting applications in speech recognition, music synthesis, chatbots, machine translation, natural language understanding, and many others. \n\nYou will:\n- Understand how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs.\n- Be able to apply sequence models to natural language problems, including text synthesis. \n- Be able to apply sequence models to audio applications, including speech recognition and music synthesis.\n\nThis is the fifth and final course of the Deep Learning Specialization.\n\ndeeplearning.ai is also partnering with the NVIDIA Deep Learning Institute (DLI) in Course 5, Sequence Models, to provide a programming assignment on Machine Translation with deep learning. You will have the opportunity to build a deep learning project with cutting-edge, industry-relevant content. Recurrent Neural Networks Natural Language Processing & Word Embeddings Sequence models & Attention mechanism Learn about recurrent neural networks. This type of model has been proven to perform extremely well on temporal data. It has several variants including LSTMs, GRUs and Bidirectional RNNs, which you are going to learn about in this section. Natural language processing with deep learning is an important combination. Using word vector representations and embedding layers you can train recurrent neural networks with outstanding performances in a wide variety of industries. Examples of applications are sentiment analysis, named entity recognition and machine translation. Sequence models can be augmented using an attention mechanism. This algorithm will help your model understand where it should focus its attention given a sequence of inputs. This week, you will also learn about speech recognition and how to deal with audio data.", "As data collection has increased exponentially, so has the need for people skilled at using and interacting with data; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a data scientist, \u00e2\u0080\u009cpart mathematician, part computer scientist, and part trend spotter\u00e2\u0080\u009d (SAS Institute, Inc.). According to Glassdoor, being a data scientist is the best job in America; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good data scientist include being able to retrieve and work with data, and to do that you need to be well versed in SQL, the standard language for communicating with database systems.\n\nThis course is designed to give you a primer in the fundamentals of SQL and working with data so that you can begin analyzing it for data science purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your organization. This course starts with the basics and assumes you do not have any knowledge or skills in SQL. It will build on that foundation and gradually have you write both simple and complex queries to help you select data from tables.  You'll start to work with different types of data like strings and numbers and discuss methods to filter and pare down your results. \n\nYou will create new tables and be able to move data into them. You will learn common operators and how to combine the data. You will use case statements and concepts like data governance and profiling. You will discuss topics on data, and practice using real-world programming assignments. You will interpret the structure, meaning, and relationships in source data and use SQL as a professional to shape your data for targeted analysis purposes. \n\nAlthough we do not have any specific prerequisites or software requirements to take this course, a simple text editor is recommended for the final project. So what are you waiting for? This is your first step in landing a job in the best occupation in the US and soon the world! Getting Started and Selecting & Retrieving Data with SQL Filtering, Sorting, and Calculating Data with SQL Subqueries and Joins in SQL Modifying and Analyzing Data with SQL In this module, you will be able to define SQL and discuss how SQL differs from other computer languages. You will be able to compare and contrast the roles of a database administrator and a data scientist, and explain the differences between one-to-one, one-to-many, and many-to-many relationships with databases. You will be able to use the SELECT statement and talk about some basic syntax rules. You will be able to add comments in your code and synthesize its importance. In this module, you will be able to use several more new clauses and operators including WHERE, BETWEEN, IN, OR, NOT, LIKE, ORDER BY, and GROUP BY. You will be able to use the wildcard function to search for more specific or parts of records, including their advantages and disadvantages, and how best to use them. You will be able to discuss how to use basic math operators, as well as aggregate functions like AVERAGE, COUNT, MAX, MIN, and others to begin analyzing our data. In this module, you will be able to discuss subqueries, including their advantages and disadvantages, and when to use them. You will be able to recall the concept of a key field and discuss how these help us link data together with JOINs. You will be able to identify and define several types of JOINs, including the Cartesian join, an inner join, left and right joins, full outer joins, and a self join. You will be able to use aliases and pre-qualifiers to make your SQL code cleaner and efficient. In this module, you will be able to discuss how to modify strings by concatenating, trimming, changing the case, and using the substring function. You will be able to discuss the date and time strings specifically. You will be able to use case statements and finish this module by discussing data governance and profiling. You will also be able to apply fundamental principles when using SQL for data science. You'll be able to use tips and tricks to apply SQL in a data science context.\n", "In this first course of the specialization Excel Skills for Business you will learn the Essentials of Microsoft Excel. Within six weeks, you will learn to expertly navigate the Excel user interface, perform basic calculations with formulas and functions, professionally format spreadsheets, and create visualizations of data through charts and graphs.\n\nWhether you are self-taught and want to fill in the gaps for better efficiency and productivity, or whether you have never used Excel before, this course will set you up with a solid foundation to become a confident user and develop more advanced skills in later courses. \n\nWe have brought together a great teaching team that will be with you every step of the way. A broad range of practice quizzes and challenges will provide great opportunities to build up your skillset. Work through each new challenge with our team and in no time you will surprise yourself with how far you have come. \n\nSpreadsheet software is one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. At a time when digital skills jobs are growing much faster than non-digital jobs, make sure to position yourself ahead of the rest by adding Excel skills to your employment portfolio. Critical Core of Excel Performing calculations Formatting Working with Data Printing Charts Final Assessment In this module, you will learn about key foundational features of Excel: The Excel user interface, basic Excel terminology, how to operate essential navigational controls in Excel and how to perform basic data entry with Excel spreadsheets.    The syntax of formulas is very important in Excel. In this module, you will get introduced to formulas and functions - learn how to write them, use them to perform calculations and understand the different cell references. Formatting helps to highlight key messages and make the data presentable. This module covers several formatting tools like font formatting, borders, alignment, number formatting, as well as the Excel styles and themes. This module is all about working with data \u00e2\u0080\u0093 and making it easy to work with. This week you will learn how you can manage your spreadsheets \u00e2\u0080\u0093 find data with Filter and Sort, retrieve and change data using Find and Replace, and use Conditional Formatting to highlight specific data. Sometimes you need to print your spreadsheets and this module will help you with that. Learn how you can optimise your spreadsheet for printing by managing margins, orientation, headers & footers, and more. Charts are one of the most common ways to present data visually. This module walks you through creating and modifying charts in Excel. ", "This course dives into the basics of machine learning using an approachable, and well-known programming language, Python. \nIn this course, we will be reviewing two main components:\nFirst, you will be learning about the purpose of Machine Learning and where it applies to the real world. \nSecond, you will get a general overview of Machine Learning topics such as supervised vs unsupervised learning,  model evaluation, and Machine Learning algorithms. \n\nIn this course, you practice with real-life examples of Machine learning and see how it affects society in ways you may not have guessed!\n\nBy just putting in a few hours a week for the next few weeks, this is what you\u00e2\u0080\u0099ll get.\n1) New skills to add to your resume, such as regression, classification, clustering, sci-kit learn and SciPy \n2) New projects that you can add to your portfolio, including cancer detection, predicting economic trends, predicting customer churn, recommendation engines, and many more.\n3) And a certificate in machine learning to prove your competency, and share it anywhere you like online or offline, such as LinkedIn profiles and social media.\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge upon successful completion of the course. Introduction to Machine Learning Regression Classification Clustering Recommender Systems Final Project In this week, you will learn about applications of Machine Learning in different fields such as health care, banking, telecommunication, and so on.  You\u00e2\u0080\u0099ll get a general overview of Machine Learning topics such as supervised vs unsupervised learning, and  the usage of each algorithm. Also, you understand the advantage of using Python libraries for implementing Machine Learning models. In this week, you will get a brief intro to regression. You learn about Linear, Non-linear, Simple and Multiple regression, and their applications. You apply all these methods on two different datasets, in the lab part. Also, you learn how to evaluate your regression model, and calculate its accuracy. In this week, you will learn about classification technique. You practice with different classification algorithms, such as KNN, Decision Trees, Logistic Regression and SVM. Also, you learn about pros and cons of each method, and different classification accuracy metrics. In this section, you will learn about different clustering approaches. You learn how to use clustering for customer segmentation, grouping same vehicles, and also clustering of weather stations. You understand 3 main types of clustering, including Partitioned-based Clustering, Hierarchical Clustering, and Density-based Clustering. In this module, you will learn about recommender systems. First, you will get introduced with main idea behind recommendation engines, then you understand two main types of recommendation engines, namely, content-based and collaborative filtering. In this module, you will do a project based of what you have learned so far. You will submit a report of your project for peer evaluation.", "In this course you will learn how to program in R and how to use R for effective data analysis. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language. The course covers practical issues in statistical computing which includes programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting R code. Topics in statistical data analysis will provide working examples. Week 1: Background, Getting Started, and Nuts & Bolts Week 2: Programming with R Week 3: Loop Functions and Debugging Week 4: Simulation & Profiling This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story.  Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week. We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice. This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R.", "Much of the world's data resides in databases. SQL (or Structured Query Language) is a powerful language which is used for communicating with and extracting data from databases. A working knowledge of databases and SQL is a must if you want to become a data scientist.\n\nThe purpose of this course is to introduce relational database concepts and help you learn and apply foundational knowledge of the SQL language. It is also intended to get you started with performing SQL access in a data science environment.  \n\nThe emphasis in this course is on hands-on and practical learning . As such, you will work with real databases, real data science tools, and real-world datasets. You will create a database instance in the cloud. Through a series of hands-on labs you will practice building and running SQL queries. You will also learn how to access databases from Jupyter notebooks using SQL and Python.\n\nNo prior knowledge of databases, SQL, Python, or programming is required.\n\nAnyone can audit this course at no-charge. If you choose to take this course and earn the Coursera course certificate, you can also earn an IBM digital badge upon successful completion of the course.\n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Week 1 - Introduction to Databases and Basic SQL Week 2 - Advanced SQL Week 3 - Accessing Databases using Python Week 4: Course Assignment In Week 1 you will be introduced to databases. You will create a database instance on the cloud.  You will learn some of the basic SQL statements. You will also write and practice basic SQL hands-on on a live database. By the end of this module you will: (1) earn how to use string patterns and ranges to search data, how to sort and group data in result sets, as well as (2) learn how to work with multiple tables in a relational database using  join operations. After completing the lessons in this week, you will learn how to explain the basic concepts related to using Python to connect to databases and then create tables, load data, query data using SQL and analyze data using Python  As a hands-on Data Science assignment, you will be working with multiple real world datasets for the city of Chicago. You will be asked questions that will help you understand the data just like a data scientist would. You will be assessed both on the correctness of your SQL queries and results.\n\n", "In this course on Linear Algebra we look at what linear algebra is and how it relates to vectors and matrices. Then we look through what vectors and matrices are and how to work with them, including the knotty problem of eigenvalues and eigenvectors, and how to use these to solve problems. Finally  we look at how to use these to do fun things with datasets - like how to rotate images of faces and how to extract eigenvectors to look at how the Pagerank algorithm works.\nSince we're aiming at data-driven applications, we'll be implementing some of these ideas in code, not just on pencil and paper. Towards the end of the course, you'll write code blocks and encounter Jupyter notebooks in Python, but don't worry, these will be quite short, focussed on the concepts, and will guide you through if you\u00e2\u0080\u0099ve not coded before.\n\nAt the end of this course you will have an intuitive understanding of vectors and matrices that will help you bridge the gap into linear algebra problems, and how to apply these concepts to machine learning. Introduction to Linear Algebra and to Mathematics for Machine Learning Vectors are objects that move around space Matrices in Linear Algebra: Objects that operate on Vectors Matrices make linear mappings Eigenvalues and Eigenvectors: Application to Data Problems In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the adding up - the point is to appreciate what they do and how they work so that, when things go wrong or there are special cases, you can understand why and what to do. In this module, we look at operations we can do with vectors - finding the modulus (size), angle between vectors (dot or inner product) and projections of one vector onto another. We can then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are what's called 'linearly independent.' This will complete our examination of vectors, allowing us to move on to matrices in module 3 and then start to solve linear algebra problems. Now that we've looked at vectors, we can turn to matrices.  First we look at how to use matrices as tools to solve linear algebra problems, and as objects that transform vectors. Then we look at how to solve systems of linear equations using matrices, which will then take us on to look at inverse matrices and determinants, and to think about what the determinant really is, intuitively speaking. Finally, we'll look at cases of special matrices that mean that the determinant is zero or where the matrix isn't invertible - cases where algorithms that need to invert a matrix will fail. In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, figure out how to apply a reflection to an image and manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally. Eigenvectors are particular vectors that are unrotated by a transformation matrix, and eigenvalues are the amount by which the eigenvectors are stretched. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course.", "What are some of the most popular data science tools, how do you use them, and what are their features? In this course, you'll learn about Jupyter Notebooks, RStudio IDE, Apache Zeppelin and Data Science Experience. You will learn about what each tool is used for, what programming languages they can execute, their features and limitations. With the tools hosted in the cloud on Cognitive Class Labs, you will be able to test each tool and follow instructions to run simple code in Python, R or Scala. To end the course, you will create a final project with a Jupyter Notebook on IBM Data Science Experience and demonstrate your proficiency preparing a notebook, writing Markdown, and sharing your work with your peers.\n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Introducing Skills Network Labs Jupyter Notebooks Apache Zeppelin Notebooks RStudio IDE IBM Watson Studio Project: Create and share a Jupyter Notebook This week, you will get an overview of the various data science tools available to you, hosted on Skills Network Labs. You will create an account and start exploring some of the features. This week, you will learn about a popular data science tool, Jupyter Notebooks, its features, and why they are so popular among data scientists today. This week, you will learn about Apache Zeppelin Notebooks, its feature, and how they are different from Jupyter Notebooks. This week, you will learn about a popular data science tool used by R programmers. You'll learn about the user interface and how to use its various features. This week, you will learn about an enterprise-ready data science platform by IBM, called Watson Studio (formerley known as Data Science Experience). You'll learn about some of the features and capabilities of what data scientists use in the industry. ", "Learn how to analyze data using Python. This course will take you from the basics of Python to exploring many different types of data. You will learn how to prepare data for analysis, perform simple statistical analysis, create meaningful data visualizations, predict future trends from data, and more!\n\nTopics covered:\n\n1) Importing Datasets\n2) Cleaning the Data\n3) Data frame manipulation\n4) Summarizing the Data\n5) Building machine learning Regression models\n6) Building data pipelines\n\n Data Analysis with Python will be delivered through lecture, lab, and assignments. It includes following parts:\n\nData Analysis libraries: will learn to use Pandas, Numpy and Scipy libraries to work with a sample dataset. We will introduce you to pandas, an open-source library, and we will use it to load, manipulate, analyze, and visualize cool datasets. Then we will introduce you to another open-source library, scikit-learn, and we will use some of its machine learning algorithms to build smart models and make cool predictions.\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  \n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Importing Datasets Data Wrangling  Exploratory Data Analysis Model Development  Model Evaluation Final Assignment IBM Digital Badge       ", "Machine learning (ML) is one of the fastest growing areas in technology and a highly sought after skillset in today\u00e2\u0080\u0099s job market. The World Economic Forum states the growth of artificial intelligence (AI) could create 58 million net new jobs in the next few years, yet it\u00e2\u0080\u0099s estimated that currently there are 300,000 AI engineers worldwide, but millions are needed. This means there is a unique and immediate opportunity for you to get started with learning the essential ML concepts that are used to build AI applications \u00e2\u0080\u0093 no matter what your skill levels are. Learning the foundations of ML now, will help you keep pace with this growth, expand your skills and even help advance your career. \n\nThis course will teach you how to get started with AWS Machine Learning. Key topics include: Machine Learning on AWS, Computer Vision on AWS, and Natural Language Processing (NLP) on AWS. Each topic consists of several modules deep-diving into variety of ML concepts, AWS services as well as insights from experts to put the concepts into practice. Introduction to Machine Learning Machine Learning Pipeline Amazon AI Services: Computer Vision Amazon AI Services: NLP Introduction to Amazon SageMaker     ", "You will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI, and know how to set direction for your team's work, this course will show you how.\n\nMuch of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two \"flight simulators\" that let you practice decision-making as a machine learning project leader. This provides \"industry experience\" that you might otherwise get only after years of ML work experience.\n\nAfter 2 weeks, you will: \n- Understand how to diagnose errors in a machine learning system, and \n- Be able to prioritize the most promising directions for reducing error\n- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance\n- Know how to apply end-to-end learning, transfer learning, and multi-task learning\n\nI've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time.\n\nThis is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization. ML Strategy (1) ML Strategy (2)  ", "In this course you will get an introduction to the main tools and ideas in the data scientist's toolbox. The course gives an overview of the data, questions, and tools that data analysts and data scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning data into actionable knowledge. The second is a practical introduction to the tools that will be used in the program like version control, markdown, git, GitHub, R, and RStudio. Data Science Fundamentals R and RStudio Version Control and GitHub R Markdown, Scientific Thinking, and Big Data  In this module, we'll introduce and define data science and data itself. We'll also go over some of the resources that data scientists use to get help when they're stuck. In this module, we'll help you get up and running with both R and RStudio. Along the way, you'll learn some basics about both and why data scientists use them.  During this module, you'll learn about version control and why it's so important to data scientists. You'll also learn how to use Git and GitHub to manage version control in data science projects. During this final module, you'll learn to use R Markdown and get an introduction to three concepts that are incredibly important to every successful data scientist: asking good questions, experimental design, and big data. ", "This 2-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.\n\nAt the end of this course, participants will be able to:\n\u00e2\u0080\u00a2 Identify the purpose and value of the key Big Data and Machine Learning products in the Google Cloud Platform\n\u00e2\u0080\u00a2 Use CloudSQL and Cloud Dataproc to migrate existing MySQL and Hadoop/Pig/Spark/Hive workloads to Google Cloud Platform\n\u00e2\u0080\u00a2 Employ BigQuery and Cloud Datalab to carry out interactive data analysis\n\u00e2\u0080\u00a2 Choose between Cloud SQL, BigTable and Datastore\n\u00e2\u0080\u00a2 Train and use a neural network using TensorFlow\n\u00e2\u0080\u00a2 Choose between different data processing products on the Google Cloud Platform\n\nBefore enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following:\n\u00e2\u0080\u00a2 A common query language such as SQL\n\u00e2\u0080\u00a2 Extract, transform, load activities\n\u00e2\u0080\u00a2 Data modeling\n\u00e2\u0080\u00a2 Machine learning and/or statistics\n\u00e2\u0080\u00a2 Programming in Python\n\nGoogle Account Notes:\n\u00e2\u0080\u00a2 Google services are currently unavailable in China.\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction to the Data and Machine Learning on Google Cloud Platform Specialization . Recommending Products using Cloud SQL and Spark Predict Visitor Purchases with BigQuery ML Create Streaming Data Pipelines with Cloud Pub/sub and Cloud Dataflow Classify Images with Pre-Built Models using Vision API and Cloud AutoML Summary Welcome to the Big Data and Machine Learning fundamentals on GCP course. Here you will learn the basics of how the course is structured and the four main big data challenges you will solve for. In this module you will have an existing Apache SparkML recommendation model that is running on-premise. You will learn about recommendation models and how you can run them in the cloud with Cloud Dataproc and Cloud SQL. In this module, you will learn the foundations of BigQuery and big data analysis at scale. You will then learn how to build your own custom machine learning model to predict visitor purchases using just SQL with BigQuery ML.  In this module you will engineer and build an auto-scaling streaming data pipeline to ingest, process, and visualize data on a dashboard. Before you build your pipeline you'll learn the foundations of message-oriented architecture and pitfalls to avoid when designing and implementing modern data pipelines. Don't want to create a custom ML model from scratch? Learn how to leverage and extend pre-built ML models like the Vision API and Cloud AutoML for image classification. In this final module, we will review the key challenges, solutions, and topics covered as part of this fundamentals course. We will also review additional resources and the steps you can take to get certified as a Google Cloud Data Engineer. ", "Despite the recent increase in computing power and access to data over the last couple of decades, our ability to use the data within the decision making process is either lost or not maximized at all too often, we don't have a solid understanding of the questions being asked and how to apply the data correctly to the problem at hand.\n\nThis course has one purpose, and that is to share a methodology that can be used within data science, to ensure that the data used in problem solving is relevant and properly manipulated to address the question at hand.\n\nAccordingly, in this course, you will learn:\n    - The major steps involved in tackling a data science problem.\n    - The major steps involved in practicing data science, from forming a concrete business or research problem, to collecting and analyzing data, to building a model, and understanding the feedback after model deployment.\n    - How data scientists think!\n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. From Problem to Approach and From Requirements to Collection From Understanding to Preparation and From Modeling to Evaluation From Deployment to Feedback In this module, you will learn about why we are interested in data science, what a methodology is, and why data scientists need a methodology. You will also learn about the data science methodology and its flowchart.You will learn about the first two stages of the data science methodology, namely Data Requirements and Data Understanding. Finally, through a lab session, you will learn how to complete the Business Understanding and the Analytic Approach stages as well Data Requirements and Data Collection stages pertaining to any data science problem.  In this module, you will learn what it means to understand data, and prepare or clean data. You will also lean about the purpose of data modeling and some characteristics of the modeling process. Finally, through a lab session, you will learn how to complete the Data Understanding and the Data Preparation stages as well as the Modeling and the Model Evaluation stages pertaining to any data science problem. In this module, you will learn about what happens when a model is deployed and why model feedback is important. Also, by completing a peer-reviewed assignment, you will demonstrate your understanding of the data science methodology by apply it to a problem that you define.", "This course will introduce students to the basics of the Structured Query Language (SQL) as well as basic database design for storing data as part of a multi-step data gathering, analysis, and processing effort.  The course will use SQLite3 as its database.  We will also build web crawlers and multi-step data gathering and visualization processes.  We will use the D3.js library to do basic data visualization.  This course will cover Chapters 14-15 of the book \u00e2\u0080\u009cPython for Everybody\u00e2\u0080\u009d. To succeed in this course, you should be familiar with the material covered in Chapters 1-13 of the textbook and the first three courses in this specialization. This course covers Python 3. Object Oriented Python Basic Structured Query Language Data Models and Relational SQL Many-to-Many Relationships in SQL Databases and Visualization To start this class out we cover the basics of Object Oriented Python. We won't be writing our own objects, but since many of the things we use like BeautifulSoup, strings, dictionaries, database connections all use Object Oriented (OO) patterns we should at least understand some of its patterns and terminology. We learn the four core CRUD operations (Create, Read, Update, and Delete) to manage data stored in a database. In this section we learn about how data is stored across multiple tables in a database and how rows are linked (i.e., we establish relationships) in the database. In this section we explore how to model situations like students enrolling in courses where each course has many students and each student is enrolled in many courses. In this section, we put it all together, retrieve and process some data and then use the Google Maps API to visualize our data.", "This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial. The issue of dimensionality of data will be discussed, and the task of clustering data, as well as evaluating those clusters, will be tackled. Supervised approaches for creating predictive models will be described, and learners will be able to apply the scikit learn predictive modelling methods while understanding process issues related to data generalizability (e.g. cross validation, overfitting). The course will end with a look at more advanced techniques, such as building ensembles, and practical limitations of predictive models. By the end of this course, students will be able to identify the difference between a supervised (classification) and unsupervised (clustering) technique, identify which technique they need to apply for a particular dataset and need, engineer features to meet that need, and write python code to carry out an analysis. \n\nThis course should be taken after Introduction to Data Science in Python and Applied Plotting, Charting & Data Representation in Python and before Applied Text Mining in Python and Applied Social Analysis in Python. Module 1: Fundamentals of Machine Learning - Intro to SciKit Learn Module 2: Supervised Machine Learning - Part 1 Module 3: Evaluation Module 4: Supervised Machine Learning - Part 2 This module introduces basic machine learning concepts, tasks, and workflow using an example classification problem based on the K-nearest neighbors method, and implemented using the scikit-learn library. This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting.  In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees.  This module covers evaluation and model selection methods that you can use to help understand and optimize the performance of your machine learning models.  This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning).  You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.", "This course introduces you to sampling and exploring data, as well as basic probability theory and Bayes' rule. You will examine various types of sampling methods, and discuss how such methods can impact the scope of inference. A variety of exploratory data analysis techniques will be covered, including numeric summary statistics and basic data visualization. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The concepts and techniques in this course will serve as building blocks for the inference and modeling courses in the Specialization. About Introduction to Probability and Data Introduction to Data Introduction to Data Project Exploratory Data Analysis and Introduction to Inference Exploratory Data Analysis and Introduction to Inference Project Introduction to Probability Introduction to Probability Project Probability Distributions Data Analysis Project This course introduces you to sampling and exploring data, as well as basic probability theory. You will examine various types of sampling methods and discuss how such methods can impact the utility of a data analysis. The concepts in this module will serve as building blocks for our later courses.Each lesson comes with a set of learning objectives that will be covered in a series of short videos. Supplementary readings and practice problems will also be suggested from OpenIntro Statistics, 3rd Edition, https://leanpub.com/openintro-statistics/, (a free online introductory statistics textbook, that I co-authored). There will be weekly quizzes designed to assess your learning and mastery of the material covered that week in the videos. In addition, each week will also feature a lab assignment, in which you will use R to apply what you are learning to real data. There will also be a data analysis project designed to enable you to answer research questions of your own choosing. Since this is a Coursera course, you are welcome to participate as much or as little as you\u00e2\u0080\u0099d like, though I hope that you will begin by participating fully. One of the most rewarding aspects of a Coursera course is participation in forum discussions about the course materials. Please take advantage of other students' feedback and insight and contribute your own perspective where you see fit to do so. You can also check out the resource page (https://www.coursera.org/learn/probability-intro/resources/crMc4) listing useful resources for this course. Thank you for joining the Introduction to Probability and Data community! Say hello in the Discussion Forums. We are looking forward to your participation in the course. Welcome to Introduction to Probability and Data! I hope you are just as excited about this course as I am! In the next five weeks, we will learn about designing studies, explore data via numerical summaries and visualizations, and learn about rules of probability and commonly used probability distributions. If you have any questions, feel free to post them on this module's forum (https://www.coursera.org/learn/probability-intro/module/rQ9Al/discussions?sort=lastActivityAtDesc&page=1) and discuss with your peers! To get started, view the learning objectives (https://www.coursera.org/learn/probability-intro/supplement/rooeY/lesson-learning-objectives) of Lesson 1 in this module. To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud. Welcome to Week 2 of Introduction to Probability and Data! Hope you enjoyed materials from Week 1. This week we will delve into numerical and categorical data in more depth, and introduce inference. To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud. Welcome to Week 3 of Introduction to Probability and Data! Last week we explored numerical and categorical data. This week we will discuss probability, conditional probability, the Bayes\u00e2\u0080\u0099 theorem, and provide a light introduction to Bayesian inference. Thank you for your enthusiasm and participation, and have a great week! I\u00e2\u0080\u0099m looking forward to working with you on the rest of this course. To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud. Great work so far! Welcome to Week 4 -- the last content week of Introduction to Probability and Data! This week we will introduce two probability distributions: the normal and the binomial distributions in particular. As usual, you can evaluate your knowledge in this week's quiz. There will be no labs for this week. Please don't hesitate to post any questions, discussions and related topics on this week's forum (https://www.coursera.org/learn/probability-intro/module/VdVNg/discussions?sort=lastActivityAtDesc&page=1). Well done! You have reached the last week of Introduction to Probability and Data! There will not be any new videos in this week, instead, you will be asked to complete an initial data analysis project with a real-world data set. The project is designed to help you discover and explore research questions of your own, using real data and statistical methods we learn in this class. The the project will be graded via peer assessments, meaning that you will need to evaluate three peers' projects after submitting your own. Get started with your data analysis in this week! It should be interesting and very exciting! As usual, feel free to post questions, concerns, and comments about the project on this week's forum (https://www.coursera.org/learn/probability-intro/module/BaTDb/discussions?sort=lastActivityAtDesc&page=1).", "Understanding statistics is essential to understand research in the social and behavioral sciences. In this course you will learn the basics of statistics; not just how to calculate them, but also how to evaluate them. This course will also prepare you for the next course in the specialization - the course Inferential Statistics. \n\nIn the first part of the course we will discuss methods of descriptive statistics. You will learn what cases and variables are and how you can compute measures of central tendency (mean, median and mode) and dispersion (standard deviation and variance). Next, we discuss how to assess relationships between variables, and we introduce the concepts correlation and regression. \n\nThe second part of the course is concerned with the basics of probability: calculating probabilities, probability distributions and sampling distributions. You need to know about these things in order to understand how inferential statistics work. \n\nThe third part of the course consists of an introduction to methods of inferential statistics - methods that help us decide whether the patterns we see in our data are strong enough to draw conclusions about the underlying population we are interested in. We will discuss confidence intervals and significance tests.\n\nYou will not only learn about all these statistical concepts, you will also be trained to calculate and generate these statistics yourself using freely available statistical software. Before we get started... Exploring Data Correlation and Regression Probability Probability Distributions Sampling Distributions Confidence Intervals Significance Tests Exam time! In this module we'll consider the basics of statistics. But before we start, we'll give you a broad sense of what the course is about and how it's organized. Are you new to Coursera or still deciding whether this is the course for you? Then make sure to check out the 'Course introduction' and 'What to expect from this course' sections below, so you'll have the essential information you need to decide and to do well in this course! If you have any questions about the course format, deadlines or grading, you'll probably find the answers here. Are you a Coursera veteran and ready to get started? Then you might want to skip ahead to the first course topic: 'Exploring data'. You can always check the general information later. Veterans and newbies alike: Don't forget to introduce yourself in the 'meet and greet' forum! In this first module, we\u00e2\u0080\u0099ll introduce the basic concepts of descriptive statistics. We\u00e2\u0080\u0099ll talk about cases and variables, and we\u00e2\u0080\u0099ll explain how you can order them in a so-called data matrix. We\u00e2\u0080\u0099ll discuss various levels of measurement and we\u00e2\u0080\u0099ll show you how you can present your data by means of tables and graphs. We\u00e2\u0080\u0099ll also introduce measures of central tendency (like mode, median and mean) and dispersion (like range, interquartile range, variance and standard deviation). We\u00e2\u0080\u0099ll not only tell you how to interpret them; we\u00e2\u0080\u0099ll also explain how you can compute them. Finally, we\u00e2\u0080\u0099ll tell you more about z-scores. In this module we\u00e2\u0080\u0099ll only discuss situations in which we analyze one single variable. This is what we call univariate analysis. In the next module we will also introduce studies in which more variables are involved. In this second module we\u00e2\u0080\u0099ll look at bivariate analyses: studies with two variables. First we\u00e2\u0080\u0099ll introduce the concept of correlation. We\u00e2\u0080\u0099ll investigate contingency tables (when it comes to categorical variables) and scatterplots (regarding quantitative variables). We\u00e2\u0080\u0099ll also learn how to understand and compute one of the most frequently used measures of correlation: Pearson's r. In the next part of the module we\u00e2\u0080\u0099ll introduce the method of OLS regression analysis. We\u00e2\u0080\u0099ll explain how you (or the computer) can find the regression line and how you can describe this line by means of an equation. We\u00e2\u0080\u0099ll show you that you can assess how well the regression line fits your data by means of the so-called r-squared. We conclude the module with a discussion of why you should always be very careful when interpreting the results of a regression analysis.      This module introduces concepts from probability theory and the rules for calculating with probabilities. This is not only useful for answering various kinds of applied statistical questions but also to understand the statistical analyses that will be introduced in subsequent modules. We start by describing randomness, and explain how random events surround us. Next, we provide an intuitive definition of probability through an example and relate this to the concepts of events, sample space and random trials. A graphical tool to understand these concepts is introduced here as well, the tree-diagram.Thereafter a number of concepts from set theory are explained and related to probability calculations. Here the relation is made to tree-diagrams again, as well as contingency tables. We end with a lesson where conditional probabilities, independence and Bayes rule are explained. All in all, this is quite a theoretical module on a topic that is not always easy to grasp. That's why we have included as many intuitive examples as possible. Probability distributions form the core of many statistical calculations. They are used as mathematical models to represent some random phenomenon and subsequently answer statistical questions about that phenomenon. This module starts by explaining the basic properties of a probability distribution, highlighting how it quantifies a random variable and also pointing out how it differs between discrete and continuous random variables. Subsequently the cumulative probability distribution is introduced and its properties and usage are explained as well. In a next lecture it is shown how a random variable with its associated probability distribution can be characterized by statistics like a mean and variance, just like observational data. The effects of changing random variables by multiplication or addition on these statistics are explained as well.The lecture thereafter introduces the normal distribution, starting by explaining its functional form and some general properties. Next, the basic usage of the normal distribution to calculate probabilities is explained. And in a final lecture the binomial distribution, an important probability distribution for discrete data, is introduced and further explained. By the end of this module you have covered quite some ground and have a solid basis to answer the most frequently encountered statistical questions. Importantly, the fundamental knowledge about probability distributions that is presented here will also provide a solid basis to learn about inferential statistics in the next modules. Methods for summarizing sample data are called descriptive statistics. However, in most studies we\u00e2\u0080\u0099re not interested in samples, but in underlying populations. If we employ data obtained from a sample to draw conclusions about a wider population, we are using methods of inferential statistics. It is therefore of essential importance that you know how you should draw samples. In this module we\u00e2\u0080\u0099ll pay attention to good sampling methods as well as some poor practices. To draw conclusions about the population a sample is from, researchers make use of a probability distribution that is very important in the world of statistics: the sampling distribution. We\u00e2\u0080\u0099ll discuss sampling distributions in great detail and compare them to data distributions and population distributions. We\u00e2\u0080\u0099ll look at the sampling distribution of the sample mean and the sampling distribution of the sample proportion.  We can distinguish two types of statistical inference methods. We can: (1) estimate population parameters; and (2) test hypotheses about these parameters. In this module we\u00e2\u0080\u0099ll talk about the first type of inferential statistics: estimation by means of a confidence interval. A confidence interval is a range of numbers, which, most likely, contains the actual population value. The  probability that the interval actually contains the population value is what we call the confidence level. In this module we\u00e2\u0080\u0099ll show you how you can construct confidence intervals for means and proportions and how you should interpret them. We\u00e2\u0080\u0099ll also pay attention to how you can decide how large your sample size should be. In this module we\u00e2\u0080\u0099ll talk about statistical hypotheses. They form the main ingredients of the method of significance testing. An hypothesis is nothing more than an expectation about a population. When we conduct a significance test, we use (just like when we construct a confidence interval) sample data to draw inferences about population parameters. The significance test is, therefore, also a method of inferential statistics. We\u00e2\u0080\u0099ll show that each significance test is based on two hypotheses:  the null hypothesis and  the alternative hypothesis. When you do a significance test, you assume that the null hypothesis is true unless your data provide strong evidence against it. We\u00e2\u0080\u0099ll show you how you can conduct a significance test about a mean and how you can conduct a test about a proportion. We\u00e2\u0080\u0099ll also demonstrate that significance tests and confidence intervals are closely related. We conclude the module by arguing that you can make right and wrong decisions while doing a test. Wrong decisions are referred to as Type I and Type II errors. This is the final module, where you can apply everything you've learned until now in the final exam. Please note that you can only take the final exam once a month, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others while taking this exam. Good luck! ", "\"A picture is worth a thousand words\". We are all familiar with this expression. It especially applies when trying to explain the insight obtained from the analysis of increasingly large datasets. Data visualization plays an essential role in the representation of both small and large-scale data.\n\nOne of the key skills of a data scientist is the ability to tell a compelling story, visualizing data and findings in an approachable and stimulating way. Learning how to leverage a software tool to visualize data will also enable you to extract information, better understand the data, and make more effective decisions.\n\nThe main goal of this Data Visualization with Python course is to teach you how to take data that at first glance has little meaning and present that data in a form that makes sense to people. Various techniques have been developed for presenting data visually but in this course, we will be using several data visualization libraries in Python, namely Matplotlib, Seaborn, and Folium.\n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Introduction to Data Visualization Tools Basic and Specialized Visualization Tools Advanced Visualizations and Geospatial Data In this module, you will learn about data visualization and some of the best practices to keep in mind when creating plots and visuals. You will also learn about the history and the architecture of Matplotlib and learn about basic plotting with Matplotlib. In addition, you will learn about the dataset on immigration to Canada, which will be used extensively throughout the course. Finally, you will briefly learn how to read csv files into a pandas dataframe and process and manipulate the data in the dataframe, and how to generate line plots using Matplotlib. In this module, you learn about area plots and how to create them with Matplotlib, histograms and how to create them with Matplotlib, bar charts, and how to create them with Matplotlib, pie charts, and how to create them with Matplotlib, box plots and how to create them with Matplotlib, and scatter plots and bubble plots and how to create them with Matplotlib.\n In this module, you will learn about advanced visualization tools such as waffle charts and word clouds and how to create them. You will also learn about seaborn, which is another visualization library, and how to use it to generate attractive regression plots. In addition, you will learn about Folium, which is another visualization library, designed especially for visualizing geospatial data. Finally, you will learn how to use Folium to create maps of different regions of the world and how to superimpose markers on top of a map, and how to create choropleth maps.", "If you are a software developer who wants to build scalable AI-powered algorithms, you need to understand how to use the tools to build them. This Specialization will teach you best practices for using TensorFlow, a popular open-source framework for machine learning.\n\nIn Course 3 of the deeplearning.ai TensorFlow Specialization, you will build natural language processing systems using TensorFlow. You will learn to process text, including tokenizing and representing sentences as vectors, so that they can be input to a neural network. You\u00e2\u0080\u0099ll also learn to apply RNNs, GRUs, and LSTMs in TensorFlow. Finally, you\u00e2\u0080\u0099ll get to train an  LSTM on existing text to create original poetry!\n\nThe Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization. Sentiment in text  Word Embeddings  Sequence models Sequence models and literature The first step in understanding sentiment in text, and in particular when training a neural network to do so is the tokenization of that text. This is the process of converting the text into numeric values, with a number representing a word or a character. This week you'll learn about the Tokenizer and pad_sequences APIs in TensorFlow and how they can be used to prepare and encode text and sentences to get them ready for training neural networks!\n Last week you saw how to use the Tokenizer to prepare your text to be used by a neural network by converting words into numeric tokens, and sequencing sentences from these tokens. This week you'll learn about Embeddings, where these tokens are mapped as vectors in a high dimension space. With Embeddings and labelled examples, these vectors can then be tuned so that words with similar meaning will have a similar direction in the vector space. This will begin the process of training a neural network to udnerstand sentiment in text -- and you'll begin by looking at movie reviews, training a neural network on texts that are labelled 'positive' or 'negative' and determining which words in a sentence drive those meanings. In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'. This week you'll start digging into a variety of model formats that are used in training models to understand context in sequence! Taking everything that you've learned in training a neural network based on NLP, we thought it might be a bit of fun to turn the tables away from classification and use your knowledge for prediction. Given a body of words, you could conceivably predict the word most likely to follow a given word or phrase, and once you've done that, to do it again, and again. With that in mind, this week you'll build a poetry generator. It's trained with the lyrics from traditional Irish songs, and can be used to produce beautiful-sounding verse of it's own!", "Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. In the United States alone, millions of job advertisements requiring Excel skills are posted every day. Research by Burning Glass Technologies and Capital One shows that digitals skills lead to higher income and better employment opportunities at a time when digital skills job are growing much faster than non-digital jobs.\n\nIn this second course of our Excel specialization Excel Skills for Business you will build on the strong foundations of the Essentials course. Intermediate Skills I will expand your Excel knowledge to new horizons. You are going to discover a whole range of skills and techniques that will become a standard component of your everyday use of Excel. In this course, you will build a solid layer of more advanced skills so you can manage large datasets and create meaningful reports. These key techniques and tools will allow you to add a sophisticated layer of automation and efficiency to your everyday tasks in Excel.\n\nOnce again, we have brought together a great teaching team that will be with you every step of the way. Prashan and Nicky will guide you through each week (and I am even going to make a guest appearance in Week 5 to help you learn about my favourite tool in Excel - shh, no spoilers!). Work through each new challenge step-by-step and in no time you will surprise yourself by how far you have come. This time around, we are going to follow Uma's trials and tribulations as she is trying to find her feet in a new position in the fictitious company PushPin. For those of you who have done the Essentials course, you will already be familiar with the company. Working through her challenges which are all too common ones that we encounter everyday, will help you to more easily relate to the skills and techniques learned in each week and apply them to familiar and new contexts. Working with Multiple Worksheets & Workbooks Text and Date Functions Named Ranges Summarising Data Tables Pivot Tables, Charts and Slicers Final Assessment This module is all about working with multiple worksheets and workbooks. Learn how you can combine data, manage datasets and perform calculations across multiple sources. And don't forget the Toolbox with handy shortcuts and ninja tips. By the end of this module, you will be an expert in Date and Text functions. This module discusses ways you can extract information and manipulate data to fulfil specific business requirements. Learn how you can create, manage and apply Named Ranges to enhance your calculations. Graduate to advanced formulas in this module. Learn how you can use functions like COUNTIFS to extract information from data, as well as generate graphical representations of it. Tables, tables, tables. Start with creating, formatting and managing tables and then move on to sorting and filtering tables to get the data you need. Finally, wrap up this module automating your tables to make your work more efficient. This module deep dives into the popular (and very useful) pivot tables. Learn how you can create and modify them to solve a variety of business problems. Then gain skills to create interactive dashboards with pivot charts and slicers. ", "In this course you will learn what Artificial Intelligence (AI) is, explore use cases and applications of AI, understand AI concepts and terms like machine learning, deep learning and neural networks. You will be exposed to various issues and concerns surrounding AI such as ethics and bias, & jobs, and get advice from experts about learning and starting a career in AI.  You will also demonstrate AI in action with a mini project.\n \nThis course does not require any programming or computer science expertise and is designed to introduce the basics of AI to anyone whether you have a technical background or not. What is AI? Applications and Examples of AI AI Concepts, Terminology, and Application Areas AI: Issues, Concerns and Ethical Considerations The Future with AI, and AI in Action This week, you will learn what AI is. You will understand its applications and use cases and how it is transforming our lives.  This week, you will learn about basic AI concepts. You will understand how AI learns, and what some of its applications are. This week, you will learn about issues and concerns surrounding AI, including - ethical considerations, bias, jobs, etc. - their impact on society. This information will help you to have an informed discussion on the costs and benefits of AI, and reassure decision makers about implementing an AI solution. This week, you will learn about the current thinking on the future with AI, as well as hear from experts about their advice to learn and start a career in AI. You will also demonstrate AI in action by utilizing Computer Vision to classify images. ", "If you are a software developer who wants to build scalable AI-powered algorithms, you need to understand how to use the tools to build them. This course is part of the upcoming Machine Learning in Tensorflow Specialization and will teach you best practices for using TensorFlow, a popular open-source framework for machine learning.\n\nIn Course 2 of the deeplearning.ai TensorFlow Specialization, you will learn advanced techniques to improve the computer vision model you built in Course 1. You will explore how to work with real-world images in different shapes and sizes, visualize the journey of an image through convolutions to understand how a computer \u00e2\u0080\u009csees\u00e2\u0080\u009d information, plot loss and accuracy, and explore strategies to prevent overfitting, including augmentation and dropout. Finally, Course 2 will introduce you to transfer learning and how learned features can be extracted from models. \n\nThe Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization. Exploring a Larger Dataset Augmentation: A technique to avoid overfitting Transfer Learning Multiclass Classifications In the first course in this specialization, you had an introduction to TensorFlow, and how, with its high level APIs you could do basic image classification, an you learned a little bit about Convolutional Neural Networks (ConvNets). In this course you'll go deeper into using ConvNets will real-world data, and learn about techniques that you can use to improve your ConvNet performance, particularly when doing image classification!\n\nIn Week 1, this week, you'll get started by looking at a much larger dataset than you've been using thus far: The Cats and Dogs dataset which had been a Kaggle Challenge in image classification! You've heard the term overfitting a number of times to this point. Overfitting is simply the concept of being over specialized in training -- namely that your model is very good at classifying what it is trained for, but not so good at classifying things that it hasn't seen. In order to generalize your model more effectively, you will of course need a greater breadth of samples to train it on. That's not always possible, but a nice potential shortcut to this is Image Augmentation, where you tweak the training set to potentially increase the diversity of subjects it covers. You'll learn all about that this week! Building models for yourself is great, and can be very powerful. But, as you've seen, you can be limited by the data you have on hand. Not everybody has access to massive datasets or the compute power that's needed to train them effectively. Transfer learning can help solve this -- where people with models trained on large datasets train them, so that you can either use them directly, or, you can use the features that they have learned and apply them to your scenario. This is Transfer learning, and you'll look into that this week! You've come a long way, Congratulations! One more thing to do before we move off of ConvNets to the next module, and that's to go beyond binary classification. Each of the examples you've done so far involved classifying one thing or another -- horse or human, cat or dog. When moving beyond binary into Categorical classification there are some coding considerations you need to take into account. You'll look at them this week!", "Data about our browsing and buying patterns are everywhere.  From credit card transactions and online shopping carts, to customer loyalty programs and user-generated ratings/reviews, there is a staggering amount of data that can be used to describe our past buying behaviors, predict future ones, and prescribe new ways to influence future purchasing decisions. In this course, four of Wharton\u00e2\u0080\u0099s top marketing professors will provide an overview of key areas of customer analytics: descriptive analytics, predictive analytics, prescriptive analytics, and their application to real-world business practices including Amazon, Google, and Starbucks to name a few. This course provides an overview of the field of analytics so that you can make informed business decisions. It is an introduction to the theory of customer analytics, and is not intended to prepare learners to perform customer analytics. \n\nCourse Learning Outcomes: \n\nAfter completing the course learners will be able to...\n\nDescribe the major methods of customer data collection used by companies and understand how this data can inform business decisions\n\nDescribe the main tools used to predict customer behavior and identify the appropriate uses for each tool \n\nCommunicate key ideas about customer analytics and how the field informs business decisions\n\nCommunicate the history of customer analytics and latest best practices at top firms Introduction to Customer Analytics Descriptive Analytics Predictive Analytics Prescriptive Analytics Application/Case Studies What is Customer Analytics? How is this course structured? What will I learn in this course? What will I learn in the Business Analytics Specialization? These short videos will give you an overview of this course and the specialization; the substantive lectures begin in Week 2. \nIn this module, you\u00e2\u0080\u0099ll learn what data can and can\u00e2\u0080\u0099t describe about customer behavior as well as the most effective methods for collecting data and deciding what it means.  You\u00e2\u0080\u0099ll understand the critical difference between data which describes a causal relationship and data which describes a correlative one as you explore the synergy between data and decisions, including the principles for systematically collecting and interpreting data to make better business decisions. You\u00e2\u0080\u0099ll also learn how data is used to explore a problem or question, and how to use that data to create products, marketing campaigns, and other strategies. By the end of this module, you\u00e2\u0080\u0099ll have a solid understanding of effective data collection and interpretation so that you can use the right data to make the right decision for your company or business.\n Once you\u00e2\u0080\u0099ve collected and interpreted data, what do you do with it? In this module, you\u00e2\u0080\u0099ll learn how to take the next step: how to use data about actions in the past to make to make predictions about actions in the future. You\u00e2\u0080\u0099ll examine the main tools used to predict behavior, and learn how to determine which tool is right for which decision purposes. Additionally, you\u00e2\u0080\u0099ll learn the language and the frameworks for making predictions of future behavior. At the end of this module, you\u00e2\u0080\u0099ll be able to determine what kinds of predictions you can make to create future strategies, understand the most powerful techniques for predictive models including regression analysis, and be prepared to take  full advantage of analytics to create effective data-driven business decisions. How do you turn data into action? In this module, you\u00e2\u0080\u0099ll learn how prescriptive analytics provide recommendations for actions you can take to achieve your business goals. First, you\u00e2\u0080\u0099ll explore how to ask the right questions, how to define your objectives, and how to optimize for success. You\u00e2\u0080\u0099ll also examine critical examples of prescriptive models, including how quantity is impacted by price, how to maximize revenue, how to maximize profits, and how to best use online advertising. By the end of this module, you\u00e2\u0080\u0099ll be able to define a problem, define a good objective, and explore models for optimization which take competition into account, so that you can write prescriptions for data-driven actions that create success for your company or business. How do top firms put data to work? In this module, you\u00e2\u0080\u0099ll learn how successful businesses use data to create cutting-edge, customer-focused marketing practices. You\u00e2\u0080\u0099ll explore real-world examples of the five-pronged attack to apply customer analytics to marketing, starting with data collection and data exploration, moving toward building predictive models and optimization, and continuing all the way to data-driven decisions. At the end of this module, you\u00e2\u0080\u0099ll know the best way to put data to work in your own company or business, based on the most innovative and effective data-driven practices of today\u00e2\u0080\u0099s top firms.", "If you are a software developer who wants to build scalable AI-powered algorithms, you need to understand how to use the tools to build them. This Specialization will teach you best practices for using TensorFlow, a popular open-source framework for machine learning.\n\nIn this fourth course, you will learn how to build time series models in TensorFlow. You\u00e2\u0080\u0099ll first implement best practices to prepare time series data. You\u00e2\u0080\u0099ll also explore how RNNs and 1D ConvNets can be used for prediction. Finally, you\u00e2\u0080\u0099ll apply everything you\u00e2\u0080\u0099ve learned throughout the Specialization to build a sunspot prediction model using real-world data!\n\nThe Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization. Sequences and Prediction Deep Neural Networks for Time Series Recurrent Neural Networks for Time Series Real-world time series data Hi Learners and welcome to this course on sequences and prediction! In this course we'll take a look at some of the unique considerations involved when handling sequential time series data -- where values change over time, like the temperature on a particular day, or the number of visitors to your web site. We'll discuss various methodologies for predicting future values in these time series, building on what you've learned in previous courses! Having explored time series and some of the common attributes of time series such as trend and seasonality, and then having used statistical methods for projection, let's now begin to teach neural networks to recognize and predict on time series! Recurrent Neural networks and Long Short Term Memory networks are really useful to classify and predict on sequential data. This week we'll explore using them with time series...\n On top of DNNs and RNNs, let's also add convolutions, and then put it all together using a real-world data series -- one which measures sunspot activity over hundreds of years, and see if we can predict using it.\n", "If you want to break into competitive data science, then this course is for you! Participating in predictive modelling competitions can help you gain practical experience, improve and harness your data modelling skills in various domains such as credit, insurance, marketing, natural language processing, sales\u00e2\u0080\u0099 forecasting and computer vision to name a few. At the same time you get to do it in a competitive context against thousands of participants where each one tries to build the most predictive algorithm. Pushing each other to the limit can result in better performance and smaller prediction errors. Being able to achieve high ranks consistently can help you accelerate your career in data science.\n\nIn this course, you will learn to analyse and solve competitively such predictive modelling tasks. \n\nWhen you finish this class, you will:\n\n- Understand how to solve predictive modelling competitions efficiently and learn which of the skills obtained can be applicable to real-world tasks.\n- Learn how to preprocess the data and generate new features from various sources such as text and images.\n- Be taught advanced feature engineering techniques like generating mean-encodings, using aggregated statistical measures or finding nearest neighbors as a means to improve your predictions.\n- Be able to form reliable cross validation methodologies that help you benchmark your solutions and avoid overfitting or underfitting when tested with unobserved (test) data. \n- Gain experience of analysing and interpreting the data. You will become aware of inconsistencies, high noise levels, errors and other data-related issues such as leakages and you will learn how to overcome them. \n- Acquire knowledge of different algorithms and learn how to efficiently tune their hyperparameters and achieve top performance. \n- Master the art of combining different machine learning models and learn how to ensemble. \n- Get exposed to past (winning) solutions and codes and learn how to read them.\n\nDisclaimer : This is not a machine learning course in the general sense. This course will teach you how to get high-rank solutions against thousands of competitors with focus on practical usage of machine learning methods rather than the theoretical underpinnings behind them.\n\nPrerequisites: \n- Python: work with DataFrames in pandas, plot figures in matplotlib, import and train models from scikit-learn, XGBoost, LightGBM.\n- Machine Learning: basic understanding of linear models, K-NN, random forest, gradient boosting and neural networks.\n\nDo you have technical problems? Write to us: coursera@hse.ru Introduction & Recap Feature Preprocessing and Generation with Respect to Models Final Project Description Exploratory Data Analysis Validation Data Leakages Metrics Optimization Advanced Feature Engineering I Hyperparameter Optimization Advanced feature engineering II Ensembling Competitions go through Final Project This week we will introduce you to competitive data science. You will learn about competitions' mechanics, the difference between competitions and a real life data science,  hardware and software that people usually use in competitions. We will also briefly recap major ML models frequently used in competitions. In this module we will summarize approaches to work with features: preprocessing, generation and extraction. We will see, that the choice of the machine learning model impacts both preprocessing we apply to the features and our approach to generation of new ones. We will also discuss feature extraction from text with Bag Of Words and Word2vec, and feature extraction from images with Convolution Neural Networks. This is just a reminder, that the final project in this course is better to start soon! The final project is in fact a competition, in this module you can find an information about it. We will start this week with Exploratory Data Analysis (EDA). It is a very broad and exciting topic and an essential component of solving process. Besides regular videos you will find a walk through EDA process for Springleaf competition data and an example of prolific EDA for NumerAI competition with extraordinary findings. In this module we will discuss various validation strategies. We will see that the strategy we choose depends on the competition setup and that correct validation scheme is one of the bricks for any winning solution.    Finally, in this module we will cover something very unique to data science competitions. That is, we will see examples how it is sometimes possible to get a top position in a competition with a very little machine learning, just by exploiting a data leakage.    This week we will first study another component of the competitions: the evaluation metrics. We will recap the most prominent ones and then see, how we can efficiently optimize a metric given in a competition. In this module we will study a very powerful technique for feature generation. It has a lot of names, but here we call it \"mean encodings\". We will see the intuition behind them, how to construct them, regularize and extend them.     In this module we will talk about hyperparameter optimization process. We will also have a special video with practical tips and tricks, recorded by four instructors. In this module we will learn about a few more advanced feature engineering techniques. Nowadays it is hard to find a competition won by a single model! Every winning solution incorporates ensembles of models. In this module we will talk about the main ensembling techniques in general, and, of course, how it is better to ensemble the models in practice.  For the 5th week we've prepared for you several \"walk-through\" videos. In these videos we discuss solutions to competitions we took prizes at. The video content is quite short this week to let you spend more time on the final project. Good luck! Final project for the course.", "A good algorithm usually comes together with a set of good data structures that allow the algorithm to manipulate the data efficiently. In this course, we consider the common data structures that are used in various computational problems. You will learn how these data structures are implemented in different programming languages and will practice implementing them in our programming assignments. This will help you to understand what is going on inside a particular built-in implementation of a data structure and what to expect from it. You will also learn typical use cases for these data structures.\n\nA few examples of questions that we are going to cover in this class are the following:\n1. What is a good strategy of resizing a dynamic array?\n2. How priority queues are implemented in C++, Java, and Python?\n3. How to implement a hash table so that the amortized running time of all operations is O(1) on average?\n4. What are good strategies to keep a binary tree balanced? \n\nYou will also learn how services like Dropbox manage to upload some large files instantly and to save a lot of storage space!\n\nDo you have technical problems? Write to us: coursera@hse.ru Basic Data Structures Dynamic Arrays and Amortized Analysis Priority Queues and Disjoint Sets Hash Tables Binary Search Trees Binary Search Trees 2 In this module, you will learn about the basic data structures used throughout the rest of this course.  We start this module by looking in detail at the fundamental building blocks: arrays and linked lists. From there, we build up two important data structures: stacks and queues. Next, we look at trees: examples of how they\u00e2\u0080\u0099re used in Computer Science, how they\u00e2\u0080\u0099re implemented, and the various ways they can be traversed. Once you\u00e2\u0080\u0099ve completed this module, you will be able to implement any of these data structures, as well as have a solid understanding of the costs of the operations, as well as the tradeoffs involved in using each data structure. In this module, we discuss Dynamic Arrays: a way of using arrays when it is unknown ahead-of-time how many elements will be needed. Here, we also discuss amortized analysis: a method of determining the amortized cost of an operation over a sequence of operations. Amortized analysis is very often used to analyse performance of algorithms when the straightforward analysis produces unsatisfactory results, but amortized analysis helps to show that the algorithm is actually efficient. It is used both for Dynamic Arrays analysis and will also be used in the end of this course to analyze Splay trees. We start this module by considering priority queues which are used to efficiently schedule jobs, either in the context of a computer operating system or in real life, to sort huge files, which is the most important building block for any Big Data processing algorithm, and to efficiently compute shortest paths in graphs, which is a topic we will cover in our next course. For this reason, priority queues have built-in implementations in many programming languages, including C++, Java, and Python. We will see that these implementations are based on a beautiful idea of storing a complete binary tree in an array that allows to implement all priority queue methods in just few lines of code. We will then switch to disjoint sets data structure that is used, for example, in dynamic graph connectivity and image processing. We will see again how simple and natural ideas lead to an implementation that is both easy to code and very efficient. By completing this module, you will be able to implement both these data structures efficiently from scratch. In this module you will learn about very powerful and widely used technique called hashing. Its applications include implementation of programming languages, file systems, pattern search, distributed key-value storage and many more. You will learn how to implement data structures to store and modify sets of objects and mappings from one type of objects to another one. You will see that naive implementations either consume huge amount of memory or are slow, and then you will learn to implement hash tables that use linear memory and work in O(1) on average! In the end, you will learn how hash functions are used in modern disrtibuted systems and how they are used to optimize storage of services like Dropbox, Google Drive and Yandex Disk! In this module we study binary search trees, which are a data structure for doing searches on dynamically changing ordered sets. You will learn about many of the difficulties in accomplishing this task and the ways in which we can overcome them. In order to do this you will need to learn the basic structure of binary search trees, how to insert and delete without destroying this structure, and how to ensure that the tree remains balanced. In this module we continue studying binary search trees. We study a few non-trivial applications. We then study the new kind of balanced search trees - Splay Trees. They adapt to the queries dynamically and are optimal in many ways.", "Do you have data and wonder what it can tell you?  Do you need a deeper understanding of the core ways in which machine learning can improve your business?  Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems?\n\nIn this course, you will get hands-on experience with machine learning from a series of practical case-studies.  At the end of the first course you will have studied how to predict house prices based on house-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend products, and search for images.  Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains.\n\nThis first course treats the machine learning method as a black box.  Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms.  Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Identify potential applications of machine learning in practice.  \n   -Describe the core differences in analyses enabled by regression, classification, and clustering.\n   -Select the appropriate machine learning task for a potential application.  \n   -Apply regression, classification, clustering, retrieval, recommender systems, and deep learning.\n   -Represent your data as features to serve as input to machine learning models. \n   -Assess the model quality in terms of relevant error metrics for each task.\n   -Utilize a dataset to fit a model to analyze new data.\n   -Build an end-to-end application that uses machine learning at its core.  \n   -Implement these techniques in Python. Welcome Regression: Predicting House Prices Classification: Analyzing Sentiment Clustering and Similarity: Retrieving Documents Recommending Products Deep Learning: Searching for Images Closing Remarks Machine learning is everywhere, but is often operating behind the scenes. <p>This introduction to the specialization provides you with insights into the power of machine learning, and the multitude of intelligent applications you personally will be able to develop and deploy upon completion.</p>We also discuss who we are, how we got here, and our view of the future of intelligent applications. This week you will build your first intelligent application that makes predictions from data.<p>We will explore this idea within the context of our first case study, predicting house prices, where you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  <p>This is just one of the many places where regression can be applied.Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.</p>You will also examine how to analyze the performance of your predictive model and implement regression in practice using a Jupyter notebook. How do you guess whether a person felt positively or negatively about an experience, just from a short review they wrote?<p>In our second case study, analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).This task is an example of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification.</p>You will analyze the accuracy of your classifier, implement an actual classifier in a Jupyter notebook, and take a first stab at a core piece of the intelligent application you will build and deploy in your capstone.   A reader is interested in a specific news article and you want to find a similar articles to recommend.  What is the right notion of similarity?  How do I automatically search over documents to find the one that is most similar?  How do I quantitatively represent the documents in the first place?<p>In this third case study, retrieving documents, you will examine various document representations and an algorithm to retrieve the most similar subset.  You will also consider structured representations of the documents that automatically group articles by similarity (e.g., document topic).</p>You will actually build an intelligent document retrieval system for Wikipedia entries in an Jupyter notebook. Ever wonder how Amazon forms its personalized product recommendations?  How Netflix suggests movies to watch?  How Pandora selects the next song to stream?  How Facebook or LinkedIn finds people you might connect with?  Underlying all of these technologies for personalized content is something called collaborative filtering. <p>You will learn how to build such a recommender system using a variety of techniques, and explore their tradeoffs.</p> One method we examine is matrix factorization, which learns features of users and products to form recommendations.  In a Jupyter notebook, you will use these techniques to build a real song recommender system. You\u00e2\u0080\u0099ve probably heard that Deep Learning is making news across the world as one of the most promising techniques in machine learning. Every industry is dedicating resources to unlock the deep learning potential, including for tasks such as image tagging, object recognition, speech recognition, and text analysis.<p>In our final case study, searching for images, you will learn how layers of neural networks provide very descriptive (non-linear) features that provide impressive performance in image classification and retrieval tasks.  You will then construct deep features, a transfer learning technique that allows you to use deep learning very easily, even when you have little data to train the model.</p>Using iPhython notebooks, you will build an image classifier and an intelligent image retrieval system with deep learning.    In the conclusion of the course, we will describe the final stage in turning our machine learning tools into a service: deployment.<p>We will also discuss some open challenges that the field of machine learning still faces, and where we think machine learning is heading.  We conclude with an overview of what's in store for you in the rest of the specialization, and the amazing intelligent applications that are ahead for us as we evolve machine learning.  ", "Reinforcement Learning is a subfield of Machine Learning, but is also a general purpose formalism for automated decision-making and AI. This course introduces you to statistical learning techniques where an agent explicitly takes actions and interacts with the world. Understanding the importance and challenges of learning agents that make decisions is of vital importance today, with more and more companies interested in interactive agents and intelligent decision-making. \n\nThis course introduces you to the fundamentals of Reinforcement Learning. When you finish this course, you will:\n- Formalize problems as Markov Decision Processes \n- Understand basic exploration methods and the exploration/exploitation tradeoff\n- Understand value functions, as a general-purpose tool for optimal decision-making\n- Know how to implement dynamic programming as an efficient solution approach to an industrial control problem\n\nThis course teaches you the key concepts of Reinforcement Learning, underlying classic and modern algorithms in RL. After completing this course, you will be able to start using RL for real problems, where you have or can specify the MDP. \n\nThis is the first course of the Reinforcement Learning Specialization. Welcome to the Course!  The K-Armed Bandit Problem Markov Decision Processes Value Functions & Bellman Equations  Dynamic Programming Welcome to: Fundamentals of Reinforcement Learning, the first course in a four-part specialization on Reinforcement Learning brought to you by the University of Alberta, Onlea, and Coursera. In this pre-course module, you'll be introduced to your instructors, get a flavour of what the course has in store for you, and be given  an in-depth roadmap to help make your journey through this specialization as smooth as possible.  For the first week of this course, you will learn how to understand the exploration-exploitation trade-off in sequential decision-making, implement incremental algorithms for estimating action-values, and compare the strengths and weaknesses to different algorithms for exploration. For this week\u00e2\u0080\u0099s graded assessment, you will implement and test an epsilon-greedy agent.  When you\u00e2\u0080\u0099re presented with a problem in industry, the first and most important step is to translate that problem into a Markov Decision Process (MDP). The quality of your solution depends heavily on how well you do this translation. This week, you will learn the definition of MDPs, you will understand goal-directed behavior and how this can be obtained from maximizing scalar rewards, and you will also understand the difference between episodic and continuing tasks. For this week\u00e2\u0080\u0099s graded assessment, you will create three example tasks of your own that fit into the MDP framework.  Once the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions. This week, you will learn the definition of policies and value functions, as well as Bellman equations, which is the key technology that all of our algorithms will use.  This week, you will learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming for industrial applications and problems. Further, you will learn about Generalized Policy Iteration as a common template for constructing algorithms that maximize reward. For this week\u00e2\u0080\u0099s graded assessment, you will implement an efficient dynamic programming agent in a simulated industrial control problem.", "Welcome to Data-driven Decision Making. In this course you'll get an introduction to Data Analytics and its role in business decisions. You'll learn why data is important and how it has evolved. You'll be introduced to \u00e2\u0080\u009cBig Data\u00e2\u0080\u009d and how it is used. You'll also be introduced to a framework for conducting Data Analysis and what tools and techniques are commonly used. Finally, you'll have a chance to put your knowledge to work in a simulated business setting.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. Introduction to Data Analytics Technology and types of data  Data analysis techniques and tools Data-driven decision making project  In this module you'll learn the basics of data analytics and how businesses use to solve problems. You'll learn the value data analytics brings to business decision-making processes. We\u00e2\u0080\u0099ll introduce you to a framework for data analysis and tools used in data analytics. Finally, we\u00e2\u0080\u0099re going to talk about careers and roles in data analytics and data science. This module is an introductory look at big data and big data analytics where you will learn the about different types of data. We\u00e2\u0080\u0099ll also introduce you to PwC's perspective on big data and explain the impact of big data on businesses.  Finally we will name some of the different types of tools and technologies used to gather data. In this module we will describe some of the tools for data analytics and some of the key technologies for data analysis. We will talk about how visualization is important to the practice of data analytics. Finally we will identify a variety of tools and languages used and consider when those tools are best used.\n The course project will give you an opportunity to practice what you have learned.  You will participate in a simulated business situation in which you will select the best course of action.  You will then prepare a final deliverable which will be evaluated by your peers.  Additionally, you will have the opportunity to provide feedback on your peer's submissions.  ", "In this first course of the specialization, you will discover just what data visualization is, and how we can use it to better see and understand data. Using Tableau, we\u00e2\u0080\u0099ll examine the fundamental concepts of data visualization and explore the Tableau interface, identifying and applying the various tools Tableau has to offer. By the end of the course you will be able to prepare and import data into Tableau and explain the relationship between data analytics and data visualization. This course is designed for the learner who has never used Tableau before, or who may need a refresher or want to explore Tableau in more depth.  No prior technical or analytical background is required.  The course will guide you through the steps necessary to create your first visualization story from the beginning based on data context,  setting the stage for you to advance to the next course in the Specialization. Getting Started & Introduction to Data Visualization Exploring and Navigating Tableau Making Data Connections Context of Data Visualization & Course Wrap-Up Welcome to this first module, where you will begin to discover the power of data visualization. You will define the meaning and purpose of data visualization and explore the various types of data visualization tools, beyond Tableau. You will install Tableau on your own device and create your first visualization. With the last module, you were able to create your first visualization through guided practice. The secret to doing visualizations is really knowing the tool you will be using. For this module, you will explore and navigate the Tableau interface and be able to use specific tools as you begin your visualization journey. Creating visualizations require data and in this module, you will discuss the various data sources for visualization and specifically what can be used in Tableau. You will prepare your data and identify the types of data connections possible with Tableau. You will be able to connect and merge to multiple data sources which can help make your visualizations more powerful. Data visualization is about telling a story using data. However, before you can be successful at data visualization, you must understand the \"who\", \"what\", and \"how\" of data context. In this final module, you will be able to determine who your audience will be and what your relationship to them is. You will analyze a real world application of data context and be able to write out a visualization story based on data context.", "What is machine learning, and what kinds of problems can it solve? Google thinks about machine learning slightly differently -- of being about logic, rather than just data. We talk about why such a framing is useful for data scientists when thinking about building a pipeline of machine learning models. \n\nThen, we discuss the five phases of converting a candidate use case to be driven by machine learning, and consider why it is important the phases not be skipped. We end with a recognition of the biases that machine learning can amplify and how to recognize this.\n\n>>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction to specialization What it means to be AI first How Google does ML Inclusive ML Python notebooks in the cloud Summary Introduces the specialization and the Google experts who will be teaching it. You will learn what we mean when we say that Google\u00e2\u0080\u0099s company strategy is to be AI-first, and what that means in practice. This module is about the organizational know-how Google has acquired over the years. This module will discuss why machine learning systems aren\u00e2\u0080\u0099t fair by default and some of the things you have to keep in mind as you infuse ML into your products. This module covers Cloud Datalab, which is the development environment you will use in this specialization. Review the core ML topics that this specialization will cover.", "This course will introduce the learner to information visualization basics, with a focus on reporting and charting using the matplotlib library. The course will start with a design and information literacy perspective, touching on what makes a good and bad visualization, and what statistical measures translate into in terms of visualizations. The second week will focus on the technology used to make visualizations in python, matplotlib, and introduce users to best practices when creating basic charts and how to realize design decisions in the framework. The third week will be a tutorial of functionality available in matplotlib, and demonstrate a variety of basic statistical charts helping learners to identify when a particular method is good for a particular problem. The course will end with a discussion of other forms of structuring and visualizing data. \n\nThis course should be taken after Introduction to Data Science in Python and before the remainder of the Applied Data Science with Python courses: Applied Machine Learning in Python, Applied Text Mining in Python, and Applied Social Network Analysis in Python. Module 1: Principles of Information Visualization Module 2: Basic Charting Module 3: Charting Fundamentals Module 4: Applied Visualizations In this module, you will get an introduction to principles of information visualization. We will be introduced to tools for thinking about design and graphical heuristics for thinking about creating effective visualizations. All of the course information on grading, prerequisites, and expectations are on the course syllabus, which is included in this module.  In this module, you will delve into basic charting. For this week\u00e2\u0080\u0099s assignment, you will work with real world CSV weather data. You will manipulate the data to display the minimum and maximum temperature for a range of dates and demonstrate that you know how to create a line graph using matplotlib. Additionally, you will demonstrate the procedure of composite charts, by overlaying a scatter plot of record breaking data for a given year. In this module you will explore charting fundamentals. For this week\u00e2\u0080\u0099s assignment you will work to implement a new visualization technique based on academic research. This assignment is flexible and you can address it using a variety of difficulties - from an easy static image to an interactive chart where users can set ranges of values to be used. In this module, then everything starts to come together. Your final assignment is entitled \u00e2\u0080\u009cBecoming a Data Scientist.\u00e2\u0080\u009d This assignment requires that you identify at least two publicly accessible datasets from the same region that are consistent across a meaningful dimension. You will state a research question that can be answered using these data sets and then create a visual using matplotlib that addresses your stated research question. You will then be asked to justify how your visual addresses your research question.", "The use of Excel is widespread in the industry. It is a very powerful data analysis tool and almost all big and small businesses use Excel in their day to day functioning. This is an introductory course in the use of Excel and is designed to give you a working knowledge of Excel with the aim of getting to use it for more advance topics in Business Statistics later. The course is designed keeping in mind two kinds of learners -  those who have very little functional knowledge of Excel and those who use Excel regularly but at a peripheral level and wish to enhance their skills. The course takes you from basic operations such as reading data into excel using various data formats, organizing and manipulating data, to some of the more advanced functionality of Excel. All along, Excel functionality is introduced using easy to understand examples which are demonstrated in a way that learners can become comfortable in understanding and applying them.\n\nTo successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. \n________________________________________\nWEEK 1\nModule 1: Introduction to Spreadsheets\nIn this module, you will be introduced to the use of Excel spreadsheets and various basic data functions of Excel.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tReading data into Excel using various formats\n\u00e2\u0080\u00a2\tBasic functions in Excel, arithmetic as well as various logical functions\n\u00e2\u0080\u00a2\tFormatting rows and columns\n\u00e2\u0080\u00a2\tUsing formulas in Excel and their copy and paste using absolute and relative referencing\n________________________________________\nWEEK 2\nModule 2: Spreadsheet Functions to Organize Data\nThis module introduces various Excel functions to organize and query data. Learners are introduced to the IF, nested IF, VLOOKUP and the HLOOKUP functions of Excel. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tIF and the nested IF functions\n\u00e2\u0080\u00a2\tVLOOKUP and HLOOKUP\n\u00e2\u0080\u00a2\tThe RANDBETWEEN function\n________________________________________\nWEEK 3\nModule 3: Introduction to Filtering, Pivot Tables, and Charts\nThis module introduces various data filtering capabilities of Excel. You\u00e2\u0080\u0099ll learn how to set filters in data to selectively access data. A very powerful data summarizing tool, the Pivot Table, is also explained and we begin to introduce the charting feature of Excel.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tVLOOKUP across worksheets\n\u00e2\u0080\u00a2\tData filtering in Excel\n\u00e2\u0080\u00a2\tUse of Pivot tables with categorical as well as numerical data\n\u00e2\u0080\u00a2\tIntroduction to the charting capability of Excel\n________________________________________\nWEEK 4\nModule 4: Advanced Graphing and Charting\nThis module explores various advanced graphing and charting techniques available in Excel. Starting with various line, bar and pie charts we introduce pivot charts, scatter plots and histograms. You will get to understand these various charts and get to build them on your own.\n\nTopics covered include\n\u00e2\u0080\u00a2\tLine, Bar and Pie charts\n\u00e2\u0080\u00a2\tPivot charts\n\u00e2\u0080\u00a2\tScatter plots\n\u00e2\u0080\u00a2\tHistograms Introduction to Spreadsheets Spreadsheet Functions to Organize Data Introduction to Filtering, Pivot Tables, and Charts Advanced Graphing and Charting Introduction to spreadsheets, reading data, manipulating data. Basic spreadsheet operations and functions. Introduction to some more useful functions such as the IF, nested IF, VLOOKUP and HLOOKUP functions in Excel. Introduction to the Data filtering capabilities of Excel, the construction of Pivot Tables to organize data and introduction to charts in Excel. Constructing various Line, Bar and Pie charts. Using the Pivot chart features of Excel. Understanding and constructing Histograms and Scatterplots.", "This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. Upon completing, you will be able to recognize NLP tasks in your day-to-day work, propose approaches, and judge what techniques are likely to work well.  The final project is devoted to one of the most hot topics in today\u00e2\u0080\u0099s NLP. You will build your own conversational chat-bot that will assist with search on StackOverflow website. The project will be based on practical assignments of the course, that will give you hands-on experience with such tasks as text classification, named entities recognition, and duplicates detection. \n\nThroughout the lectures, we will aim at finding a balance between traditional and deep learning techniques in NLP and cover them in parallel. For example, we will discuss word alignment models in machine translation and see how similar it is to attention mechanism in encoder-decoder neural networks. Core techniques are not treated as black boxes. On the contrary, you will get in-depth understanding of what\u00e2\u0080\u0099s happening inside. To succeed in that, we expect your familiarity with the basics of linear algebra and probability theory, machine learning setup, and deep neural networks. Some materials are based on one-month-old papers and introduce you to the very state-of-the-art in NLP research.\n\nDo you have technical problems? Write to us: coursera@hse.ru Intro and text classification Language modeling and sequence tagging Vector Space Models of Semantics Sequence to sequence tasks Dialog systems In this module we will have two parts: first, a broad overview of NLP area and our course goals, and second, a text classification task. It is probably the most popular task that you would deal with in real life. It could be news flows classification, sentiment analysis, spam filtering, etc. You will learn how to go from raw texts to predicted classes both with traditional methods (e.g. linear classifiers) and deep learning techniques (e.g. Convolutional Neural Nets). In this module we will treat texts as sequences of words. You will learn how to predict next words given some previous words. This task is called language modeling and it is used for suggests in search, machine translation, chat-bots, etc. Also you will learn how to predict a sequence of tags for a sequence of words. It could be used to determine part-of-speech tags, named entities or any other tags, e.g. ORIG and DEST in \"flights from Moscow to Zurich\" query. We will cover methods based on probabilistic graphical models and deep learning. This module is devoted to a higher abstraction for texts: we will learn vectors that represent meanings. First, we will discuss traditional models of distributional semantics. They are based on a very intuitive idea: \"you shall know the word by the company it keeps\". Second, we will cover modern tools for word and sentence embeddings, such as word2vec, FastText, StarSpace, etc. Finally, we will discuss how to embed the whole documents with topic models and how these models can be used for search and data exploration. Nearly any task in NLP can be formulates as a sequence to sequence task: machine translation, summarization, question answering, and many more. In this module we will learn a general encoder-decoder-attention architecture that can be used to solve them. We will cover machine translation in more details and you will see how attention technique resembles word alignment task in traditional pipeline. This week we will overview so-called task-oriented dialog systems like Apple Siri or Amazon Alexa. We will look in details at main building blocks of such systems namely Natural Language Understanding (NLU) and Dialog Manager (DM). We hope this week will encourage you to build your own dialog system as a final project!", "This capstone project course will give you a taste of what data scientists go through in real life when working with data. \n\nYou will learn about location data and different location data providers, such as Foursquare. You will learn how to make RESTful API calls to the Foursquare API to retrieve data about venues in different neighborhoods around the world. You will also learn how to be creative in situations where data are not readily available by scraping web data and parsing HTML code. You will utilize Python and its pandas library to manipulate data, which will help you refine your skills for exploring and analyzing data. \n\nFinally, you will be required to use the Folium library to great maps of geospatial data and to communicate your results and findings.\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge upon successful completion of the course.  \n\nLIMITED TIME OFFER: Subscription is only $39 USD per month for access to graded materials and a certificate. Introduction Foursquare API Neighborhood Segmentation and Clustering The Battle of Neighborhoods The Battle of Neighborhoods (Cont'd) In this module, you will learn about the scope of this capstone course and the context of the project that you will be working on. You will learn about different location data providers and what location data is normally composed of. Finally, you will be required to submit a link to a new repository on your Github account dedicated to this course. In this module, you will learn in details about Foursquare, which is the location data provider we will be using in this course, and its API. Essentially, you will learn how to create a Foursquare developer account, and use your credentials to search for nearby venues of a specific type, explore a particular venue, and search for trending venues around a location. In this module, you will learn about k-means clustering, which is a form of unsupervised learning. Then you will use clustering and the Foursquare API to segment and cluster the neighborhoods in the city of New York. Furthermore, you will learn how to scrape website and parse HTML code using the Python package Beautifulsoup, and convert data into a pandas dataframe. In this module, you will start working on the capstone project. You will clearly define a problem and discuss the data that you will be using to solve the problem. In this module, you will carry out all the remaining work to complete your capstone project. You will submit a report of your project for peer evaluation.", "Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world!\n\nAt the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V\u00e2\u0080\u0099s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking \u00e2\u0080\u009cAbout This Mac.\u00e2\u0080\u009d Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Welcome  Big Data: Why and Where Characteristics of Big Data and Dimensions of Scalability Data Science: Getting Value out of Big Data Foundations for Big Data Systems and Programming Systems: Getting Started with Hadoop Welcome to the Big Data Specialization!  We're excited for you to get to know us and we're looking forward to learning about you!  Data -- it's been around (even digitally) for a while.  What makes data \"big\" and where does this big data come from? You may have heard of the \"Big Vs\".  We'll give examples and descriptions of the commonly discussed 5.  But, we want to propose a 6th V and we'll ask you to practice writing Big Data questions targeting this V -- value. We love science and we love computing, don't get us wrong.  But the reality is we care about Big Data because it can bring value to our companies, our lives, and the world.  In this module we'll introduce a 5 step process for approaching data science problems. Big Data requires new programming frameworks and systems.  For this course, we don't  programming knowledge or experience -- but we do want to give you a grounding in some of the key concepts. Let's look at some details of Hadoop and MapReduce.  Then we'll go \"hands on\" and actually perform a simple MapReduce task in the Cloudera VM.  Pay attention - as we'll guide you in \"learning by doing\" in diagramming a MapReduce task as a Peer Review.", "The goal of this course is to give learners basic understanding of modern neural networks and their applications in computer vision and natural language understanding. The course starts with a recap of linear models and discussion of stochastic optimization methods that are crucial for training deep neural networks. Learners will study all popular building blocks of neural networks including fully connected layers, convolutional and recurrent layers. \nLearners will use these building blocks to define complex modern architectures in TensorFlow and Keras frameworks. In the course project learner will implement deep neural network for the task of image captioning which solves the problem of giving a text description for an input image.\n\nThe prerequisites for this course are: \n1) Basic knowledge of Python.\n2) Basic linear algebra and probability.\n\nPlease note that this is an advanced course and we assume basic knowledge of machine learning. You should understand:\n1) Linear regression: mean squared error, analytical solution.\n2) Logistic regression: model, cross-entropy loss, class probability estimation.\n3) Gradient descent for linear models. Derivatives of MSE and cross-entropy loss functions.\n4) The problem of overfitting.\n5) Regularization for linear models.\n\nDo you have technical problems? Write to us: coursera@hse.ru Introduction to optimization Introduction to neural networks Deep Learning for images Unsupervised representation learning Deep learning for sequences Final Project Welcome to the \"Introduction to Deep Learning\" course! In the first week you'll learn about linear models and stochatic optimization methods. Linear models are basic building blocks for many deep architectures, and stochastic optimization is used to learn every model that we'll discuss in our course. This module is an introduction to the concept of a deep neural network. You'll begin with the linear model and finish with writing your very first deep network. In this week you will learn about building blocks of deep learning for image input. You will learn how to build Convolutional Neural Network (CNN) architectures with these blocks and how to quickly solve a new task using so-called pre-trained models. This week we're gonna dive into unsupervised parts of deep learning. You'll learn how to generate, morph and search images with deep learning. In this week you will learn how to use deep learning for sequences such as texts, video, audio, etc. You will learn about several Recurrent Neural Network (RNN) architectures and how to apply them for different tasks with sequential input/output. In this week you will apply all your knowledge about neural networks for images and texts for the final project. You will solve the task of generating descriptions for real world images!", "Organizations large and small are inundated with data about consumer choices. But that wealth of information does not always translate into better decisions. Knowing how to interpret data is the challenge -- and marketers in particular are increasingly expected to use analytics to inform and justify their decisions. \n\nMarketing analytics enables marketers to measure, manage and analyze marketing performance to maximize its effectiveness and optimize return on investment (ROI). Beyond the obvious sales and lead generation applications, marketing analytics can offer profound insights into customer preferences and trends, which can be further utilized for future marketing and business decisions. \n\nThis course gives you the tools to measure brand and customer assets, understand regression analysis, and design experiments as a way to evaluate and optimize marketing campaigns. You'll leave the course with a solid understanding of how to use marketing analytics to predict outcomes and systematically allocate resources.\n\nFor more information on marketing analytics, you may visit; http://dmanalytics.org. You can also follow my posts in Twitter, @rajkumarvenk, and on linkedin; www.linkedin.com/in/rajkumar-venkatesan-14970a3. The Marketing Process Metrics for Measuring Brand Assets Customer Lifetime Value Marketing Experiments Regression Basics Welcome! We'll start with an overview of the marketing process and the transformational role of analytics. Then we'll walk through a case study. Ever heard of Airbnb? They're a powerhouse of the online community marketplace matching travelers to hosts. You'll see how they use analytics and the surprising results of their analyses. Firms spend millions on branding for one reason: It allows them to charge more for their products and services. In this module, we'll explore this valuable, if intangible, asset.  We'll discuss how to build and define a brand architecture and how to measure the impact of marketing efforts on brand value over time. By the end of this module, you'll be able to measure and track brand value. So let's get started! How valuable are your customers? That's a tough question that we'll show you how to answer in this module where we'll explore Customer Lifetime Value, or the future net value of a customer relationship. This forward-looking measure of the customer relationship helps you connect marketing strategies to future financial consequences and invest marketing dollars in the right place to maximize return over a customer's lifetime. By the end of this module, you will know how to measure customer lifetime value  and evaluate strategic marketing alternatives based on whether they improve customer retention and lifetime value. Ever wonder how much you have to cut prices to drive the most sales? Or which advertisement copy is more effective in customer conversion? Do an experiment! Experiments allow you to understand the effectiveness of different marketing strategies and forecast expected ROI. This week, we'll explore how to design basic experiments so that you can assess your marketing efforts and invest your marketing dollars most effectively. We'll help you avoid a gap between your test results and field implementation, and explore how web experiments can be implemented cheaply and quickly.  By the end of this module, you'll be able to design and conduct effective experiments that test your marketing campaigns--and then use the results to make future marketing decisions.  Ever wonder how variables influence consumer behavior in the real world--like how weather and a price promotion affect ice cream consumption? In this module, we will take a look at regression and how it's used to understand that relationship. We will discuss how to set up regressions and interpret outputs, explore confounding effects and biases, and distinguish between economic and statistical significance. We'll finish the week with a series of interviews with real marketing professionals who share their experiences and knowledge about how they use analytics on the job.", "Data science courses contain math\u00e2\u0080\u0094no avoiding that! This course is designed to teach learners the basic math you will need in order to be successful in almost any data science math course and was created for learners who have basic math skills but may not have taken algebra or pre-calculus. Data Science Math Skills introduces the core math that data science is built upon, with no extra complexity, introducing unfamiliar ideas and math symbols one-at-a-time. \n\nLearners who complete this course will master the vocabulary, notation, concepts, and algebra rules that all data scientists must know before moving on to more advanced material.\n\nTopics include:\n~Set theory, including Venn diagrams\n~Properties of the real number line\n~Interval notation and algebra with inequalities\n~Uses for summation and Sigma notation\n~Math on the Cartesian (x,y) plane, slope and distance formulas\n~Graphing and describing functions and their inverses on the x-y plane,\n~The concept of instantaneous rate of change and tangent lines to a curve\n~Exponents, logarithms, and the natural log function.\n~Probability theory, including Bayes\u00e2\u0080\u0099 theorem.\n\nWhile this course is intended as a general introduction to the math skills needed for data science, it can be considered a prerequisite for learners interested in the course, \"Mastering Data Analysis in Excel,\" which is part of the Excel to MySQL Data Science Specialization.  Learners who master Data Science Math Skills will be fully prepared for success with the more advanced math concepts introduced in \"Mastering Data Analysis in Excel.\" \n\nGood luck and we hope you enjoy the course! Welcome to Data Science Math Skills Building Blocks for Problem Solving Functions and Graphs Measuring Rates of Change Introduction to Probability Theory This short module includes an overview of the course's structure, working process, and information about course certificates, quizzes, video lectures, and other important course details. Make sure to read it right away and refer back to it whenever needed This module contains three lessons that are build to basic math vocabulary. The first lesson, \"Sets and What They\u00e2\u0080\u0099re Good For,\" walks you through the basic notions of set theory, including unions, intersections, and cardinality. It also gives a real-world application to medical testing. The second lesson, \"The Infinite World of Real Numbers,\" explains notation we use to discuss intervals on the real number line. The module concludes with the third lesson, \"That Jagged S Symbol,\" where you will learn how to compactly express a long series of additions and use this skill to define statistical quantities like mean and variance. This module builds vocabulary for graphing functions in the plane. In the first lesson, \"Descartes Was Really Smart,\" you will get to know the Cartesian Plane, measure distance in it, and find the equations of lines. The second lesson introduces the idea of a function as an input-output machine, shows you how to graph functions in the Cartesian Plane, and goes over important vocabulary. This module begins a very gentle introduction to the calculus concept of the derivative.  The first lesson, \"This is About the Derivative Stuff,\" will give basic definitions, work a few examples, and show you how to apply these concepts to the real-world problem of optimization. We then turn to exponents and logarithms, and explain the rules and notation for these math tools. Finally we learn about the rate of change of continuous growth, and the special constant known as \u00e2\u0080\u009ce\u00e2\u0080\u009d that captures this concept in a single number\u00e2\u0080\u0094near 2.718. This module introduces the vocabulary and notation of probability theory \u00e2\u0080\u0093 mathematics for the study of outcomes that are uncertain but have predictable rates of occurrence. \n\nWe start with the basic definitions and rules of probability, including the probability of two or more events both occurring, the sum rule and the product rule, and then proceed to Bayes\u00e2\u0080\u0099 Theorem and how it is used in practical problems.", "Important: The focus of this course is on math - specifically, data-analysis concepts and methods - not on Excel for its own sake. We use Excel to do our calculations, and all math formulas are given as Excel Spreadsheets, but we do not attempt to cover Excel Macros, Visual Basic, Pivot Tables, or other intermediate-to-advanced Excel functionality.\n\nThis course will prepare you to design and implement realistic predictive models based on data. In the Final Project (module 6) you will assume the role of a business data analyst for a bank, and develop two different predictive models to determine which applicants for credit cards should be accepted and which rejected. Your first model will focus on minimizing default risk, and your second on maximizing bank profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of business metric drives your choice of an optimal model.\n\nThe second big idea this course seeks to demonstrate is that your data-analysis results cannot and should not aim to eliminate all uncertainty. Your role as a data-analyst is to reduce uncertainty for decision-makers by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to real-world examples the most important uncertainty measures used in business, including classification error rates, entropy of information, and confidence intervals for linear regression.\n\nAll the data you need is provided within the course, all assignments are designed to be done in MS Excel, and you will learn enough Excel to complete all assignments. The course will give you enough practice with Excel to become fluent in its most commonly used business functions, and you\u00e2\u0080\u0099ll be ready to learn any other Excel functionality you might need in the future (module 1).\n\nThe course does not cover Visual Basic or Pivot Tables and you will not need them to complete the assignments. All advanced concepts are demonstrated in individual Excel spreadsheet templates that you can use to answer relevant questions. You will emerge with substantial vocabulary and practical knowledge of how to apply business data analysis methods based on binary classification (module 2), information theory and entropy measures (module 3), and linear regression (module 4 and 5), all using no software tools more complex than Excel. About This Course Excel Essentials for Beginners Binary Classification Information Measures Linear Regression Additional Skills for Model Building Final Course Project This course will prepare you to design and implement realistic predictive models based on data. In the Final Project (module 6) you will assume the role of a business data analyst for a bank, and develop two different predictive models to determine which applicants for credit cards should be accepted and which rejected.  Your first model will focus on minimizing default risk, and your second on maximizing bank profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of business metric drives your choice of an optimal model.The second big idea this course seeks to demonstrate is that your data-analysis results cannot and should not aim to eliminate all uncertainty.  Your role as a data-analyst is to reduce uncertainty for decision-makers by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to real-world examples the most important uncertainty measures used in business, including classification error rates, entropy of information, and confidence intervals for linear regression. All the data you need is provided within the course, and all assignments are designed to be done in MS Excel. The course will give you enough practice with Excel to become fluent in its most commonly used business functions, and you\u00e2\u0080\u0099ll be ready to learn any other Excel functionality you might need in future (module 1). The course does not cover Visual Basic or Pivot Tables and you will not need them to complete the assignments. All advanced concepts are demonstrated in individual Excel spreadsheet templates that you can use to answer relevant questions. You will emerge with substantial vocabulary and practical knowledge of how to apply business data analysis methods based on binary classification (module 2), information theory and entropy measures (module 3), and linear regression (module 4 and 5), all using no software tools more  complex than Excel.  In this module, will explore the essential Excel skills to address typical business situations you may encounter in the future. The Excel vocabulary and functions taught throughout this module make it possible for you to understand the additional explanatory Excel spreadsheets that accompany later videos in this course.  Separating collections into two categories, such as \u00e2\u0080\u009cbuy this stock, don\u00e2\u0080\u0099t but that stock\u00e2\u0080\u009d or \u00e2\u0080\u009ctarget this customer with a special offer, but not that one\u00e2\u0080\u009d is the ultimate goal of most business data-analysis projects. There is a specialized vocabulary of measures for comparing and optimizing the performance of the algorithms used to classify collections into two groups. You will learn how and why to apply these different metrics, including how to calculate the all-important AUC: the area under the Receiver Operating Characteristic (ROC) Curve.  In this module, you will learn how to calculate and apply the vitally useful uncertainty metric known as \u00e2\u0080\u009centropy.\u00e2\u0080\u009d In contrast to the more familiar \u00e2\u0080\u009cprobability\u00e2\u0080\u009d that represents the uncertainty that a single outcome will occur, \u00e2\u0080\u009centropy\u00e2\u0080\u009d quantifies the aggregate uncertainty of all possible outcomes.\nThe entropy measure provides the framework for accountability in data-analytic work. Entropy gives you the power to quantify the uncertainty of future outcomes relevant to your business twice: using the best-available estimates before you begin a project, and then again after you have built a predictive model.\nThe difference between the two measures is the Information Gain contributed by your work. The Linear Correlation measure is a much richer metric for evaluating associations than is commonly realized. You can use it to quantify how much a linear model reduces uncertainty.  When used to forecast future outcomes, it can be converted into a \u00e2\u0080\u009cpoint estimate\u00e2\u0080\u009d plus a \u00e2\u0080\u009cconfidence interval,\u00e2\u0080\u009d or converted into an information gain measure. You will develop a fluent knowledge of these concepts and the many valuable uses to which linear regression is put in business data analysis. This module also teaches how to use the Central Limit Theorem (CLT) to solve practical problems. The two topics are closely related because regression and the CLT both make use of a special family of probability distributions called \u00e2\u0080\u009cGaussians.\u00e2\u0080\u009d You will learn everything you need to know to work with Gaussians in these and other contexts. \n This module gives you additional valuable concepts and skills related to building high-quality models. \nAs you know, a \u00e2\u0080\u009cmodel\u00e2\u0080\u009d is a description of a process applied to available data (inputs) that produces an estimate of a future and as yet unknown outcome as output. \nVery often, models for outputs take the form of a probability distribution. This module covers how to estimate probability distributions from data (a \u00e2\u0080\u009cprobability histogram\u00e2\u0080\u009d), and how to describe and generate the most useful probability distributions used by data scientists. It also covers in detail how to develop a binary classification model with parameters optimized to maximize the AUC, and how to apply linear regression models when your input consists of multiple types of data for each event. \nThe module concludes with an explanation of \u00e2\u0080\u009cover-fitting\u00e2\u0080\u009d which is the main reason that apparently good predictive models often fail in real life business settings. We conclude with some tips for how you can avoid over-fitting in you own predictive model for the final project \u00e2\u0080\u0093 and in real life.\n The final course project is a comprehensive assessment covering all of the course material, and consists of four quizzes and a peer review assignment.  For quiz one and quiz two, there are learning points that explain components of the quiz.  These learning points will unlock only after you complete the quiz with a passing grade. Before you start, please read through the final project instructions.  From past student experience, the final project which includes all the quizzes and peer assessment, takes anywhere from 10-12 hours.", "How can you put data to work for you? Specifically, how can numbers in a spreadsheet tell us about present and past business activities, and how can we use them to forecast the future? The answer is in building quantitative models, and this course is designed to help you understand the fundamentals of this critical, foundational, business skill. Through a series of short lectures, demonstrations, and assignments, you\u00e2\u0080\u0099ll learn the key ideas and process of quantitative modeling so that you can begin to create your own models for your own business or enterprise. By the end of this course, you will have seen a variety of practical commonly used quantitative models as well as the building blocks that will allow you to start structuring your own models. These building blocks will be put to use in the other courses in this Specialization. Module 1: Introduction to Models  Module 2: Linear Models and Optimization Module 3: Probabilistic Models Module 4: Regression Models In this module, you will learn how to define a model, and how models are commonly used. You\u00e2\u0080\u0099ll examine the central steps in the modeling process, the four key mathematical functions used in models, and the essential vocabulary used to describe models. By the end of this module, you\u00e2\u0080\u0099ll be able to identify the four most common types of models, and how and when they should be used. You\u00e2\u0080\u0099ll also be able to define and correctly use the key terms of modeling, giving you not only a foundation for further study, but also the ability to ask questions and participate in conversations about quantitative models. This module introduces linear models, the building block for almost all modeling. Through close examination of the common uses together with examples of linear models, you\u00e2\u0080\u0099ll learn how to apply linear models, including cost functions and production functions to your business. The module also includes a presentation of growth and decay processes in discrete time, growth and decay in continuous time, together with their associated present and future value calculations. Classical optimization techniques are discussed. By the end of this module, you\u00e2\u0080\u0099ll be able to identify and understand the key structure of linear models, and suggest when and how to use them to improve outcomes for your business. You\u00e2\u0080\u0099ll also be able to perform present value calculations that are foundational to valuation metrics. In addition, you will understand how you can leverage models for your business, through the use of optimization to really fine tune and optimize your business functions.\n\n This module explains probabilistic models, which are ways of capturing risk in process. You\u00e2\u0080\u0099ll need to use probabilistic models when you don\u00e2\u0080\u0099t know all of your inputs. You\u00e2\u0080\u0099ll examine how probabilistic models incorporate uncertainty, and how that uncertainty continues through to the outputs of the model. You\u00e2\u0080\u0099ll also discover how propagating uncertainty allows you to determine a range of values for forecasting. You\u00e2\u0080\u0099ll learn the most-widely used models for risk, including regression models, tree-based models, Monte Carlo simulations, and Markov chains, as well as the building blocks of these probabilistic models, such as random variables, probability distributions, Bernoulli random variables, binomial random variables, the empirical rule, and perhaps the most important of all of the statistical distributions, the normal distribution, characterized by mean and standard deviation. By the end of this module, you\u00e2\u0080\u0099ll be able to define a probabilistic model, identify and understand the most commonly used probabilistic models, know the components of those models, and determine the most useful probabilistic models for capturing and exploring risk in your own business. This module explores regression models, which allow you to start with data and discover an underlying process. Regression models are the key tools in predictive analytics, and are also used when you have to incorporate uncertainty explicitly in the underlying data.  You\u00e2\u0080\u0099ll learn more about what regression models are, what they can and cannot do, and the questions regression models can answer. You\u00e2\u0080\u0099ll examine correlation and linear association, methodology to fit the best line to the data, interpretation of regression coefficients, multiple regression, and logistic regression. You\u00e2\u0080\u0099ll also see how logistic regression will allow you to estimate probabilities of success. By the end of this module, you\u00e2\u0080\u0099ll be able to identify regression models and their key components, understand when they are used, and be able to interpret them so that you can discuss your model and convince others that your model makes sense, with the ultimate goal of implementation.", "Course Overview: https://youtu.be/JgFV5qzAYno\n\nPython is now becoming the number 1 programming language for data science. Due to python\u00e2\u0080\u0099s simplicity and high readability, it is gaining its importance in the financial industry.  The course combines both python coding and statistical concepts and applies into analyzing financial data, such as stock data.\n\nBy the end of the course, you can achieve the following using python:\n\n- Import, pre-process, save and visualize financial data into pandas Dataframe\n\n- Manipulate the existing financial data by generating new variables using multiple columns\n\n- Recall and apply the important statistical concepts (random variable, frequency, distribution, population and sample, confidence interval, linear regression, etc. ) into financial contexts\n\n- Build a trading model using multiple linear regression model \n\n- Evaluate the performance of the trading model using different investment indicators\n\nJupyter Notebook environment is configured in the course platform for practicing python coding without installing any client applications. Visualizing and Munging Stock Data Random variables and distribution Sampling and Inference  Linear Regression Models for Financial Analysis Why do investment banks and consumer banks use Python to build quantitative models to predict returns and evaluate risks? What makes Python one of the most popular tools for financial analysis? You are going to learn basic python to import, manipulate and visualize stock data in this module. As Python is highly readable and simple enough, you can build one of the most popular trading models - Trend following strategy by the end of this module! In the previous module, we built a simple trading strategy base on Moving Average 10 and 50, which are \"random variables\" in statistics. In this module, we are going to explore basic concepts of random variables. By understanding the frequency and distribution of random variables, we extend further to the discussion of probability. In the later part of the module, we apply the probability concept in measuring the risk of investing a stock by looking at the distribution of log daily return using python. Learners are expected to have basic knowledge of probability before taking this module. In financial analysis, we always infer the real mean return of stocks, or equity funds, based on the  historical data of a couple years. This situation is in line with a core part of statistics - Statistical Inference - which we also base on sample data to infer the population of a target variable.In this module, you are going to understand the basic concept of statistical inference such as population, samples and random sampling. In the second part of the module, we shall estimate the range of mean return of a stock using a concept called confidence interval, after we understand the distribution of sample mean.We will also testify the claim of investment return using another statistical concept - hypothesis testing. In this module, we will explore the most often used prediction method - linear regression. From learning the association of random variables to simple and multiple linear regression model, we finally come to the most interesting part of this course: we will build a model using multiple indices from the global markets and predict the price change of an ETF of S&P500. In addition to building a stock trading model, it is also great fun to test the performance of your own models, which I will also show you how to evaluate them!", "In the capstone, students will build a series of applications to retrieve, process and visualize data using Python.   The projects will involve all the elements of the specialization.  In the first part of the capstone, students will do some visualizations to become familiar with the technologies in use and then will pursue their own project to visualize some other data that they have or can find.  Chapters 15 and 16 from the book \u00e2\u0080\u009cPython for Everybody\u00e2\u0080\u009d will serve as the backbone for the capstone. This course covers Python 3. Welcome to the Capstone Building a Search Engine Exploring Data Sources (Project) Spidering and Modeling Email Data Accessing New Data Sources (Project) Visualizing Email Data Visualizing new Data Sources (Project) Congratulations to everyone for making it this far. Before you begin, please view the Introduction video and read the Capstone Overview. The Course Resources section contains additional course-wide material that you may want to refer to in future weeks. This week we will download and run a simple version of the Google PageRank Algorithm and practice spidering some content. The assignment is peer-graded, and the first of three required assignments in the course. This a continuation of the material covered in Course 4 of the specialization, and is based on Chapter 16 of the textbook.  The optional Capstone project is your opportunity to select, process, and visualize the data of your choice, and receive feedback from your peers.  The project is not graded, and can be as simple or complex as you like. This week's assignment is to identify a data source and make a short discussion forum post describing the data source and outlining some possible analysis that could be done with it. You will not be required to use the data source presented here for your actual analysis. In our second required assignment, we will retrieve and process email data from the Sakai open source project. Video lectures will walk you through the process of retrieving, cleaning up, and modeling the data. The task for this week is to make a discussion thread post that reflects the progress you have made to date in retrieving and cleaning up your data source so can perform your analysis.  Feedback from other students is encouraged to help you refine the process. In the final required assignment, we will do two visualizations of the email data you have retrieved and processed: a word cloud to visualize the frequency distribution and a timeline to show how the data is changing over time. This week you will discuss the analysis of your data to the class. While many of the projects will result in a visualization of the data, any other results of analyzing the data are equally valued, so use whatever form of analysis and display is most appropriate to the data set you have selected.", "If you\u00e2\u0080\u0099ve ever skipped over`the results section of a medical paper because terms like \u00e2\u0080\u009cconfidence interval\u00e2\u0080\u009d or \u00e2\u0080\u009cp-value\u00e2\u0080\u009d go over your head, then you\u00e2\u0080\u0099re in the right place. You may be a clinical practitioner reading research articles to keep up-to-date with developments in your field or a medical student wondering how to approach your own research. Greater confidence in understanding statistical analysis and the results can benefit both working professionals and those undertaking research themselves. \n\nIf you are simply interested in properly understanding the published literature or if you are embarking on conducting your own research, this course is your first step. It offers an easy entry into interpreting common statistical concepts without getting into nitty-gritty mathematical formulae. To be able to interpret and understand these concepts is the best way to start your journey into the world of clinical literature. That\u00e2\u0080\u0099s where this course comes in - so let\u00e2\u0080\u0099s get started!\n\nThe course is free to enroll and take. You will be offered the option of purchasing a certificate of completion which you become eligible for, if you successfully complete the course requirements. This can be an excellent way of staying motivated!  Financial Aid is also available. Getting things started by defining study types Describing your data  Building an intuitive understanding of statistical analysis The important first steps: Hypothesis testing and confidence levels Which test should you use? Categorical data and analyzing accuracy of results  Welcome to the first week of this course. We\u00e2\u0080\u0099ll be tackling five broad topics to provide you with an intuitive understanding of clinical research results. This isn\u00e2\u0080\u0099t a comprehensive statistics course, but it offers a practical orientation to the field of medical research and commonly used statistical analysis. The first topics will look at research methods and the collection of data - with a specific focus on study types. By the end of the lectures you should be able to identify which study types are being used and why the researchers selected them when you are reading a paper. With the next topics, we finally get started with the statistics. Have you ever looked at the methods and results section of any healthcare research publication and noted the variety of statistical tests used?  You would have come across terms like t-test, Mann-Whitney-U test, Wilcoxon test, Fisher\u00e2\u0080\u0099s exact test and the ubiquitous chi-squared test.  Why so many tests you might wonder?  It\u00e2\u0080\u0099s all about types of data.  In this week,  I am going to tackle the differences in data which determine what type of statistical test we can use in making sense of our data. There is hardly any healthcare professional who is unfamiliar with the p-value.  It is usually understood to have a watershed value of 0.05.  If a research question is evaluated through the collection of data points and statistical analysis reveals a value less that 0.05, we accept this a proof that some significant difference was found, at least statistically.In reality things are a bit more complicated than that.  The literature is currently full of questions about the ubiquitous p-vale and why it is not the panacea many of us have used it as. During this week you will develop an intuitive understanding of concept of a p-value. From there, I'll move on to the heart of probability theory, the Central Limit Theorem and data distribution. In general, a researcher has a question in mind that he or she needs an answer to.  Everyone might have an opinion on the question (or answer), but an investigator looks for the answer by designing an experiment and investigating the outcome. In the first lesson we will look at hypotheses and how they relate to ethical and unbiased research and reporting.We'll also tackle Confidence intervals which I believe are one of the least understood and often misrepresented values in healthcare research. The most common tests used in the literature to compare numerical data point values are t-tests, analysis of variance, and linear regression.  In the last lesson we take a closer look at these tests, but perhaps more importantly, their strict assumptions.  The most common statistical test that you might come across in the literature is the t-test.  There are, in actual fact, a few t-tests, but the one most are familiar with, is of course, Student\u00e2\u0080\u0099s t-test and its ubiquitous p-value.  Not everyone, though, knows that the name Student was actually a pseudonym, used by William Gosset (1876 - 1937).  Parametric tests have very strict assumptions that must be met before their use is justified.  In this lesson we take a closer look at these tests, but perhaps more importantly, their strict assumptions.  Once you know these, you will be able to identify when these tests are used inappropriately. Congratulations! You've reached the final week of the course Understanding Clinical Research. In this lesson we will take a look at how good tests are at picking up the presence or absence of disease, helping us choose appropriate tests, and how to interpret positive and negative results.  We\u00e2\u0080\u0099ll decipher sensitivity, specificity, positive and negative predictive values. You'll end of this course with a final exam, to test the knowledge and application you've learned in this course. I hope you've enjoyed this course and it helps your understanding of clinical research. ", "In this course, you will learn best practices for how to use data analytics to make any company more competitive and more profitable. You will be able to recognize the most critical business metrics and distinguish them from mere data.\n \nYou\u00e2\u0080\u0099ll get a clear picture of the vital but different roles business analysts, business data analysts, and data scientists each play in various types of companies. And you\u00e2\u0080\u0099ll know exactly what skills are required to be hired for, and succeed at, these high-demand jobs.\n \nFinally, you will be able to use a checklist provided in the course to score any company on how effectively it is embracing big data culture. Digital companies like Amazon, Uber and Airbnb are transforming entire industries through their creative use of big data. You\u00e2\u0080\u0099ll understand why these companies are so disruptive and how they use data-analytics techniques to out-compete traditional companies. About This Specialization and Course  Introducing Business Metrics Working in the Business Data Analytics Marketplace Going Deeper into Business Metrics  Applying Business Metrics to a Business Case Study This Coursera Specialization: Excel to MySQL: Analytic Techniques for Business, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided. In the first course of the specialization: Business Metrics for Data-Driven Companies, you will be able to learn best practices for using data analytics to make any company more competitive and more profitable; learn to recognize the most critical business metrics and distinguish those from mere data; understand the vital but different roles business analysts, business data analysts, and data scientists each play in various types of companies; and know exactly the skills required to be hired for, and succeed at, these high-demand jobs. Finally, using a 20-item checklist for evaluating a business, you'll be able to score any company on how effectively it is embracing big data culture. Digital companies like Amazon, Uber and Airbnb are transforming entire industries through their creative use of big data. You\u00e2\u0080\u0099ll understand why these companies are so disruptive, and how they use data-analytics techniques to out-compete traditional companies.To get started, please begin with the video 'About This Specialization.'I hope you enjoy this week's materials! Welcome! This week we will explore business metrics - the critical numbers that help companies figure out how to survive and thrive. Inside every pile of data is a vital metric trying to get out! By the end of this week, you will be able to: distinguish business metrics from mere business data; identify critical business metrics such as cash flow, profitability, and online retail marketing metrics;  distinguish revenue, profitability and risk metrics; and distinguish traditional from dynamic metrics. Included in this week\u00e2\u0080\u0099s course materials is a Cash Flow and P&L statement for Egger\u00e2\u0080\u0099s Roast Coffee, as a supplemental document, so be sure to review it carefully and refer to the glossary for key information.  Welcome!  This week, we will meet some great people - all former students of mine - now working at super-interesting and exciting jobs as business analysts, business data analysts, or data scientists. We\u00e2\u0080\u0099ll explore what they do, how their role relates to big data, and the skills they needed to get hired! Our hope is this information will give you a better understanding of the type of data-related job you might apply for once you've completed this specialization, and a sense of the type of company you would find most appealing to work for. By the end of this week, you will be able to:  differentiate among different job roles within a company that work with data; identify how each role works with data; and describe the skills required to perform each job role. You will differentiate how different types of companies relate to big data culture, and rank any company according to a 20-item checklist. You will also learn to differentiate how different types of companies relate to big data culture. Included in this week\u00e2\u0080\u0099s materials is a 20-item checklist to rank companies. This week also includes in-video polls so you can see how others are ranking their businesses. Welcome! This week we\u00e2\u0080\u0099re going to go deeper into the critically-important metrics for web marketing - metrics every type of business needs to understand in order to survive. We\u00e2\u0080\u0099ll dive into the 'vertical' market of financial services - where digital companies are threatening to take away the market from traditional 'brick-and- mortar' companies.By the end of this week, you will be able to: Identify critical business metrics for all companies engaged in web-based marketing; and identify critical business metrics for financial services companies. You\u00e2\u0080\u0099ll find additional website links that expand some of the course materials covered in this week\u00e2\u0080\u0099s video lectures.  This week contains the final course assignment, a peer assessment in which you will identify business metrics of interest in a case example, describe those metrics, and propose a business process change that could be supported by the metric chosen.", "This course explores Excel as a tool for solving business problems. In this course you will learn the basic functions of excel through guided demonstration. Each week you will build on your excel skills and be provided an opportunity to practice what you\u00e2\u0080\u0099ve learned. Finally, you will have a chance to put your knowledge to work in a final project.  Please note, the content in this course was developed using a Windows version of Excel 2013.  \n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. Overview of Excel vLookups and Data Cleansing Logical Functions & Pivot Tables More Advanced Formulas In this module, you will learn the basics of Excel navigation and Excel basic functionality. You will learn how to navigate the basic Excel screen including using formulas, subtotals and text formatting. We will provide you an opportunity to perform a problem solving exercise using basic Excel skills. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen. In this module you will learn about VLookup, value cleansing and text functions. We will also introduce you to PwC's perspective on the value in cleansing data and using the appropriate functions.  Finally, we will provide you an opportunity to perform a problem solving exercise using VLookup, value cleansing and text function. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen. In this module, you will learn about logical functions and pivot tables. We will show you how to create and use pivot tables to solve business problems. We will give you an opportunity to practice creating and using a pivot table to solve a business problem. Finally, we will share some insight on PwC\u00e2\u0080\u0099s perspectives on the impact of Excel on your career and work. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen. In this module you will learn more advanced Excel formulas. We will show you how to create statistical formulas, perform an index match, and lastly, build financial formulas. We will provide you with an opportunity to problem solve using statistical formulas. Finally, we will give you an opportunity to practice what you have learned through a final project. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen.", "Ever wonder how major search engines such as Google, Bing and Yahoo rank your website within their searches? Or how content such as videos or local listings are shown and ranked based on what the search engine considers most relevant to users? Welcome to the world of Search Engine Optimization (SEO). This course is the first within the SEO Specialization and it is intended to give you a taste of SEO. \n\nYou will be introduced to the foundational elements of how search engines work, how the SEO landscape has changed and what you can expect in the future. You discuss core SEO strategies and tactics used to drive more organic search results to a specific website or set of websites, as well as tactics to avoid to prevent penalization from search engines. You will also discover how to position yourself for a successful career in SEO should this subject prove interesting to you. We hope this taste of SEO, will entice you to continue through the Specialization. Getting Started & Introduction to SEO Evolution of SEO Current SEO Best Practices SEO of Today, Tomorrow and Beyond & Course Wrap-Up Welcome to the first week of the course! In the lessons that follow, you will discuss and define the foundational overview of SEO and explore types of careers and salary expectations within SEO industry. By the end of this module, you should be able to define Search Engine Optimization and explain the basics of SEO as a business (as well as how SEO shapes the Internet itself.) You'll also be able to explain the differences between three main SEO job types, and be prepared to choose a career that best suits your current goals.  Welcome to Module 2! We hope you're enjoying the course so far, and are ready to learn more about the evolution of SEO! By the end of this module, you should be able to at least summarize the timeline of search engine development, as well as demonstrate an understanding of key time periods and individuals who changed the way search worked and the way humans interact with the Web. Later on in the module, you'll gain the ability to critique the role of advertisements and corporate funding in the development of search, and explain the process by which the Web became monetized. This high-level look at the history of SEO will give you a solid foundation for understanding the material we'll be covering later in the course.  In Module 3, we'll be discussing items that SEOs spend a great deal of time dealing with: SEO best practices, the algorithm updates that look for them and potential penalties for not adhering to them. By the end of this module, you'll be able to illustrate the concept of relevancy as it applies to search results, compare and contrast the functionality of search engine algorithm updates, and critically examine the ways in which webmasters attempt to circumvent these algorithms. You'll also be able to define important ranking factors used by modern search engines, and learn the necessary steps to avoid (or correct) any penalties applied by search engine algorithms.  You've made it to Module 4! In this module, you will gain an understanding of where SEO fits into the broader digital marketing landscape. By the end of this module, you will be able to explain how concepts like topic association and semantic analysis relate to the relevancy and trustworthiness of search results. You'll also be well prepared to write and optimize your own content for a website that will improve search results, as well as develop an optimization strategy for a client to implement that would help to increase their ranking while following best practices. Finally, by the end of this module you will be able to demonstrate the impact of brands and branding on search results, and critically analyze the role of social media and other emerging technologies on the landscape of SEO today, tomorrow, and beyond!", "This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach. In particular, the Bayesian approach allows for better accounting of uncertainty, results that have more intuitive and interpretable meaning, and more explicit statements of assumptions. This course combines lecture videos, computer demonstrations, readings, exercises, and discussion boards to create an active learning experience. For computing, you have the choice of using Microsoft Excel or the open-source, freely available statistical package R, with equivalent content for both options. The lectures provide some of the basic mathematical development as well as explanations of philosophy and interpretation. Completion of this course will give you an understanding of the concepts of the Bayesian approach, understanding the key differences between Bayesian and Frequentist approaches, and the ability to do basic data analyses. Probability and Bayes' Theorem Statistical Inference Priors and Models for Discrete Data Models for Continuous Data In this module, we review the basics of probability and Bayes\u00e2\u0080\u0099 theorem. In Lesson 1, we introduce the different paradigms or definitions of probability and discuss why probability provides a coherent framework for dealing with uncertainty. In Lesson 2, we review the rules of conditional probability and introduce Bayes\u00e2\u0080\u0099 theorem. Lesson 3 reviews common probability distributions for discrete and continuous random variables. This module introduces concepts of statistical inference from both frequentist and Bayesian perspectives. Lesson 4 takes the frequentist view, demonstrating maximum likelihood estimation and confidence intervals for binomial data. Lesson 5 introduces the fundamentals of Bayesian inference. Beginning with a binomial likelihood and prior probabilities for simple hypotheses, you will learn how to use Bayes\u00e2\u0080\u0099 theorem to update the prior with data to obtain posterior probabilities. This framework is extended with the continuous version of Bayes theorem to estimate continuous model parameters, and calculate posterior probabilities and credible intervals. In this module, you will learn methods for selecting prior distributions and building models for discrete data. Lesson 6 introduces prior selection and predictive distributions as a means of evaluating priors. Lesson 7 demonstrates Bayesian analysis of Bernoulli data and introduces the computationally convenient concept of conjugate priors. Lesson 8 builds a conjugate model for Poisson data and discusses strategies for selection of prior hyperparameters. This module covers conjugate and objective Bayesian analysis for continuous data. Lesson 9 presents the conjugate model for exponentially distributed data. Lesson 10 discusses models for normally distributed data, which play a central role in statistics. In Lesson 11, we return to prior selection and discuss \u00e2\u0080\u0098objective\u00e2\u0080\u0099 or \u00e2\u0080\u0098non-informative\u00e2\u0080\u0099 priors. Lesson 12 presents Bayesian linear regression with non-informative priors, which yield results comparable to those of classical regression.\n", "Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, from databases and from colleagues in various formats. It will also cover the basics of data cleaning and how to make data \u00e2\u0080\u009ctidy\u00e2\u0080\u009d. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data. Week 1 Week 2 Week 3 Week 4 In this first week of the course, we look at finding data and reading different file types. Welcome to Week 2 of Getting and Cleaning Data! The primary goal is to introduce you to the most common data storage systems and the appropriate tools to extract data from web or from databases like MySQL.  Welcome to Week 3 of Getting and Cleaning Data! This week the lectures will focus on organizing, merging and managing the data you have collected using the lectures from Weeks 1 and 2.  Welcome to Week 4 of Getting and Cleaning Data! This week we finish up with lectures on text and date manipulation in R. In this final week we will also focus on peer grading of Course Projects. \n", "This course will introduce the learner to text mining and text manipulation basics. The course begins with an understanding of how text is handled by python, the structure of text both to the machine and to humans, and an overview of the nltk framework for manipulating text. The second week focuses on common manipulation needs, including regular expressions (searching for text), cleaning text, and preparing text for use by machine learning processes. The third week will apply basic natural language processing methods to text, and demonstrate how text classification is accomplished. The final week will explore more advanced methods for detecting the topics in documents and grouping them by similarity (topic modelling). \n\nThis course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python. Module 1: Working with Text in Python Module 2: Basic Natural Language Processing Module 3: Classification of Text Module 4: Topic Modeling    ", "Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces across the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. In this third course of our Excel specialization Excel Skills for Business you will delve deeper into some of the most powerful features Excel has to offer. When you have successfully completed the course you will be able to\n\nCheck for and prevent errors in spreadsheets; \nCreate powerful automation in spreadsheets; \nApply advanced formulas and conditional logic to help make informed business decisions; and\nCreate spreadsheets that help forecast and model data. \n\nOnce again, we have brought together a great teaching team that will be with you every step of the way. Nicky, Prashan and myself will guide you through each week. As we are exploring these more advanced topics, we are following Alex who is an Excel consultant called in by businesses that experience issues with their spreadsheets. Data Validation Conditional Logic Automating Lookups Formula Auditing and Protection Data Modelling Recording Macros Final Assessment We kick off this course with data validation and conditional formatting. This module takes you through creating and applying data validation, as well as working with formulas in data validation. This is followed by basic and advanced conditional formatting. Excel has several logical functions and this module explores some of them. Start by learning the concept of conditional logic in formulas, followed by how to conduct logic tests and use conditional operations, to your benefit. We will also look at nested IF functions to evaluate data. How do you find information from different parts of a workbook? This module introduces you to functions like CHOOSE, VLOOKUP, INDEX, MATCH and other dynamic lookups to find and display data from several sources. If you are worried that errors have crept into your worksheet, this module will show you how to check for errors, trace precedents and dependents, resolve circular references, and finally, protect your worksheets and workbooks from further harm. This module is all about data modelling. Learn to model different scenarios based on input, assumptions and/or outcomes. Also learn the use of functionalities like Goal Seek, Data Tables and the Scenario Manager to make your models more robust. We all love a bit of automation, and this module teaches you just that. By the time you have completed this module, you will be able to identify the uses of macros, as well as create, edit and manage them to increase your efficiency. ", "Welcome!\nDo you wish to know how to analyze and solve business and economic questions with data analysis tools? Then Econometrics by Erasmus University Rotterdam is the right course for you, as you learn how to translate data into models to make forecasts and to support decision making.\n\n* What do I learn?\nWhen you know econometrics, you are able to translate data into models to make forecasts and to support decision making in a wide variety of fields, ranging from macroeconomics to finance and marketing. Our course starts with introductory lectures on simple and multiple regression, followed by topics of special interest to deal with model specification, endogenous variables, binary choice data, and time series data.  You learn these key topics in econometrics by watching the videos with in-video quizzes and by making post-video training exercises. \n\n* Do I need prior knowledge?\nThe course is suitable for (advanced undergraduate) students in economics, finance, business, engineering, and data analysis, as well as for those who work in these fields. The course requires some basics of matrices, probability, and statistics, which are reviewed in the Building Blocks module. If you are searching for a MOOC on econometrics of a more introductory nature that needs less background in mathematics, you may be interested in the Coursera course \u00e2\u0080\u009cEnjoyable Econometrics\u00e2\u0080\u009d that is also from Erasmus University Rotterdam.\n\n* What literature can I consult to support my studies?\nYou can follow the MOOC without studying additional sources. Further reading of the discussed topics (including the Building Blocks) is provided in the textbook that we wrote and on which the MOOC is based: Econometric Methods with Applications in Business and Economics, Oxford University Press. The connection between the MOOC modules and the book chapters is shown in the Course Guide \u00e2\u0080\u0093 Further Information \u00e2\u0080\u0093 How can I continue my studies.\n\n* Will there be teaching assistants active to guide me through the course?\nStaff and PhD students of our Econometric Institute will provide guidance in January and February of each year. In other periods, we provide only elementary guidance. We always advise you to connect with fellow learners of this course to discuss topics and exercises.\n\n* How will I get a certificate?\nTo gain the certificate of this course, you are asked to make six Test Exercises (one per module) and a Case Project. Further, you perform peer-reviewing activities of the work of three of your fellow learners of this MOOC. You gain the certificate if you pass all seven assignments.\n\nHave a nice journey into the world of Econometrics!\nThe Econometrics team Welcome Module Simple Regression Multiple Regression Model Specification Endogeneity Binary Choice Time Series Case Project OPTIONAL: Building Blocks Welcome!                                                                                                                                                                                          \n                                                                                                                                                                                                           \nDo you wish to know how to analyze and solve business and economic questions with data analysis tools? Then Econometrics by Erasmus University Rotterdam is the right course for you, as you learn how to translate data into models to make forecasts and to support decision making.                                                                                              \n                                                                                                                                                                                                       What do I learn?                                                                                                                                                                         \n                                                                                                                                                                                                      \nWhen you know econometrics, you are able to translate data into models to make forecasts and to support decision making in a wide variety of fields, ranging from macroeconomics to finance and marketing. Our course starts with introductory lectures on simple and multiple regression, followed by topics of special interest to deal with model specification, endogenous variables, binary choice data, and time series data.  You learn these key topics in econometrics by watching the videos with in-video quizzes and by making post-video training exercises.                  \n                                                                                                                                                                                                           \nDo I need prior knowledge?                                                                                                                                                         \n                                                                                                                                                                                                          \nThe course is suitable for (advanced undergraduate) students in economics, finance, business, engineering, and data analysis, as well as for those who work in these fields. The course requires some basics of matrices, probability, and statistics, which are reviewed in the Building Block module. If you are searching for a MOOC on econometrics of a more introductory nature that needs less background in mathematics, you may be interested in the Coursera course \u00e2\u0080\u009cEnjoyable Econometrics\u00e2\u0080\u009d that is also from Erasmus University Rotterdam.                                                         \n                                                                                                                                                                                                          \nWill there be teaching assistants active to guide me through the course?                                                                          \n                                                                                                                                                                                                           We advise you to connect with fellow learners of this course to discuss topics and exercises. Staff of our Econometric Institute provides guidance in January and February of each year.                                                            \n                                                                                                                                                                                                        \nHow will I get a certificate?                                                                                                                                                             \n                                                                                                                                                                                                            To gain the certificate of this course, you are asked to make six Test Exercises (one per module) and a Case Project. Further, you perform peer-reviewing activities of the work of three of your fellow learners of this MOOC. You gain the certificate if your average grade score is at least 50%.                                                                                                  \n                                                                                                                                                                                                       \nHave a nice journey into the world of Econometrics!                                                                                                             \n                                                                                                                                                                                                          The Econometrics team       This Case Project is the final assignment of our MOOC. It is of an applied nature, and it asks you to answer practical questions by means of econometric methods. By doing the case, you will integrate various econometric methods and skills that were trained in our MOOC.  By studying this module, you get the required background on matrices, probability and statistics. Each topic is illustrated with simple examples, and you get hands-on training by doing the training exercise that concludes each lecture. Three lectures on matrices show you the basic terminology and properties of matrices, including transpose, trace, rank, inverse, and positive definiteness. Two lectures on probability teach you the basics of univariate and multivariate probability distributions, especially the normal and associated distributions, including mean, variance, and covariance. Finally, two lectures on statistics present you with the basic ideas of statistical inference, in particular parameter estimation and testing, including the use of matrix methods and probability methods. ", "In this course, learners will be introduced to the field of statistics, including where data come from, study design, data management, and exploring and visualizing data. Learners will identify different types of data, and learn how to visualize, analyze, and interpret summaries for both univariate and multivariate data. Learners will also be introduced to the differences between probability and non-probability sampling from larger populations, the idea of how sample estimates vary, and how inferences can be made about larger populations based on probability sampling.\n\nAt the end of each week, learners will apply the statistical concepts they\u00e2\u0080\u0099ve learned using Python within the course environment. During these lab-based sessions, learners will discover the different uses of Python as a tool, including the Numpy, Pandas, Statsmodels, Matplotlib, and Seaborn libraries. Tutorial videos are provided to walk learners through the creation of visualizations and data management, all within Python. This course utilizes the Jupyter Notebook environment within Coursera. WEEK 1 - INTRODUCTION TO DATA WEEK 2 - UNIVARIATE DATA WEEK 3 - MULTIVARIATE DATA WEEK 4 - POPULATIONS AND SAMPLES In the first week of the course, we will review a course outline and discover the various concepts and objectives to be mastered in the weeks to come. You will get an introduction to the field of statistics and explore a variety of perspectives the field has to offer. We will identify numerous types of data that exist and observe where they can be found in everyday life. You will delve into basic Python functionality, along with an introduction to Jupyter Notebook. All of the course information on grading, prerequisites, and expectations are on the course syllabus and you can find more information on our Course Resources page. In the second week of this course, we will be looking at graphical and numerical interpretations for one variable (univariate data). In particular, we will be creating and analyzing histograms, box plots, and numerical summaries of our data in order to give a basis of analysis for quantitative data and bar charts and pie charts for categorical data. A few key interpretations will be made about our numerical summaries such as mean, IQR, and standard deviation. An assessment is included at the end of the week concerning numerical summaries and interpretations of these summaries. In the third week of this course on looking at data, we\u00e2\u0080\u0099ll introduce key ideas for examining research questions that require looking at more than one variable.  In particular, we will consider both numerically and visually how different variables interact, how summaries can appear deceiving if you don\u00e2\u0080\u0099t properly account for interactions, and differences between quantitative and categorical variables.  This week\u00e2\u0080\u0099s assignment will consist of a writing assignment along with reviewing those of your peers. In this week, you\u00e2\u0080\u0099ll spend more time thinking about where data come from. The highest-quality statistical analyses of data will always incorporate information about the process used to generate the data, or features of the data collection design. You\u00e2\u0080\u0099ll be exposed to important concepts related to sampling from larger populations, including probability and non-probability sampling, and how we can make inferences about larger populations based on well-designed samples. You\u00e2\u0080\u0099ll also learn about the concept of a sampling distribution, and how estimation of the variance of that distribution plays a critical role in making statements about populations. Finally, you\u00e2\u0080\u0099ll learn about the importance of reading the documentation for a given data set; a key step in looking at data is also looking at the available documentation for that data set, which describes how the data were generated.\n", "One of the skills that characterizes great business data analysts is the ability to communicate practical implications of quantitative analyses to any kind of audience member.  Even the most sophisticated statistical analyses are not useful to a business if they do not lead to actionable advice, or if the answers to those business questions are not conveyed in a way that non-technical people can understand.  \n\nIn this course you will learn how to become a master at communicating business-relevant implications of data analyses.  By the end, you will know how to structure your data analysis projects to ensure the fruits of your hard labor yield results for your stakeholders.  You will also know how to streamline your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the business world.  Using other Tableau features, you will be able to make effective visualizations that harness the human brain\u00e2\u0080\u0099s innate perceptual and cognitive tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting business \u00e2\u0080\u009cdata stories\u00e2\u0080\u009d that use these visualizations, capitalizing on business-tested methods and design principles. About this Specialization and Course Asking The \"Right Questions\" Data Visualization with Tableau Dynamic Data Manipulation and Presentation in Tableau Your Communication Toolbox: Visualizations, Logic, and Stories Final Project <p>The Coursera Specialization: <a href='https://www.coursera.org/specializations/excel-mysql' target='_blank'>Excel to MySQL: Analytic Techniques for Business</a>, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided.<p>In the third course of the specialization: <b>Data Visualization and Communication with Tableau</b>, you will learn how to communicate business-relevant implications of data analyses.  <p>Specifically, you will:<ul><li>craft the right questions to ensure your analysis projects succeed;</li> <li>leverage questions to design logical and structured analysis plans;</li> <li>create the most important graphs used in business analysis and transform data in Tableau;</li><li>design business dashboards with Tableau;</li> <li>tell stories with data;</li><li>design effective slide presentations to showcase your data story; and </li><li>deliver compelling business presentations.</li></ul> <p>By the end of this course, you will know how to structure your data analysis projects to ensure the fruits of your hard labor yield results for your stakeholders.  You will also know how to streamline your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the business world.  Using other Tableau features, you will be able to make effective visualizations that harness the human brain\u00e2\u0080\u0099s innate perceptual and cognitive tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting business \u00e2\u0080\u009cdata stories\u00e2\u0080\u009d that use these visualizations, capitalizing on business-tested methods and design principles by completing a final peer assessed project recommending a business process change. <P>To get started, please begin with the video 'About This Specialization.'<P>I hope you enjoy this week's materials!</P> Welcome! This week, you will learn how data analysts ask the right questions to ensure project success. By the end of this week, you will be able to: <p><ul><li>Craft the right questions to ensure your analysis projects succeed</li><li> Leverage questions to design logical and structured analysis plans</ul></li> <p> Remember to refer back to the Additional Resources reading: Identifying and Eliciting Information from Stakeholders). In addition, you will complete a  graded quiz. <p> As always, if you have any questions, post them to the <b>Discussions.</b> <p>To get started, please begin with the video \u00e2\u0080\u009cTips for Becoming a Data Analyst.\u00e2\u0080\u009d <p>I hope you enjoy this week's materials!</p> Welcome to week 2! This week you'll install Tableau Desktop to learn how visualizing data helps you figure out what your data mean efficiently, and in the process of doing so, helps you narrow in on what factors you should take into consideration in your statistical models or predictive algorithms.  Over the next two weeks, we\u00e2\u0080\u0099re going to learn how to use Tableau to implement this type of visualization and to help you find, and communicate, answers to business questions, as well as work with the Tableau functions that all data analysts should be familiar with.  You will learn to install Tableau Desktop and learn to use the program by working with two data sets. In addition, through a series of practice exercises, you will use a data set to do example analyses and to answer specific sample questions about salaries for certain data-related jobs across the United State. Then for graded exercises, you will use a different data set to work out analyses and questions that will require you to directly apply the Tableau skills you have acquired through practice. <p>By the end of this week, you will be able to: <ul><li>Create the most important graphs used in business analysis and transform data in Tableau </li></ul><p>Once you have watched the \"Why Tableau\" video, review the \"Written Instructions to install Tableau Desktop\" and install the software. Remember to refer back to the Salary Data Set and to the Dognition Data Set resources posted on the course site this week. You will also complete a graded quiz at the end of the week. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p>To get started, please begin with the video \u00e2\u0080\u009cUse Data Visualization to Drive Your Analysis\" and then review the \"Written Instructions to install Tableau Desktop.<p>I hope you enjoy this week's materials!  Welcome to week 3! This week you'll continue learning how to use Tableau to answer data analysis questions. You will learn how to use Tableau to both find, and eventually communicate answers to business questions. You'll learn about the process of elicitation, and learn how to ensure your data story is not undermined by overgeneralization or bias and how to format your data charts to begin creating a compelling data story.  By the end of this week, you will be able to: <ul><li>Write calculations and equations in Tableau</li> <li>Publish online business dashboards with Tableau.</li></ul> <p>Remember to refer  to the additional resources for this week: \u00e2\u0080\u009cExamples of Tableau Dashboards and Stories\u00e2\u0080\u009d and \"Using Tableau Dashboards When You Don't Have To.\" <p>You will also complete a graded quiz. <p>As always, if you have any questions, post them to the Discussions. <p>To get started, please begin with the video \u00e2\u0080\u009cCustomizing and Sharing New Data in Tableau.\u00e2\u0080\u009d <p>I hope you enjoy this week's materials! Welcome to week 4! This week you will become a master at getting people to agree with your data-driven business recommendations as you learn to deliver a compelling business presentation.  You\u00e2\u0080\u0099ll learn about the insight from the intersection of visualization science and decision science, and what this means for you as a  data analyst, who seeks to design a compelling and effective business presentations. If you intend to affect people\u00e2\u0080\u0099s decisions, you need to influence where they look. This week we will review  a set of tools and concepts you can use to optimize your visualizations and your presentation style. You will soon  be a master at getting people to agree with your data-driven business recommendations! <p>By the end of this week, you will be able to:<p><ul><li>Tell stories with data</li> <li>Design effective slide presentations to showcase your data story, and</li> <li> Deliver compelling business presentations</ul></li><p> Remember to refer back to the Study Guide: Designing and Delivering Effective Presentations. You will also complete a graded quiz. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p> To get started, please begin with the video \u00e2\u0080\u009cUsing Visualization to Influence Business Decisions.\u00e2\u0080\u009d<p> I hope you enjoy this week's materials! Welcome to week 5! This week you will complete your final project. This assignment requires you to submit a recording of yourself giving a 4-5 minute presentation in which you present a data-driven business process change proposal to Dognition company management about how to increase the numbers of tests users complete. Students will give a short, peer-reviewed business presentation that uses a specified chart in Tableau. The final project will assess your mastery of the following: <p> <ul><li>Demonstrated understanding the Tableau functions discussed in this course</li><li>Adapting visualizations to make them maximally communicative</li><li>Storyboarding skills</li> <li>Translating your story into a presentation ready for the boardroom</li><li>Effective presentation delivery</li><li>Evaluating business presentations</ul></li><p> Remember to refer to the Background Information for Peer Review Assignment  on the course web site before you begin. This final course project is a comprehensive assessment covering all of the course material and will take approximately 6-8 hours to complete.<p>As always, if you have any questions, post them to the Discussions. Thank you for your contributions to this  final project!<p> ", "This course is for users who want to learn how to write SAS programs to access, explore, prepare, and analyze data. It is the entry point to learning SAS programming for data science, machine learning, and artificial intelligence. It is a prerequisite to many other SAS courses.\n\nBy the end of this course, you will know how to use SAS Studio to write and submit SAS programs that access SAS, Microsoft Excel, and text data. You will know how to explore and validate data, prepare data by subsetting rows and computing new columns, analyze and report on data, export data and results to other formats, use SQL in SAS to query and join tables.\n\nPrerequisites:\nLearners should have experience using computer software. Specifically, you should be able to understand file structures and system commands on your operating systems and access data files on your operating systems. No prior SAS experience is needed. Course Overview and Data Setup Essentials Accessing Data Exploring and Validating Data Preparing Data Analyzing and Reporting on Data Exporting Results Using SQL in SAS In this module you learn about the course and you set up the data you need to do the practices in the course.  In this module you learn how to use SAS programming tools and the fundamentals of SAS program structure and syntax.  In this module, you learn to identify the features of a SAS table, access data through SAS libraries, and import data into SAS. In this module, you learn to use SAS procedures that provide insights about your data. You also learn to subset data so you can focus on particular segments, format data so you can easily understand it, and sort data to identify and resolve duplicate values. In this module, you learn how to do some common data manipulations, such as filtering rows and columns, computing new columns, and performing conditional processing. In this module, we concentrate on summarizing data by using the SAS procedures that we touched on for data exploration. You also learn how to use titles, column labels, footnotes, and macro variables to enhance your reports and make them more meaningful. In this module, you learn to export SAS tables and results to Excel, Microsoft Word, and PDF files.  In this module, you learn to use the SQL procedure to read and filter data. You also learn to create and join tables by using SQL.", "Welcome to Practical Time Series Analysis!\n\nMany of us are \"accidental\" data analysts. We trained in the sciences, business, or engineering and then found ourselves confronted with data for which we have no formal analytic training.  This course is designed for people with some technical competencies who would like more than a \"cookbook\" approach, but who still need to concentrate on the routine sorts of presentation and analysis that deepen the understanding of our professional topics. \n\nIn practical Time Series Analysis we look at data sets that represent sequential information, such as stock prices, annual rainfall, sunspot activity, the price of agricultural products, and more.  We look at several mathematical models that might be used to describe the processes which generate these types of data. We also look at graphical representations that provide insights into our data. Finally, we also learn how to make forecasts that say intelligent things about what we might expect in the future.\n\nPlease take a few minutes to explore the course site. You will find video lectures with supporting written materials as well as quizzes to help emphasize important points. The language for the course is R, a free implementation of the S language. It is a professional environment and fairly easy to learn.\n\nYou can discuss material from the course with your fellow learners. Please take a moment to introduce yourself!\n\nTime Series Analysis can take effort to learn- we have tried to present those ideas that are \"mission critical\" in a way where you understand enough of the math to fell satisfied while also being immediately productive. We hope you enjoy the class! WEEK 1: Basic Statistics Week 2: Visualizing Time Series, and Beginning to Model Time Series Week 3: Stationarity, MA(q) and AR(p) processes Week 4: AR(p) processes, Yule-Walker equations, PACF Week 5: Akaike Information Criterion (AIC), Mixed Models, Integrated Models Week 6: Seasonality, SARIMA, Forecasting During this first week, we show how to download and install R on Windows and the Mac. We review those basics of inferential and descriptive statistics that you'll need during the course. In this week, we begin to explore and visualize time series available as acquired data sets. We also take our first steps on developing the mathematical models needed to analyze time series data. In Week 3, we introduce few important notions in time series analysis: Stationarity, Backward shift operator, Invertibility, and Duality. We begin to explore Autoregressive processes and Yule-Walker equations.  In this week, partial autocorrelation is introduced. We work more on Yule-Walker equations, and apply what we have learned so far to few real-world datasets.   In Week 5, we start working with Akaike Information criterion as a tool to judge our models, introduce mixed models such as ARMA, ARIMA and model few real-world datasets.  In the last week of our course, another model is introduced: SARIMA. We fit SARIMA models to various datasets and start forecasting. ", "People apply Bayesian methods in many areas: from game development to drug discovery. They give superpowers to many machine learning algorithms: handling missing data, extracting much more information from small datasets. Bayesian methods also allow us to estimate uncertainty in predictions, which is a desirable feature for fields like medicine. \nWhen applied to deep learning, Bayesian methods allow you to compress your models a hundred folds, and automatically tune hyperparameters, saving your time and money.\nIn six weeks we will discuss the basics of Bayesian methods: from how to define a probabilistic model to how to make predictions from it. We will see how one can automate this workflow and how to speed it up using some advanced techniques. \nWe will also see applications of Bayesian methods to deep learning and how to generate new images with it. We will see how new drugs that cure severe diseases be found with Bayesian methods.\n\nDo you have technical problems? Write to us: coursera@hse.ru Introduction to Bayesian methods & Conjugate priors Expectation-Maximization algorithm Variational Inference & Latent Dirichlet Allocation Markov chain Monte Carlo Variational Autoencoder Gaussian processes & Bayesian optimization Final project Welcome to first week of our course! Today we will discuss what bayesian methods are and what are probabilistic models. We will see how they can be used to model real-life situations and how to make conclusions from them. We will also learn about conjugate priors \u00e2\u0080\u0094 a class of models where all math becomes really simple. This week we will about the central topic in probabilistic modeling: the Latent Variable Models and how to train them, namely the Expectation Maximization algorithm. We will see models for clustering and dimensionality reduction where Expectation Maximization algorithm can be applied as is. In the following weeks, we will spend weeks 3, 4, and 5 discussing numerous extensions to this algorithm to make it work for more complicated models and scale to large datasets. This week we will move on to approximate inference methods. We will see why we care about approximating distributions and see variational inference \u00e2\u0080\u0094 one of the most powerful methods for this task. We will also see mean-field approximation in details. And apply it to text-mining algorithm called Latent Dirichlet Allocation This week we will learn how to approximate training and inference with sampling and how to sample from complicated distributions. This will allow us to build simple method to deal with LDA and with Bayesian Neural Networks \u00e2\u0080\u0094 Neural Networks which weights are random variables themselves and instead of training (finding the best value for the weights) we will sample from the posterior distributions on weights. Welcome to the fifth week of the course! This week we will combine many ideas from the previous weeks and add some new to build Variational Autoencoder -- a model that can learn a distribution over structured data (like photographs or molecules) and then sample new data points from the learned distribution, hallucinating new photographs of non-existing people. We will also the same techniques to Bayesian Neural Networks and will see how this can greatly compress the weights of the network without reducing the accuracy. Welcome to the final week of our course! This time we will see nonparametric Bayesian methods. Specifically, we will learn about Gaussian processes and their application to Bayesian optimization that allows one to perform optimization for scenarios in which each function evaluation is very expensive: oil probe, drug discovery and neural network architecture tuning. In this module you will apply methods that you learned in this course to this final project", "This course covers commonly used statistical inference methods for numerical and categorical data. You will learn how to set up and perform hypothesis tests, interpret p-values, and report the results of your analysis in a way that is interpretable for clients or the public. Using numerous data examples, you will learn to report estimates of quantities in a way that expresses the uncertainty of the quantity of interest. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The course introduces practical tools for performing data analysis and explores the fundamental concepts necessary to interpret and report results for both categorical and numerical data About the Specialization and the Course Central Limit Theorem and Confidence Interval Inference and Significance Inference for Comparing Means Inference for Proportions  Data Analysis Project This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Inferential Statistics. Please take several minutes to browse them through. Thanks for joining us in this course! Welcome to Inferential Statistics! In this course we will discuss Foundations for Inference. Check out the learning objectives, start watching the videos, and finally work on the quiz and the labs of this week. In addition to videos that introduce new concepts, you will also see a few videos that walk you through application examples related to the week's topics. In the first week we will introduce Central Limit Theorem (CLT) and confidence interval. Welcome to Week Two! This week we will discuss formal hypothesis testing and relate testing procedures back to estimation via confidence intervals. These topics will be introduced within the context of working with a population mean, however we will also give you a brief peek at what's to come in the next two weeks by discussing how the methods we're learning can be extended to other estimators. We will also discuss crucial considerations like decision errors and statistical vs. practical significance. The labs for this week will illustrate concepts of sampling distributions and confidence levels. Welcome to Week Three of the course! This week we will introduce the t-distribution and comparing means as well as a simulation based method for creating a confidence interval: bootstrapping. If you have questions or discussions, please use this week's forum to ask/discuss with peers. Welcome to Week Four of our course! In this unit, we\u00e2\u0080\u0099ll discuss inference for categorical data. We use methods introduced this week to answer questions like \u00e2\u0080\u009cWhat proportion of the American public approves of the job of the Supreme Court is doing?\u00e2\u0080\u009d. In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "This 1-week, accelerated course builds upon previous courses in the Data Engineering on Google Cloud Platform specialization. Through a combination of video lectures, demonstrations, and hands-on labs, you'll learn how to create and manage computing clusters to run Hadoop, Spark, Pig and/or Hive jobs on Google Cloud Platform.  You will also learn how to access various cloud storage options from their compute clusters and integrate Google\u00e2\u0080\u0099s machine learning capabilities into their analytics programs.  \n\nIn the hands-on labs, you will create and manage Dataproc Clusters using the Web Console and the CLI, and use cluster to run Spark and Pig jobs. You will then create iPython notebooks that integrate with BigQuery and storage and utilize Spark. Finally, you integrate the machine learning APIs into your data analysis.\n\nPre-requisites\n\u00e2\u0080\u00a2 Google Cloud Platform Big Data & Machine Learning Fundamentals (or equivalent experience)\n\u00e2\u0080\u00a2 Some knowledge of Python\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Module 1: Introduction to Cloud Dataproc Module 2: Running Dataproc jobs Module 3: Leveraging GCP Module 4: Analyzing Unstructured Data    ", "Database Management Essentials provides the foundation you need for a career in database development, data warehousing, or business intelligence, as well as for the entire Data Warehousing for Business Intelligence specialization. In this course, you will create relational databases, write SQL statements to extract information to satisfy business reporting requests, create entity relationship diagrams (ERDs) to design databases, and analyze table designs for excessive redundancy. As you develop these skills, you will use either Oracle or MySQL to execute SQL statements and a database diagramming tool such as the ER Assistant or Visual Paradigm to create ERDs. We\u00e2\u0080\u0099ve designed this course to ensure a common foundation for specialization learners. Everyone taking the course can jump right in with writing SQL statements in Oracle or MySQL. Course Introduction Introduction to Databases and DBMSs Relational Data Model and the CREATE TABLE Statement Basic Query Formulation with SQL Extended Query Formulation with SQL Notation for Entity Relationship Diagrams ERD Rules and Problem Solving Developing Business Data Models Data Modeling Problems and Completion of an ERD Schema Conversion Normalization Concepts and Practice Module 1 provides the context for Database Management Essentials. When you\u00e2\u0080\u0099re done, you\u00e2\u0080\u0099ll understand the objectives for the course and know what topics and assignments to expect. Keeping these course objectives in mind will help you succeed throughout the course! You should read about the database software requirements in the last lesson of module 1. I recommend that you try to install the DBMS software this week before assignments begin in week 2. We\u00e2\u0080\u0099ll launch into an exploration of databases and database technology and their impact on organizations in Module 2. We\u00e2\u0080\u0099ll investigate database characteristics, database technology features, including non-procedural access, two key processing environments, and an evolution of the database software industry. This short informational module will ensure that we all have the same background and context, which is critical for success in the later modules that emphasize details and hands-on skills.\n Now that you have the informational context for database features and environments, you\u00e2\u0080\u0099ll start building! In this module, you\u00e2\u0080\u0099ll learn relational data model terminology, integrity rules, and the CREATE TABLE statement. You\u00e2\u0080\u0099ll apply what you\u00e2\u0080\u0099ve learned in practice and graded problems using a database management system (DBMS), either Oracle or MySQL, creating tables using the SQL CREATE TABLE statement and populating your tables using given SQL INSERT statements.\n This module is all about acquiring query formulation skills. Now that you know the relational data model and have basic skills with the CREATE TABLE statement, we can cover basic syntax of the SQL SELECT statement and the join operator for combining tables. SELECT statement examples are presented for single table conditions, join operations, and grouping operations. You\u00e2\u0080\u0099ll practice writing simple SELECT statements using the tables that you created in the assignment for module 3.\n Now that you can identify and use the SELECT statement and the join operator, you\u00e2\u0080\u0099ll extend your problem solving skills in this module so you can gain confidence on more complex queries. You will work on retrieval problems with multiple tables and grouping. In addition, you\u00e2\u0080\u0099ll learn to use the UNION operator in the SQL SELECT statement and write SQL modification statements.\n Module 6 represents another shift in your learning. In previous modules, you\u00e2\u0080\u0099ve created and populated tables and developed query formulation skills using the SQL SELECT statement. Now you\u00e2\u0080\u0099ll start to develop skills that allow you to create a database design to support business requirements. You\u00e2\u0080\u0099ll learn basic notation used in entity relationship diagrams (ERDs), a graphical notation for data modeling. You will create simple ERDs using basic diagram symbols and relationship variations to start developing your data modeling skills. \n Module 7 builds on your knowledge of database development using basic ERD symbols and relationship variations. We\u00e2\u0080\u0099ll be practicing precise usage of ERD notation and basic problem solving skills. You will learn about diagram rules and work problems to help you gain confidence using and creating ERDs.\n In Module 8, you\u00e2\u0080\u0099ll use your ERD notation skills and your ability to avoid diagram errors to develop ERDs that satisfy specific business data requirements. You will learn and practice powerful problem-solving skills as you analyze narrative statements and transformations to generate alternative ERDs.\n Now that you have practiced data modeling techniques, you\u00e2\u0080\u0099ll get to wrestle with narrative problem analyses and transformations for generating alternative database designs in Module 9. At the end of this module, you\u00e2\u0080\u0099ll learn guidelines for documentation and detection of design errors that will serve you well as you design databases for business situations.\n Modules 6 to 9 covered conceptual data modeling, emphasizing precise usage of ERD notation, analysis of narrative problems, and generation of alternative designs. Modules 10 and 11 cover logical database design, the next step in the database development process. In Module 10, we\u00e2\u0080\u0099ll cover schema conversion, the first step in the logical database design phase. You will learn to convert an ERD into a table design that can be implemented on a relational DBMS.\n Module 11 covers normalization, the second part of the logical database design process. Normalization provides tools to remove unwanted redundancy in a table design. You\u00e2\u0080\u0099ll discover the motivation for normalization, constraints to reason about unwanted redundancy, and rules that detect excessive redundancy in a table design. You\u00e2\u0080\u0099ll practice integrating and applying normalization techniques in the final lesson of this course.\n", "Statistical inference is the process of drawing conclusions about populations or scientific truths from data. There are many modes of performing inference including statistical modeling, data oriented strategies and explicit use of designs and randomization in analyses. Furthermore, there are broad theories (frequentists, Bayesian, likelihood, design based, \u00e2\u0080\u00a6) and numerous complexities (missing data, observed and unobserved confounding, biases) for performing inference. A practitioner can often be left in a debilitating maze of techniques, philosophies and nuance. This course presents the fundamentals of inference in a practical approach for getting things done. After taking this course, students will understand the broad directions of statistical inference and use this information for making informed choices in analyzing data. Week 1: Probability & Expected Values Week 2: Variability, Distribution, & Asymptotics Week: Intervals, Testing, & Pvalues Week 4: Power, Bootstrapping, & Permutation Tests This week, we'll focus on the fundamentals including probability, random variables, expectations and more.  We're going to tackle variability, distributions, limits, and confidence intervals. We will be taking a look at intervals, testing, and pvalues in this lesson. We will begin looking into power, bootstrapping, and permutation tests.", "Case Study - Predicting Housing Prices\n\nIn our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  This is just one of the many places where regression can be applied.  Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.\n\nIn this course, you will explore regularized linear regression models for the task of prediction and feature selection.  You will be able to handle very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions.  To fit these models, you will implement optimization algorithms that scale to large datasets.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Describe the input and output of a regression model.\n   -Compare and contrast bias and variance when modeling data.\n   -Estimate model parameters using optimization algorithms.\n   -Tune parameters with cross validation.\n   -Analyze the performance of the model.\n   -Describe the notion of sparsity and how LASSO leads to sparse solutions.\n   -Deploy methods to select between models.\n   -Exploit the model to form predictions. \n   -Build a regression model to predict prices using a housing dataset.\n   -Implement these techniques in Python. Welcome Simple Linear Regression Multiple Regression Assessing Performance Ridge Regression Feature Selection & Lasso Nearest Neighbors & Kernel Regression Closing Remarks Regression is one of the most important and broadly used machine learning and statistics tools out there.  It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response.  Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. Our course starts from the most basic regression model: Just fitting a line to data.  This simple model for forming predictions from a single, univariate feature of the data is appropriately called \"simple linear regression\".<p> In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent.  Based on this fitted function, you will interpret the estimated model parameters and form predictions.  You will also analyze the sensitivity of your fit to outlying observations.<p> You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house. The next step in moving beyond simple linear regression is to consider \"multiple regression\" where multiple features of the data are used to form predictions.  <p> More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., 'square feet') and the observed response (like 'house sales price').  This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value.  You will also learn how to incorporate multiple input variables (e.g., 'square feet', '# bedrooms', '# bathrooms').  You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple \"features\".   Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. <p>Here, you will also implement a gradient descent algorithm for fitting a multiple regression model. Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data.  You are also ready to select amongst possible models to choose the best performing.  <p> This module is all about these important topics of model selection and assessment.  You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the \"loss\" of your predictions, and use this to define training, test, and generalization error.  For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance.  This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning.  Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. <p>The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course. You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data.   In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue.  This method is called \"ridge regression\".  You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions.  To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective.  You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression.  To select the strength of the bias away from overfitting, you will explore a general-purpose method called \"cross validation\". <p>You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant. A fundamental machine learning task is to select amongst a set of features to include in a model.  In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions. <p> To start, you will examine methods that search over an enumeration of models including different subsets of features.  You will analyze both exhaustive search and greedy algorithms.  Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge.  This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. <p>Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning.  Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset.  In this module, we instead turn our attention to a class of \"nonparametric\" methods.  These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.  <p> We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set.  This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors.  Building on this idea, we turn to kernel regression.  Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point.  You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle.  You will also implement these techniques and observe their practical behavior. In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization.  We also briefly discuss some important regression techniques we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ", "Starting from a history of machine learning, we discuss why neural networks today perform so well in a variety of data science problems. We then discuss how to set up a supervised learning problem and find a good solution using gradient descent. This involves creating datasets that permit generalization; we talk about methods of doing so in a repeatable way that supports experimentation.\n\nCourse Objectives:\nIdentify why deep learning is currently popular\nOptimize and evaluate models using loss functions and performance metrics\nMitigate common problems that arise in machine learning\nCreate repeatable and scalable training, evaluation, and test datasets\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Practical ML Optimization Generalization and Sampling Summary In this course you\u00e2\u0080\u0099ll get foundational ML knowledge so that you understand the terminology that we use throughout the specialization. You will also learn practical tips and pitfalls from ML practitioners here at Google and walk away with the code and the knowledge to bootstrap your own ML models. In this module, we will introduce some of the main types of machine learning and review the history of ML leading up to the state of the art so that you can accelerate your growth as an ML practitioner. In this module we will walk you through how to optimize your ML models. Now it\u00e2\u0080\u0099s time to answer a rather weird question: when is the most accurate ML model not the right one to pick?  As we hinted at in the last module on Optimization -- simply because a model has a loss metric of 0 for your training dataset does not mean it will perform well on new data in the real world.  ", "Have you ever heard about such technologies as HDFS, MapReduce, Spark? Always wanted to learn these new tools but missed concise starting material? Don\u00e2\u0080\u0099t miss this course either!\n \nIn this 6-week course you will:\n- learn some basic technologies of the modern Big Data landscape, namely: HDFS, MapReduce and Spark;\n- be guided both through systems internals and their applications;\n- learn about distributed file systems, why they exist and what function they serve;\n- grasp the MapReduce framework, a workhorse for many modern Big Data applications;\n- apply the framework to process texts and solve sample business cases;\n- learn about Spark, the next-generation computational framework;\n- build a strong understanding of Spark basic concepts;\n- develop skills to apply these tools to creating solutions in finance, social networks, telecommunications and many other fields.\n\nYour learning experience will be as close to real life as possible with the chance to evaluate your practical assignments on a real cluster. No mocking, a friendly considerate atmosphere to make the process of your learning smooth and enjoyable.\n \nGet ready to work with real datasets alongside with real masters!\n\nSpecial thanks to:\n- Prof. Mikhail Roytberg, APT dept., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the road.\n- Oleg Sukhoroslov (PhD, Senior Researcher at IITP RAS), who has been teaching  MapReduce, Hadoop  and friends since 2008. Now he is leading the infrastructure team.\n- Oleg Ivchenko (PhD student APT dept., MIPT), Pavel Akhtyamov (MSc. student at APT dept., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl State University), superbrains who have developed and now maintain the infrastructure used for practical assignments in this course.\n- Asya Roitberg, Eugene Baulin, Marina Sudarikova. These people never sleep to babysit this course day and night, to make your learning experience productive, smooth and exciting. Welcome What are BigData and distributed file systems (e.g. HDFS)? Solving Problems with MapReduce Solving Problems with MapReduce (practice week)  Introduction to Apache Spark Introduction to Apache Spark (practice week) Real-World Applications       ", "People analytics is a data-driven approach to managing people at work. For the first time in history, business leaders can make decisions about their people based on deep analysis of data rather than the traditional methods of personal relationships, decision making based on experience, and risk avoidance. In this brand new course, three of Wharton\u00e2\u0080\u0099s top professors, all pioneers in the field of people analytics, will explore the state-of-the-art techniques used to recruit and retain great people, and demonstrate how these techniques are used at cutting-edge companies. They\u00e2\u0080\u0099ll explain how data and sophisticated analysis is brought to bear on people-related issues, such as recruiting, performance evaluation, leadership, hiring and promotion, job design, compensation, and collaboration. This course is an introduction to the theory of people analytics, and is not intended to prepare learners to perform complex talent management data analysis. By the end of this course, you\u00e2\u0080\u0099ll understand how and when hard data is used to make soft-skill decisions about hiring and talent development, so that you can position yourself as a strategic partner in your company\u00e2\u0080\u0099s talent management decisions. This course is intended to introduced you to Organizations flourish when the people who work in them flourish. Analytics can help make both happen. This course in People Analytics is designed to help you flourish in your career, too. Introduction to People Analytics, and Performance Evaluation Staffing Collaboration Talent Management and Future Directions In this module, you'll meet Professors Massey, Bidwell, and Haas, cover the structore and scope of the course, and dive into the first topic: Performance Evaluation. Performance evaluation plays an influential role in our work lives, whether it is used to reward or punish and/or to gather feedback. Yet its fundamental challenge is that the measures we used to evaluate performance are imperfect: we can't infer how hard or smart an employee is working based solely on outcomes. In this module, you\u00e2\u0080\u0099ll learn the four key issues in measuring performance: regression to the mean, sample size, signal independence, and process vs. outcome, and see them at work in current companies, including an extended example from the NFL. By the end of this module, you\u00e2\u0080\u0099ll understand how to separate skill from luck and learn to read noisy performance measures, so that you can go into your next performance evaluation sensitive to the role of chance, knowing your environment, and aware of the four most common biases, so that you can make more informed data-driven decisions about your company's most valuable asset: its employees. In this module, you'll learn how to use data to better analyze the key components of the staffing cycle: hiring, internal mobility and career development, and attrition. You'll explore different analytic approaches to predicting performance for hiring and for optimizing internal mobility, to understanding and reducing turnover, and to predicting attrition. You'll also learn the critical skill of understanding causality so that you can avoid using data incorrectly. By the end of this module, you'll be able to use data to improve the quality of the decisions you make in getting the right people into the right jobs and helping them stay there, to benefit not only your organization but also employee's individual careers.  In this module, you'll learn the basic principles behind using people analytics to improve collaboration between employees inside an organization so they can work together more successfully. You'll explore how data is used to describe, map, and evaluate collaboration networks, as well as how to intervene in collaboration networks to improve collaboration using examples from real-world companies. By the end of this module, you'll know how to deploy the tools and techniques of organizational network analysis to understand and improve collaboration patterns inside your organization to make your organization, and the people working within in it, more productive, effective, and successful.  In this module, you explore talent analytics: how data may be used in talent assessment and development to maximize employee ability. You'll learn how to use data to move from performance evaluation to a more deeper analysis of employee evaluation so that you may be able to improve the both the effectiveness and the equitability of the promotion process at your firm. By the end of this module, you'll will understand the four major challenges of talent analytics: context, interdependence, self-fulfilling prophecies, and reverse causality, the challenges of working with algorithms, and some practical tips for incorporating data sensitively, fairly, and effectively into your own talent assessment and development processes to make your employees and your organization more successful. In the course conclusion, you'll also learn the current challenges and future directions of the field of people analytics, so that you may begin putting employee data to work in a ways that are smarter, practical and more powerful.", "By now you have definitely heard about data science and big data. In this one-week class, we will provide a crash course in what these terms mean and how they play a role in successful organizations. This class is for anyone who wants to learn what all the data science action is about, including those who will eventually need to manage data scientists. The goal is to get you up to speed as quickly as possible on data science without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials.\n\nThis is a focused course designed to rapidly get you up to speed on the field of data science. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know. \n\n1. How to describe the role data science plays in various contexts\n2. How statistics, machine learning, and software engineering play a role in data science\n3. How to describe the structure of a data science project\n4. Know the key terms and tools used by data scientists\n5. How to identify a successful and an unsuccessful data science project\n3. The role of a data science manager\n\n\nCourse cover image by r2hox. Creative Commons BY-SA: https://flic.kr/p/gdMuhT A Crash Course in Data Science This one-module course constitutes the first \"week\" of the Executive Data Science Specialization. This is an intensive introduction to what you need to know about data science itself. You'll learn important terminology and how successful organizations use data science.", "Spreadsheet software remains one of the most ubiquitous pieces of software used in workplaces around the world. Learning to confidently operate this software means adding a highly valuable asset to your employability portfolio. Across the globe, millions of job advertisements requiring Excel skills are posted every day. At a time when digital skills jobs are growing much faster than non-digital jobs, completing this course will position you ahead of others, so keep reading.\n\nIn this last course of our Specialization Excel Skills for Business you will build on the strong foundations of the first three courses: Essentials, Intermediate I + II.  In the Advanced course, we will prepare you to become a power user of Excel - this is your last step before specializing at a professional level. The topics we have prepared will challenge you as you learn how to use advanced formula techniques and sophisticated lookups. You will clean and prepare data for analysis, and learn how to work with dates and financial functions. An in-depth look at spreadsheet design and documentation will prepare you for our big finale, where you will learn how to build professional dashboards in Excel. Spreadsheet Design and Documentation Advanced Formula Techniques Data Cleaning and Preparation Financial Functions and Working with Dates Advanced Lookup Functions Building Professional Dashboards Final Assessment As an intermediate Excel user, you have the basics down. The first module of the Advanced course focusses on creating and maintaining accurate, flexible, responsive and user-friendly spreadsheets. You will learn how to design flexible and auditable spreadsheets, construct transparent calculations, create user-friendly navigation, and use styles, themes and formatting to tie it all up in a nice bow. Excel often requires you to perform complex analysis on large sets of data. This can be made a lot easier using array formulas, which can perform multiple calculations simultaneously and ensure the accuracy of your spreadsheet. This module covers the use of array formulas, how to build them and use them effectively. Been handed a messy spreadsheet? This module focuses on data cleaning and preparation. You will learn how to use dynamic and automated fixes, clean messy data, automate these processes through functions, and ensure data remains clean dynamically. We will use functions to fix dates, replaces blanks and remove unwanted characters from data \u00e2\u0080\u0093 all to help us build an efficient solution that lasts. This module introduces financial functions into our Excel knowledge base. Financial functions are often used in businesses and Excel has a variety of tools to help you with that. This week, you will learn to apply date formulas in calculations, create loan schedules with financial functions, as well as use depreciation functions. It is always a good idea to make your spreadsheets (and work!) more automated. Automated lookups can make your work fast and efficient. This module builds on your array capabilities and explores a range of functions to create dynamic lookup ranges \u00e2\u0080\u0093 INDIRECT, ADDRESS, OFFSET and INDEX. To bring it home, this module guides you through the steps of building a professional dashboard. Data is meaningless unless it tells a story and can be used as business intelligence. This module will help you set up a dashboard \u00e2\u0080\u0093 learn how to visualise data through graphs and charts, create data models, and add interactivity. ", "This course provides an introduction to basic computational methods for understanding what nervous systems do and for determining how they function. We will explore the computational principles governing various aspects of vision, sensory-motor control, learning, and memory. Specific topics that will be covered include representation of information by spiking neurons, processing of information in neural networks, and algorithms for adaptation and learning. We will make use of Matlab/Octave/Python demonstrations and exercises to gain a deeper understanding of concepts and methods introduced in the course. The course is primarily aimed at third- or fourth-year undergraduates and beginning graduate students, as well as professionals and distance learners interested in learning how the brain processes information. Introduction & Basic Neurobiology (Rajesh Rao) What do Neurons Encode? Neural Encoding Models (Adrienne Fairhall) Extracting Information from Neurons: Neural Decoding (Adrienne Fairhall) Information Theory & Neural Coding (Adrienne Fairhall) Computing in Carbon (Adrienne Fairhall) Computing with Networks (Rajesh Rao) Networks that Learn: Plasticity in the Brain & Learning (Rajesh Rao) Learning from Supervision and Rewards (Rajesh Rao) This module includes an Introduction to Computational Neuroscience, along with a primer on Basic Neurobiology.  This module introduces you to the captivating world of neural information coding. You will learn about the technologies that are used to record brain activity. We will then develop some mathematical formulations that allow us to characterize spikes from neurons as a code, at increasing levels of detail. Finally we investigate variability and noise in the brain, and how our models can accommodate them. In this module, we turn the question of neural encoding around and ask: can we estimate what the brain is seeing, intending, or experiencing just from its neural activity? This is the problem of neural decoding and it is playing an increasingly important role in applications such as neuroprosthetics and brain-computer interfaces, where the interface must decode a person's movement intentions from neural activity. As a bonus for this module, you get to enjoy a guest lecture by well-known computational neuroscientist Fred Rieke.  This module will unravel the intimate connections between the venerable field of information theory and that equally venerable object called our brain. This module takes you into the world of biophysics of neurons, where you will meet one of the most famous mathematical models in neuroscience, the Hodgkin-Huxley model of action potential (spike) generation. We will also delve into other models of neurons and learn how to model a neuron's structure, including those intricate branches called dendrites. This module explores how models of neurons can be connected to create network models. The first lecture shows you how to model those remarkable connections between neurons called synapses. This lecture will leave you in the company of a simple network of integrate-and-fire neurons which follow each other or dance in synchrony. In the second lecture, you will learn about firing rate models and feedforward networks, which transform their inputs to outputs in a single \"feedforward\" pass. The last lecture takes you to the dynamic world of recurrent networks, which use feedback between neurons for amplification, memory, attention, oscillations, and more! This module investigates models of synaptic plasticity and learning in the brain, including a Canadian psychologist's prescient prescription for how neurons ought to learn (Hebbian learning) and the revelation that brains can do statistics (even if we ourselves sometimes cannot)! The next two lectures explore unsupervised learning and theories of brain function based on sparse coding and predictive coding. In this last module, we explore supervised learning and reinforcement learning. The first lecture introduces you to supervised learning with the help of famous faces from politics and Bollywood, casts neurons as classifiers, and gives you a taste of that bedrock of supervised learning, backpropagation, with whose help you will learn to back a truck into a loading dock.The second and third lectures focus on reinforcement learning. The second lecture will teach you how to predict rewards \u00c3\u00a0 la Pavlov's dog and will explore the connection to that important reward-related chemical in our brains: dopamine. In the third lecture, we will learn how to select the best actions for maximizing rewards, and examine a possible neural implementation of our computational model in the brain region known as the basal ganglia. The grand finale: flying a helicopter using reinforcement learning!", "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. \n\nThis course is the first in a sequence of three. It describes the two basic PGM representations: Bayesian Networks, which rely on a directed graph; and Markov networks, which use an undirected graph. The course discusses both the theoretical properties of these representations as well as their use in practice. The (highly recommended) honors track contains several hands-on assignments on how to represent some real-world problems. The course also presents some important extensions beyond the basic PGM representation, which allow more complex models to be encoded compactly. Introduction and Overview Bayesian Network (Directed Models) Template Models for Bayesian Networks Structured CPDs for Bayesian Networks Markov Networks (Undirected Models) Decision Making Knowledge Engineering & Summary This module provides an overall introduction to probabilistic graphical models, and defines a few of the key concepts that will be used later in the course. In this module, we define the Bayesian network representation and its semantics. We also analyze the relationship between the graph structure and the independence properties of a distribution represented over that graph. Finally, we give some practical tips on how to model a real-world situation as a Bayesian network. In many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models. A table-based representation of a CPD in a Bayesian network has a size that grows exponentially in the number of parents. There are a variety of other form of CPD that exploit some type of structure in the dependency model to allow for a much more compact representation. Here we describe a number of the ones most commonly used in practice. In this module, we describe Markov networks (also called Markov random fields): probabilistic graphical models based on an undirected graph representation. We discuss the representation of these models and their semantics. We also analyze the independence properties of distributions encoded by these graphs, and their relationship to the graph structure. We compare these independencies to those encoded by a Bayesian network, giving us some insight on which type of model is more suitable for which scenarios. In this module, we discuss the task of decision making under uncertainty. We describe the framework of decision theory, including some aspects of utility functions. We then talk about how decision making scenarios can be encoded as a graphical model called an Influence Diagram, and how such models provide insight both into decision making and the value of information gathering. This module provides an overview of graphical model representations and some of the real-world considerations when modeling a scenario as a graphical model. It also includes the course final exam.", "This course covers the essential exploratory techniques for summarizing data. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data. Week 1 Week 2 Week 3 Week 4 This week covers the basics of analytic graphics and the base plotting system in R. We've also included some background material to help you install R if you haven't done so already.  Welcome to Week 2 of Exploratory Data Analysis. This week covers some of the more advanced graphing systems available in R: the Lattice system and the ggplot2 system. While the base graphics system provides many important tools for visualizing data, it was part of the original R system and lacks many features that may be desirable in a plotting system, particularly when visualizing high dimensional data. The Lattice and ggplot2 systems also simplify the laying out of plots making it a much less tedious process. Welcome to Week 3 of Exploratory Data Analysis. This week covers some of the workhorse statistical methods for exploratory analysis. These methods include clustering and dimension reduction techniques that allow you to make graphical displays of very high dimensional data (many many variables). We also cover novel ways to specify colors in R so that you can use color as an important and useful dimension when making data graphics. All of this material is covered in chapters 9-12 of my book Exploratory Data Analysis with R. This week, we'll look at two case studies in exploratory data analysis. The first involves the use of cluster analysis techniques, and the second is a more involved analysis of some air pollution data. How one goes about doing EDA is often personal, but I'm providing these videos to give you a sense of how you might proceed with a specific type of dataset. ", "This course is an introduction to how to use relational databases in business analysis.  You will learn how relational databases work, and how to use entity-relationship diagrams to display the structure of the data held within them.  This knowledge will help you understand how data needs to be collected in business contexts, and help you identify features you want to consider if you are involved in implementing new data collection efforts.  You will also learn how to execute the most useful query and table aggregation statements for business analysts, and practice using them with real databases. No more waiting 48 hours for someone else in the company to provide data to you \u00e2\u0080\u0093 you will be able to get the data by yourself!\n\nBy the end of this course, you will have a clear understanding of how relational databases work, and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses. Analysts that understand how to access this data \u00e2\u0080\u0093 this means you! \u00e2\u0080\u0093 will have a strong competitive advantage in this data-smitten business world. About this Specialization and Course  Understanding Relational Databases  Queries to Extract Data from Single Tables  Queries to Summarize Groups of Data from Multiple Tables   Queries to Address More Detailed Business Questions Strengthen and Test Your Understanding  The Coursera Specialization, \"Managing Big Data with MySQL\" is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more about the specialization, please review the first lesson below, \"Specialization Introduction: Excel to MySQL: Analytic Techniques for Business.\"  In this fourth course of this specialization, \"Managing Big Data with MySQL\u00e2\u0080\u009d you will learn how relational databases  work and how they are used in business analysis. Specifically, you will: (1) Describe the structure of relational databases; (2) Interpret and create entity-relationship diagrams and relational schemas that describe the contents of specific databases; (3) Write queries that retrieve and sort data that meet specific criteria, and retrieve such data from real MySQL and Teradata business databases that contain over 1 million rows of data; (4) Execute practices that limit the impact of your queries on other coworkers; (5) Summarize rows of data using aggregate functions, and segment aggregations according to specified variables; (6) Combine and manipulate data from multiple tables across a database; (7) Retrieve records and compute calculations that are dependent on dynamic data features; (8) Translate data analysis questions into SQL queries that accommodate the types of anomalies found in real data sets. By the end of this course, you will have a clear understanding of how relational databases work and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses.  Analysts that understand how to access this data \u00e2\u0080\u0093 this means you! \u00e2\u0080\u0093 will have a strong competitive advantage in this data-smitten business world.  To get started with this course, you can begin with, \"Introduction to Managing Big Data with MySQL.\"  Please take some time to not only watch the videos, but also read through the course overview as there is extremely important course information in the overview.   Welcome to week 1! This week  you will learn how relational databases are organized, and practice making and interpreting Entity Relationship (ER) diagrams and relational schemas that describe the structure of data stored in a database. <p>By the end of the week, you will be able to:<ul><li>Describe the fundamental principles of relational database design <li>Interpret Entity Relationship (ER) diagrams and Entity Relationship (ER) schemas, and</li><li>Create your own ER diagrams and relational schemas using a software tool called ERDPlus that you will use to aid your query-writing later in the course.</li></ul><p>This week\u00e2\u0080\u0099s exercises are donated from a well-known Database Systems textbook, and will help you deepen and strengthen your understanding of how relational databases are organized.  This deeper understanding will help you navigate complicated business databases, and allow you to write more efficient queries.  At the conclusion of the week, you will test your understanding of database design principles by completing the Week 1 graded quiz.</p> <p>To get started, please begin with the video \u00e2\u0080\u009cProblems with Having a Lot of Data Used by a Lot of People.\u00e2\u0080\u009d <p>As always, if you have any questions, post them to the Discussions. <p>I hope you enjoy this week's materials! Welcome to week 2! This week, you will start interacting with business databases. You will write SQL queries that query data from two real companies. One data set, donated from a local start-up in Durham, North Carolina called Dognition, is a MySQL database containing tables of over 1 million rows. The other data set, donated from a national US department store chain called Dillard\u00e2\u0080\u0099s, is a Teradata database containing tables with over a hundred million rows. By the end of the week, you will be able to:1.  Use two different database user interfaces2.  Write queries to verify and describe all the contents of the Dognition MySQL database and the Dillard\u00e2\u0080\u0099s Teradata database3.  Retrieve data that meet specific criteria in a socially-responsible using SELECT, FROM, WHERE, LIMIT, and TOP clauses, and4.  Format the data you retrieve using aliases, DISTINCT clauses, and ORDER BY clauses.Make sure to watch the instructional videos about how to use the database interfaces we have established for this course, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 2 graded quiz.To get started, please begin with the video \u00e2\u0080\u009cIntroduction to Week 2.\u00e2\u0080\u009d  As always, if you have any questions, post them to the Discussions. Enjoy this week's materials!\n <p>Welcome to week 3! This week, we are going to learn the SQL syntax that allows you to segment your data into separate categories and segment.  We are also going to learn how to combine data stored in separate tables.</p><p>By the end of the week, you will be able to:</p><ul><li>Summarize values across entire columns, and break those summaries up according to specific variables or values in others columns using GROUP BY and HAVING clauses</li><li>Combine information from multiple tables using inner and outer joins</li><li>Use strategies to manage joins between tables with duplicate rows, many-to-many relationships, and atypical configurations</li><li>Practice one of the slightly more challenging use cases of aggregation functions, and</li><li>Work with the Dognition database to learn more about how MySQL handles mismatched aggregation levels.</li></ul><p>Make sure to watch the videos about joins, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 3 graded quiz.</p><p>We strongly encourage you to use the course Discussions to help each other with questions. </p> <p>To get started, please begin with the video 'Welcome to Week 3.\u00e2\u0080\u0099</p><p>I hope you enjoy this week\u00e2\u0080\u0099s materials!</p> <p>Welcome to week 4, the final week of Managing Big Data with MySQL!  This week you will practice integrating the SQL syntax you\u00e2\u0080\u0099ve learn so far into queries that address analysis questions typical of those you will complete as a business data analyst.</p>  <p>By the end of the week, you will be able to:</p><ul><li>Design and execute subqueries</li><li>Introduce logical conditions into your queries using IF and CASE statements</li><li>Implement analyses that accommodate missing data or data mistakes, and</li><li>Write complex queries that incorporate many tables and clauses.</li></ul><p>By the end of this week you will feel confident claiming that you know how to write SQL queries to create business value. Due to the extensive nature of the queries we will practice this week, we have put the graded quiz that tests your understanding of the SQL strategies you will practice in its own week rather than including it in this week\u00e2\u0080\u0099s materials. </p> <p>Make sure to complete both the MySQL exercises and the Teradata exercises, and we strongly encourage you to use the course Discussions to help each other with questions.  </p><p>To get started, please begin with the video 'Welcome to Week 4.\u00e2\u0080\u0099</p><p>I hope you enjoy this week\u00e2\u0080\u0099s materials!</p> This week contains the final ungraded Teradata exercises, and the final graded quiz for the course. The exercises are intended to hone and build your understanding of the last important concepts in the course, and lead directly to the quiz so be sure to do both!", "This course will expose you to the data analytics practices executed in the business world. We will explore such key areas as the analytical process, how data is created, stored, accessed, and how the organization works with data and creates the environment in which analytics can flourish.\n\nWhat you learn in this course will give you a strong foundation in all the areas that support analytics and will help you to better position yourself for success within your organization. You\u00e2\u0080\u0099ll develop skills and a perspective that will make you more productive faster and allow you to become a valuable asset to your organization.\n\nThis course also provides a basis for going deeper into advanced investigative and computational methods, which you have an opportunity to explore in future courses of the Data Analytics for Business specialization. Data and Analysis in the Real World Analytical Tools Data Extraction Using SQL Real World Analytical Organizations Welcome to week 1! In this module we\u00e2\u0080\u0099ll learn how to think about analytical problems and examine the process by which data enables analysis & decision making. We\u00e2\u0080\u0099ll introduce a framework called the Information-Action Value chain which describes the path from events in the world to business action, and we\u00e2\u0080\u0099ll look at some of the source systems that are used to capture data. At the end of this course you will be able to: Explain the information lifecycle from events in the real world to business actions, and how to think about analytical problems in that context , Recognize the types of events and characteristics that are often used in business analytics, and explain how the data is captured by source systems and stored using both traditional and emergent technologies, Gain a high-level familiarity with relational databases and learn how to use a simple but powerful language called SQL to extract analytical data sets of interest, Appreciate the spectrum of roles involved in the data lifecycle, and gain exposure to the various ways that organizations structure analytical functions,  Summarize some of the key ideas around data quality, data governance, and data privacy In this module we\u00e2\u0080\u0099ll learn about the technologies that enable analytical work.  We\u00e2\u0080\u0099ll examine data storage and databases, including the relational database.  We\u00e2\u0080\u0099ll talk about Big Data and Cloud technologies and ideas like federation, virtualization, and in-memory computing.  We\u00e2\u0080\u0099ll also walk through a landscape of some of the more common tool classes and learn how these tools support common analytical tasks.\n In this module we\u00e2\u0080\u0099ll learn how to extract data from a relational database using Structured Query Language, or SQL.  We\u00e2\u0080\u0099ll cover all the basic SQL commands and learn how to combine and stack data from different tables.  We\u00e2\u0080\u0099ll also learn how to expand the power of our queries using operators and handle additional complexity using subqueries. In this module we focus on the people and organizations that work with data and actually execute analytics.  We\u00e2\u0080\u0099ll discuss who does what and see how organizational structures can influence efficiency and effectiveness.  We\u00e2\u0080\u0099ll also look at the supporting rules & processes that help an analytical organization run smoothly, like Data Governance, Data Privacy, and Data Quality.", "In the first course of this specialization, we will recap what was covered in the  Machine Learning with TensorFlow on Google Cloud Platform Specialization (https://www.coursera.org/specializations/machine-learning-tensorflow-gcp).\n\nOne of the best ways to review something is to work with the concepts and technologies that you have learned.\n\nSo, this course is set up as a workshop and in this workshop, you will do End-to-End Machine Learning with TensorFlow on Google Cloud Platform\n\nPrerequisites:\nBasic SQL, familiarity with Python and TensorFlow\n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Welcome to the Course Machine Learning (ML) on Google Cloud Platform (GCP) Explore the Data Create the dataset Build the Model Operationalize the model Summary We'll give you an overview of this course and reveal the upcoming Advanced Machine Learning with TensorFlow on GCP specialization. This module reviews the steps of deploying machine learning in a production environment. This module explores a large dataset using Datalab and BigQuery. This modules shows how to use Pandas in Datalab and sample a dataset for local development. This module let's you develop a machine learning model in Tensorflow. This module explains how to preprocess data at scale for machine learning and lets you train a machine learning model at scale on Cloud AI Platform. This module reviews what you've learned in this course.", "One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation. Week 1: Prediction, Errors, and Cross Validation Week 2: The Caret Package Week 3: Predicting with trees, Random Forests, & Model Based Predictions Week 4: Regularized Regression and Combining Predictors This week will cover prediction, relative importance of steps, errors, and cross validation. This week will introduce the caret package, tools for creating features and preprocessing. This week we introduce a number of machine learning algorithms you can use to complete your course project. This week, we will cover regularized regression and combining predictors.  ", "This course is all about presenting the story of the data, using PowerPoint. You'll learn how to structure a presentation, to include insights and supporting data. You'll also learn some design principles for effective visuals and slides. You'll gain skills for client-facing communication - including public speaking, executive presence and compelling storytelling. Finally, you'll be given a client profile, a business problem, and a set of basic Excel charts, which you'll need to turn into a presentation - which you'll deliver with iterative peer feedback.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. Preparing a Presentation Communication styles Creating effective slides using PowerPoint Delivering a presentation This course is about presenting the story of the data, using PowerPoint. You'll learn how to structure a presentation and how to include insights and supporting data. You'll also learn some design principles for creating effective PowerPoint slides with visuals displaying data. Though application based exercises, you'll gain foundational communication skills - including public speaking, professional presence and compelling storytelling. Finally, you'll be given a client profile, a business problem, and a set of basic Excel charts, that you will use to create  a presentation.  You\u00e2\u0080\u0099ll receive peer feedback that you can use to enhance future presentations. This course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017 This week, we will be covering the different types of communications styles.  You\u00e2\u0080\u0099ll start off by gaining an understanding of your personal professional presence and learn how to maximize it.  You\u00e2\u0080\u0099ll learn about verbal and nonverbal communications, and strategies to enhance your questioning and listening skills. We will also discuss how differences in culture can impact how you communicate. This week, we're discussing how to create effective slides using PowerPoint. You\u00e2\u0080\u0099ll learn about the tools available within PowerPoint, how to structure your storyline, create storyboards, identify primary elements of slide design, display data and finalize your slide presentation. There is a peer review activity where you will apply the skills learned and create a storyboard. Finally, you will also get a chance to identify errors in a presentation to test your knowledge of standard industry practices. This week, you\u00e2\u0080\u0099re going to build and deliver a presentation to your peers, and receive feedback from them.  You will create a presentation of about 10 slides, employing the guidelines and industry best practices that have been discussed in this course. You can use the presentation storyboard that you created last week, which your peers have reviewed and given you feedback on.  Review what you\u00e2\u0080\u0099ve developed so far, and make changes or additions that you think will enhance the presentation. Once you\u00e2\u0080\u0099ve finalized your presentation, you will present it in a video using your smartphone or computer.\nOnce you\u00e2\u0080\u0099re satisfied with the PowerPoint presentation and video, you will be submitting both for peer review.  You can use this feedback for current and future presentations that you will make during your career.", "In this course, you will get hands-on instruction of advanced Excel 2013 functions.  You\u00e2\u0080\u0099ll learn to use PowerPivot to build databases and data models.  We\u00e2\u0080\u0099ll show you how to perform different types of scenario and simulation analysis and you\u00e2\u0080\u0099ll have an opportunity to practice these skills by leveraging some of Excel's built in tools including, solver, data tables, scenario manager and goal seek.  In the second half of the course, will cover how to visualize data, tell a story and explore data by reviewing core principles of data visualization and dashboarding.  You\u00e2\u0080\u0099ll use Excel to build complex graphs and Power View reports and then start to combine them into dynamic dashboards.\n\nNote: Learners will need PowerPivot to complete some of the exercises. Please use MS Excel 2013 version. If you have other MS Excel versions or a MAC you might not be able to complete all assignments.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. Preparing a Professional Excel  Advanced Scenario Analysis  Data Visualization Dashboarding  During this first week, you are going to learn about the development of data models and databases.  We will cover the components of data sets and the relational database models, database keys, relationships, and joins.  We will also look at a tool called PowerPivot that is used to import and prepare data to build relational models, as well as visualize data.  By the end of the week, you will have a working knowledge of how to develop a data model. \n\nBe sure to complete lessons in the order in which they are sequenced in the course. This week, we are going to explore three different analytical methods used to help model different scenarios and deal with variable uncertainty. These methods are scenario analysis, sensitivity analysis and simulation.  We\u00e2\u0080\u0099ll look at what each method is and then go deeper into why and how you use each.  Following some guided demonstration, you\u00e2\u0080\u0099ll be given a chance to practice in an Excel workbook and demonstrate what you\u00e2\u0080\u0099ve learned. \n This week we are going to focus on data visualization. We will start off by discussing data visualization basics, outlining the theory and concepts behind data visualization. We will also discuss how to enable effective story telling through the correct selection, creation, and presentation of tables and charts. You\u00e2\u0080\u0099ll get a chance to learn how to create detailed graphs and charts to effectively tell a story about your data.  In the final week of this course, you are going to learn how to create a dynamic dashboard. We are going to discuss how to establish a good understanding of your audience and how to collect key requirements in order to determine what type of dashboard to build.  We will talk about some guiding design principles and things to consider when building a dashboard. You\u00e2\u0080\u0099ll have a chance to practice everything you learn this week by creating your own functional dashboard in Excel.  \n", "This one-week accelerated on-demand course provides participants a a hands-on introduction to designing and building machine learning models on Google Cloud Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn machine learning (ML) and TensorFlow concepts, and develop hands-on skills in developing, evaluating, and productionizing ML models.\n\nOBJECTIVES\n\nThis course teaches participants the following skills:\n\n  \u00e2\u0097\u008f Identify use cases for machine learning\n\n  \u00e2\u0097\u008f Build an ML model using TensorFlow\n\n  \u00e2\u0097\u008f Build scalable, deployable ML models using Cloud ML\n\n  \u00e2\u0097\u008f Know the importance of preprocessing and combining features\n\n  \u00e2\u0097\u008f Incorporate advanced ML concepts into their models\n\n  \u00e2\u0097\u008f Productionize trained ML models\n\n\nPREREQUISITES\n\nTo get the most of out of this course, participants should have:\n\n  \u00e2\u0097\u008f Completed Google Cloud Fundamentals- Big Data and Machine Learning course OR have equivalent experience\n\n  \u00e2\u0097\u008f Basic proficiency with common query language such as SQL\n\n  \u00e2\u0097\u008f Experience with data modeling, extract, transform, load activities\n\n  \u00e2\u0097\u008f Developing applications using a common programming language such Python\n\n  \u00e2\u0097\u008f Familiarity with Machine Learning and/or statistics\n\nGoogle Account Notes:\n\u00e2\u0080\u00a2 Google services are currently unavailable in China.\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Welcome to Serverless Machine Learning on Google Cloud Platform Module 1: Getting Started with Machine Learning Module 2: Building ML models with Tensorflow Module 3: Scaling ML models with Cloud ML Engine Module 4: Feature Engineering     ", "This intermediate-level course introduces the mathematical foundations to derive Principal Component Analysis (PCA), a fundamental dimensionality reduction technique. We'll cover some basic statistics of data sets, such as mean values and variances, we'll compute distances and angles between vectors using inner products and derive orthogonal projections of data onto lower-dimensional subspaces. Using all these tools, we'll then derive PCA as a method that minimizes the average squared reconstruction error between data points and their reconstruction.\n\nAt the end of this course, you'll be familiar with important mathematical concepts and you can implement PCA all by yourself. If you\u00e2\u0080\u0099re struggling, you'll find a set of jupyter notebooks that will allow you to explore properties of the techniques and walk you through what you need to do to get on track. If you are already an expert, this course may refresh some of your knowledge.\n\nThe lectures, examples and exercises require:\n1. Some ability of abstract thinking\n2. Good background in linear algebra (e.g., matrix and vector algebra, linear independence, basis)\n3. Basic background in multivariate calculus (e.g., partial derivatives, basic optimization)\n4. Basic knowledge in python programming and numpy\n\nDisclaimer: This course is substantially more abstract and requires more programming than the other two courses of the specialization. However, this type of abstract thinking, algebraic manipulation and programming is necessary if you want to understand and develop machine learning algorithms. Statistics of Datasets Inner Products Orthogonal Projections Principal Component Analysis Principal Component Analysis (PCA) is one of the most important dimensionality reduction algorithms in machine learning. In this course, we lay the mathematical foundations to derive and understand PCA from a geometric point of view. In this module, we learn how to summarize datasets (e.g., images) using basic statistics, such as the mean and the variance. We also look at properties of the mean and the variance when we shift or scale the original data set. We will provide mathematical intuition as well as the skills to derive the results. We will also implement our results in code (jupyter notebooks), which will allow us to practice our mathematical understand to compute averages of image data sets. Data can be interpreted as vectors. Vectors allow us to talk about geometric concepts, such as lengths, distances and angles to characterise similarity between vectors. This will become important later in the course when we discuss PCA. In this module, we will introduce and practice the concept of an inner product. Inner products allow us to talk about geometric concepts in vector spaces. More specifically, we will start with the dot product (which we may still know from school) as a special case of an inner product, and then move toward a more general concept of an inner product, which play an integral part in some areas of machine learning, such as kernel machines (this includes support vector machines and Gaussian processes). We have a lot of exercises in this module to practice and understand the concept of inner products. In this module, we will look at orthogonal projections of vectors, which live in a high-dimensional vector space, onto lower-dimensional subspaces. This will play an important role in the next module when we derive PCA. We will start off with a geometric motivation of what an orthogonal projection is and work our way through the corresponding derivation. We will end up with a single equation that allows us to project any vector onto a lower-dimensional subspace. However, we will also understand how this equation came about. As in the other modules, we will have both pen-and-paper practice and a small programming example with a jupyter notebook. We can think of dimensionality reduction as a way of compressing data with some loss, similar to jpg or mp3. Principal Component Analysis (PCA) is one of the most fundamental dimensionality reduction techniques that are used in machine learning. In this module, we use the results from the first three modules of this course and derive PCA from a geometric point of view. Within this course, this module is the most challenging one, and we will go through an explicit derivation of PCA plus some coding exercises that will make us a proficient user of PCA. ", "This course is designed to impact the way you think about transforming data into better decisions. Recent extraordinary improvements in data-collecting technologies have changed the way firms make informed and effective business decisions. The course on operations analytics, taught by three of Wharton\u00e2\u0080\u0099s leading experts, focuses on how the data can be used to profitably match supply with demand in various business settings. In this course, you will learn how to model future demand uncertainties, how to predict the outcomes of competing policy choices and how to choose the best course of action in the face of risk. The course will introduce frameworks and ideas that provide insights into a spectrum of real-world business challenges, will teach you methods and software available for tackling these challenges quantitatively as well as the issues involved in gathering the relevant data.\n\nThis course is appropriate for beginners and business professionals with no prior analytics experience. Introduction, Descriptive and Predictive Analytics Prescriptive Analytics, Low Uncertainty Predictive Analytics, Risk Prescriptive Analytics, High Uncertainty  In this module you\u00e2\u0080\u0099ll be introduced to the Newsvendor problem, a fundamental operations problem of matching supply with demand in uncertain settings. You'll also cover the foundations of descriptive analytics for operations, learning how to use historical demand data to build forecasts for future demand.  Over the week, you\u00e2\u0080\u0099ll be introduced to underlying analytic concepts, such as random variables, descriptive statistics, common forecasting tools, and measures for judging the quality of  your forecasts. In this module, you'll learn how to identify the best decisions in settings with low uncertainty by building optimization models and applying them to specific business challenges. During the week, you\u00e2\u0080\u0099ll use algebraic formulations to concisely express optimization problems, look at how algebraic models should be converted into a spreadsheet format, and learn how to use spreadsheet Solvers as tools for identifying the best course of action.  How can you evaluate and compare decisions when their impact is uncertain? In this module you will learn how to build and interpret simulation models that can help you to evaluate complex business decisions in uncertain settings. During the week, you will be introduced to some common measures of risk and reward, you\u00e2\u0080\u0099ll use simulation to estimate these quantities, and you\u00e2\u0080\u0099ll learn how to interpret and visualize your simulation results. This module introduces decision trees, a useful tool for evaluating decisions made under uncertainty. Using a concrete example, you'll learn how optimization, simulation, and decision trees can be used together to solve more complex business problems with high degrees of uncertainty. You'll also discover how the Newsvendor problem introduced in Week 1 can be solved with the simulation and optimization framework introduced in Weeks 2 and 3.", "In this course you will learn how to quickly and easily get started with Artificial Intelligence using IBM Watson. You will understand how Watson works, become familiar with its use cases and real life client examples, and be introduced to several of Watson AI services from IBM that enable anyone to easily apply AI and build smart apps. You will also work with several Watson services to demonstrate AI in action.\n \nThis course does not require any programming or computer science expertise and is designed for anyone whether you have a technical background or not. Watson AI Overview Watson AI Services More Watson AI Services Watson in Action This week, you will learn how Watson AI works. You will understand the many ways Watson AI is helping professionals and businesses reimagine their workflows, learn more from less data, and protect their insights. \n This week, you will learn about some of the Watson AI services offered on the IBM Cloud. You will understand how organizations can use Watson AI services and the types of situations in which each service applies. This week, you will learn about some of the Watson AI services offered on the IBM Cloud. You will understand how organizations can use Watson AI services and the types of situations in which each service applies. This week, you will learn about common use cases for AI, and look at some case studies involving Watson AI. You will also experience and demonstrate AI in action yourself using Watson. ", "Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions.  Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist\u00e2\u0080\u0099s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing. Week 1: Least Squares and Linear Regression Week 2: Linear Regression & Multivariable Regression Week 3: Multivariable Regression, Residuals, & Diagnostics Week 4: Logistic Regression and Poisson Regression This week, we focus on least squares and linear regression. This week, we will work through the remainder of linear regression and then turn to the first part of  multivariable regression. This week, we'll build on last week's introduction to multivariable regression with some examples and then cover residuals, diagnostics, variance inflation, and model comparison.  This week, we will work on generalized linear models, including binary outcomes and Poisson regression. ", "Welcome to the Coursera specialization, From Data to Insights with Google Cloud Platform brought to you by the Google Cloud team. I\u00e2\u0080\u0099m Evan Jones (a data enthusiast) and I\u00e2\u0080\u0099m going to be your guide.\n\nThis first course in this specialization is Exploring and Preparing your Data with BigQuery. Here we will see what the common challenges faced by data analysts are and how to solve them with the big data tools on Google Cloud Platform. You\u00e2\u0080\u0099ll pick up some SQL along the way and become very familiar with using BigQuery and Cloud Dataprep to analyze and transform your datasets.\n\nThis course should take about one week to complete, 5-7 total hours of work.  By the end of this course, you\u00e2\u0080\u0099ll be able to query and draw insight from millions of records in our BigQuery public datasets. You\u00e2\u0080\u0099ll learn how to assess the quality of your datasets and develop an automated data cleansing pipeline that will output to BigQuery. Lastly, you\u00e2\u0080\u0099ll get to practice writing and troubleshooting SQL on a real Google Analytics e-commerce dataset to drive marketing insights.\n\n>>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Welcome to From \u00e2\u0080\u008bData \u00e2\u0080\u008bto \u00e2\u0080\u008bInsights \u00e2\u0080\u008bwith \u00e2\u0080\u008bGoogle \u00e2\u0080\u008bCloud Platform: \u00e2\u0080\u008bExploring \u00e2\u0080\u008band \u00e2\u0080\u008bPreparing \u00e2\u0080\u008byour \u00e2\u0080\u008bData Module 1: Introduction \u00e2\u0080\u008bto \u00e2\u0080\u008bData \u00e2\u0080\u008bon Google \u00e2\u0080\u008bCloud \u00e2\u0080\u008bPlatform Module 2: \u00e2\u0080\u008bBig \u00e2\u0080\u008bData \u00e2\u0080\u008bTools \u00e2\u0080\u008bOverview Module 3: \u00e2\u0080\u008bExploring \u00e2\u0080\u008byour \u00e2\u0080\u008bData \u00e2\u0080\u008bwith SQL Module 4: \u00e2\u0080\u008bGoogle \u00e2\u0080\u008bBigQuery \u00e2\u0080\u008bPricing Module 5: \u00e2\u0080\u008bCleaning \u00e2\u0080\u008band \u00e2\u0080\u008bTransforming your \u00e2\u0080\u008bData Learn the courses, content, and technologies that are part of this data analyst specialization Understand the core principles behind Google Cloud Platform and how to leverage them for big data analysis Learn what are the key big data tools on Google Cloud Platform that you will be using to analyze, prepare, and visualize data Learn how to query your data with the basics of SQL (Structured Query Language) and practice writing queries in BigQuery Understand how pricing works in BigQuery and how you can best optimize your queries Understand the importance of creating high quality datasets and learn the tools that will help you transform your data", "Process mining is the missing link between model-based process analysis and data-oriented analysis techniques. Through concrete data sets and easy to use software the course provides data science knowledge that can be applied directly to analyze and improve processes in a variety of domains.\n\nData science is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis. Process mining bridges the gap between traditional model-based process analysis (e.g., simulation and other business process management techniques) and data-centric analysis techniques such as machine learning and data mining. Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically). This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as \"data science in action\".\n\nThe course explains the key analysis techniques in process mining. Participants will learn various process discovery algorithms. These can be used to automatically learn process models from raw event data. Various other process analysis techniques that use event data will be presented. Moreover, the course will provide easy-to-use software, real-life data sets, and practical skills to directly apply the theory in a variety of application domains.\n\nThis course starts with an overview of approaches and technologies that use event data to support decision making and business process (re)design. Then the course focuses on process mining as a bridge between data mining and business process modeling. The course is at an introductory level with various practical assignments.\n\nThe course covers the three main types of process mining.\n\n1. The first type of process mining is discovery. A discovery technique takes an event log and produces a process model without using any a-priori information. An example is the Alpha-algorithm that takes an event log and produces a process model (a Petri net) explaining the behavior recorded in the log.\n\n2. The second type of process mining is conformance. Here, an existing process model is compared with an event log of the same process. Conformance checking can be used to check if reality, as recorded in the log, conforms to the model and vice versa.\n\n3. The third type of process mining is enhancement. Here, the idea is to extend or improve an existing process model using information about the actual process recorded in some event log. Whereas conformance checking measures the alignment between model and reality, this third type of process mining aims at changing or extending the a-priori model. An example is the extension of a process model with performance information, e.g., showing bottlenecks. Process mining techniques can be used in an offline, but also online setting. The latter is known as operational support. An example is the detection of non-conformance at the moment the deviation actually takes place. Another example is time prediction for running cases, i.e., given a partially executed case the remaining processing time is estimated based on historic information of similar cases.\n\nProcess mining provides not only a bridge between data mining and business process management; it also helps to address the classical divide between \"business\" and \"IT\". Evidence-based business process management based on process mining helps to create a common ground for business process improvement and information systems development.\n\nThe course uses many examples using real-life event logs to illustrate the concepts and algorithms. After taking this course, one is able to run process mining projects and have a good understanding of the Business Process Intelligence field.\n\nAfter taking this course you should:\n- have a good understanding of Business Process Intelligence techniques (in particular process mining),\n- understand the role of Big Data in today\u00e2\u0080\u0099s society,\n- be able to relate process mining techniques to other analysis techniques such as simulation, business intelligence, data mining, machine learning, and verification,\n- be able to apply basic process discovery techniques to learn a process model from an event log (both manually and using tools),\n- be able to apply basic conformance checking techniques to compare event logs and process models (both manually and using tools),\n- be able to extend a process model with information extracted from the event log (e.g., show bottlenecks),\n- have a good understanding of the data needed to start a process mining project,\n- be able to characterize the questions that can be answered based on such event data,\n- explain how process mining can also be used for operational support (prediction and recommendation), and\n- be able to conduct process mining projects in a structured manner. Introduction and Data Mining Process Models and Process Discovery Different Types of Process Models Process Discovery Techniques and Conformance Checking Enrichment of Process Models Operational Support and Conclusion This first module contains general course information (syllabus, grading information) as well as the first lectures introducing data mining and process mining. In this module we introduce process models and the key feature of process mining: discovering process models from event data. Now that you know the basics of process mining, it is time to dive a little bit deeper and show you other ways of discovering a process model from event data. In this module we conclude process discovery by discussing alternative approaches. We also introduce how to check the conformance of the event data and the process model. In this module we focus on enriching process models. We can for instance add the data aspect to process models, show bottlenecks on the process model and analyse the social aspects of the process. In this final module we discuss how process mining can be applied on running processes. We also address how to get the (right) event data, process mining software, and how to get from data to results.", "This course presents critical concepts and practical methods to support planning, collection, storage, and dissemination of data in clinical research.\n\nUnderstanding and implementing solid data management principles is critical for any scientific domain. Regardless of your current (or anticipated) role in the research enterprise, a strong working knowledge and skill set in data management principles and practice will increase your productivity and improve your science. Our goal is to use these modules to help you learn and practice this skill set. \n\nThis course assumes very little current knowledge of technology other than how to operate a web browser. We will focus on practical lessons, short quizzes, and hands-on exercises as we explore together best practices for data management. Research Data Collection Strategy Electronic Data Capture Fundamentals Planning a Data Strategy for a Prospective Study Practicing What We've Learned: Implementation Post-Study Activities and Other Considerations Data Collection with Surveys This introductory module reviews the course structure and basic concepts in clinical research. We also discuss best practices for designing your clinical research data collection. This module covers standards for study processes, concepts for regulatory compliance, and electronic data capture fundamentals. This module reviews the process of planning data elements for a real-world research study. This week, we set up an Electronic Data Capture (EDC) instrument in REDCap for the Morphine vs. Marinol Study. We also review data processes that occur during the running of a study, including an overview of key data quality operations. In this week, we cover activities to wrap up your study and share data and results, as well as two lectures on other electronic sources of data that can be used in research.  In response to learner requests, we've also added several lectures on clinical data management in resource-limited settings, in collaboration with research colleagues from Indiana University. This is a long week of videos, but next week will be short on videos in exchange! In the final week, we cover how to collect data using surveys and review an example together. This week's assignment includes designing, distributing, and reporting on your own survey.", "We introduce low-level TensorFlow and work our way through the necessary concepts and APIs so as to be able to write distributed machine learning models. Given a TensorFlow model, we explain how to scale out the training of that model and offer high-performance predictions using Cloud Machine Learning Engine.\n\nCourse Objectives:\nCreate machine learning models in TensorFlow\nUse the TensorFlow libraries to solve numerical problems\nTroubleshoot and debug common TensorFlow code pitfalls\nUse tf.estimator to create, train, and evaluate an ML model\nTrain, deploy, and productionalize ML models at scale with Cloud ML Engine\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Core TensorFlow Estimator API Scaling TensorFlow models Summary The tool we will use to write machine learning programs is TensorFlow and so in this course, we will introduce you to TensorFlow. In the first course, you learned how to formulate business problems as machine learning problems and in the second course, you learned how machine works in practice and how to create datasets that you can use for machine learning. Now that you have the data in place, you are ready to get started writing machine learning programs. We will introduce you to the core components of TensorFlow and you will get hands-on practice building machine learning programs. You will compare and write lazy evaluation and imperative programs, work with graphs, sessions, variables, as finally debug TensorFlow programs. In this module we will walk you through the Estimator API. I\u00e2\u0080\u0099m here to talk about how you would go about taking your TensorFlow model and training it on GCP\u00e2\u0080\u0099s managed infrastructure for machine learning model training and deployed. Here we summarize the TensorFlow topics we covered so far in this course. We'll revisit core TensorFlow code, the Estimator API, and end with scaling your machine learning models with Cloud Machine Learning Engine.", "Case Studies: Analyzing Sentiment & Loan Default Prediction\n\nIn our case study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).  In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. \n\nIn this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks.  You will become familiar with  the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting.  In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent.  You will implement these technique on real-world, large-scale machine learning tasks.  You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier.  This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data.  We've also included optional content in every module, covering advanced topics for those who want to go even deeper! \n\nLearning Objectives: By the end of this course, you will be able to:\n   -Describe the input and output of a classification model.\n   -Tackle both binary and multiclass classification problems.\n   -Implement a logistic regression model for large-scale classification.  \n   -Create a non-linear model using decision trees.\n   -Improve the performance of any model using boosting.\n   -Scale your methods with stochastic gradient ascent.\n   -Describe the underlying decision boundaries.  \n   -Build a classification model to predict sentiment in a product review dataset.  \n   -Analyze financial data to predict loan defaults.\n   -Use techniques for handling missing data.\n   -Evaluate your models using precision-recall metrics.\n   -Implement these techniques in Python (or in the language of your choice, though Python is highly recommended). Welcome! Linear Classifiers & Logistic Regression Learning Linear Classifiers Overfitting & Regularization in Logistic Regression Decision Trees Preventing Overfitting in Decision Trees Handling Missing Data Boosting Precision-Recall Scaling to Huge Datasets & Online Learning Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. Linear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task. Once familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression.  You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier. As we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers' outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data. Along with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. In this module, you will become familiar with the core decision trees representation. You will then design a simple, recursive greedy algorithm to learn decision trees from data. Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data. Out of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam's razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set.  Real-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation.   One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it's used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called  AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data.  In many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data. With the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called \"Big Data\". Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data. ", "This course will introduce you to the wonderful world of Python programming!  We'll learn about the essential elements of programming and how to construct basic Python programs. We will cover expressions, variables, functions, logic, and conditionals, which are foundational concepts in computer programming. We will also teach you how to use Python modules, which enable you to benefit from the vast array of functionality that is already a part of the Python language. These concepts and skills will help you to begin to think like a computer programmer and to understand how to go about writing Python programs.\n\nBy the end of the course, you will be able to write short Python programs that are able to accomplish real, practical tasks. This course is the foundation for building expertise in Python programming. As the first course in a specialization, it provides the necessary building blocks for you to succeed at learning to write more complex Python programs.\n\nThis course uses Python 3.  While many Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This first course will use a Python 3 version of the CodeSkulptor development environment, which is specifically designed to help beginning programmers learn quickly.  CodeSkulptor runs within any modern web browser and does not require you to install any software, allowing you to start writing and running small programs immediately.  In the later courses in this specialization,  we will help you to move to more sophisticated desktop development environments. Python as a Calculator Functions Logic and Conditionals Python Modules This module will expose you to Python so that you can run your first simple programs.  You will use Python to compute the results of arithmetic expressions, as you would when using a calculator. This module will teach you how to define and call functions. Functions allow you to write code once that you can execute repeatedly with different inputs. This module will teach you how to use logic and conditionals to change the behavior of the program based upon values within the program. This module will introduce you to the concept of modules. Python modules allow code to be divided up into different files and reused in different programs.  Python provides many modules that you can use within your programs.", "Unix forms a foundation that is often very helpful for accomplishing other goals you might have for you and your computer, whether that goal is running a business, writing a book, curing disease, or creating the next great app. The means to these goals are sometimes carried out by writing software. Software can\u00e2\u0080\u0099t be mined out of the ground, nor can software seeds be planted in spring to harvest by autumn. Software isn\u00e2\u0080\u0099t produced in factories on an assembly line. Software is a hand-made, often bespoke good. If a software developer is an artisan, then Unix is their workbench. Unix provides an essential and simple set of tools in a distraction-free environment. Even if you\u00e2\u0080\u0099re not a software developer learning Unix can open you up to new methods of thinking and novel ways to scale your ideas. \n\nThis course is intended for folks who are new to programming and new to Unix-like operating systems like macOS and Linux distributions like Ubuntu. Most of the technologies discussed in this course will be accessed via a command line interface. Command line interfaces can seem alien at first, so this course attempts to draw parallels between using the command line and actions that you would normally take while using your mouse and keyboard. You\u00e2\u0080\u0099ll also learn how to write little pieces of software in a programming language called Bash, which allows you to connect together the tools we\u00e2\u0080\u0099ll discuss. My hope is that by the end of this course you be able to use different Unix tools as if they\u00e2\u0080\u0099re interconnecting Lego bricks. Unix and Command Line Basics Working with Unix Bash Programming Git and GitHub Nephology This week we'll help you get access to Unix (you may already be using it), and you'll start using the command line. We'll draw parallels between using your mouse and keyboard with your computer's graphics versus only using the command line. Now we'll get into the power of different Unix tools. We'll walk through several scenarios where you could use Unix to perform tasks at a much faster speed than you would be able to normally. During this week we'll unleash the command line's usefulness as a programming language. By the end of this week you'll be writing your own little computer programs that you can use on the command line. First you'll learn how to use Git, which is like \"track changes\" for your code and plain text files, but much more powerful. We'll then explore how to use Git with GitHub, a social coding network where you can publish you projects and explore other's code.  Finally we'll set up a cloud computing environment so we can explore how computers communicate with each other using the internet.", "This course is for novice programmers or business people who would like to understand the core tools used to wrangle and analyze big data. With no prior experience, you will have the opportunity to walk through hands-on examples with Hadoop and Spark frameworks, two of the most common in the industry. You will be comfortable explaining the specific components and basic processes of the Hadoop architecture, software stack, and execution environment.   In the assignments you will be guided in how data scientists apply the important concepts and techniques such as Map-Reduce that are used to solve fundamental problems in big data.  You'll feel empowered to have conversations about big data and the data analysis process. Hadoop Basics Introduction to the Hadoop Stack Introduction to Hadoop Distributed File System (HDFS) Introduction to Map/Reduce Spark Welcome to the first module of the Big Data Platform course. This first module will provide insight into Big Data Hype, its technologies opportunities and challenges.  We will take a deeper look into the Hadoop stack and tool and technologies associated with Big Data solutions.  \n In this module we will take a detailed look at the Hadoop stack ranging from the basic HDFS components, to application execution frameworks, and languages, services. In this module we will take a detailed look at the Hadoop Distributed File System (HDFS). We will cover the main design goals of HDFS, understand the read/write process to HDFS, the main configuration parameters that can be tuned to control HDFS performance and robustness, and get an overview of the different ways you can access data on HDFS. This module will introduce Map/Reduce concepts and practice.  You will learn about the big idea of Map/Reduce and you will learn how to design, implement, and execute tasks in the map/reduce framework. You will also learn the trade-offs in map/reduce and how that motivates other tools. Welcome to module 5, Introduction to Spark, this week we will focus on the Apache Spark cluster computing framework, an important contender of Hadoop MapReduce in the Big Data Arena.\n\nSpark provides great performance advantages over Hadoop MapReduce,especially for iterative algorithms, thanks to in-memory caching. Also, gives Data Scientists an easier way to write their analysis pipeline in Python and Scala,even providing interactive shells to play live with data.", "Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for pattern-based classification and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns. Course Orientation Week 1: The Computer and the Human Week 2: Visualization of Numerical Data Week 3: Visualization of Non-Numerical Data Week 4: The Visualization Dashboard You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. In this week's module, you will learn what data visualization is, how it's used, and how computers display information. You'll also explore different types of visualization and how humans perceive information. In this week's module, you will start to think about how to visualize data effectively. This will include assigning data to appropriate chart elements, using glyphs, parallel coordinates, and streamgraphs, as well as implementing principles of design and color to make your visualizations more engaging and effective. In this week's module, you will learn how to visualize graphs that depict relationships between data items. You'll also plot data using coordinates that are not specifically provided by the data set. In this week's module, you will start to put together everything you've learned by designing your own visualization system for large datasets and dashboards. You'll create and interpret the visualization you created from your data set, and you'll also apply techniques from user-interface design to create an effective visualization system.", "\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a8\u00e3\u0081\u00af\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00aa\u00e3\u0082\u0082\u00e3\u0081\u00ae\u00e3\u0081\u00a7\u00e3\u0080\u0081\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00aa\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0081\u00ae\u00e8\u00a7\u00a3\u00e6\u00b1\u00ba\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a4\u00e3\u0081\u00ae\u00e3\u0081\u00a7\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0081\u008b\u00e3\u0080\u0082Google \u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00a0\u00e3\u0081\u0091\u00e3\u0081\u00a7\u00e3\u0081\u00aa\u00e3\u0081\u008f\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00ae\u00e9\u009d\u00a2\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e3\u0082\u0082\u00e7\u008b\u00ac\u00e8\u0087\u00aa\u00e3\u0081\u00ae\u00e8\u00a6\u0096\u00e7\u0082\u00b9\u00e3\u0081\u00a7\u00e8\u0080\u0083\u00e3\u0081\u0088\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u0086\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e6\u008d\u0089\u00e3\u0081\u0088\u00e6\u0096\u00b9\u00e3\u0081\u008c\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0082\u0092\u00e8\u0080\u0083\u00e3\u0081\u0088\u00e3\u0082\u008b\u00e3\u0081\u0086\u00e3\u0081\u0088\u00e3\u0081\u00a7\u00e3\u0081\u00aa\u00e3\u0081\u009c\u00e6\u009c\u0089\u00e5\u008a\u00b9\u00e3\u0081\u00aa\u00e3\u0081\u00ae\u00e3\u0081\u008b\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ac\u00a1\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e5\u0080\u0099\u00e8\u00a3\u009c\u00e3\u0081\u00a8\u00e3\u0081\u00aa\u00e3\u0082\u008b\u00e3\u0083\u00a6\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a7\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e5\u00bd\u00a2\u00e3\u0081\u00ab\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b 5 \u00e3\u0081\u00a4\u00e3\u0081\u00ae\u00e6\u00ae\u00b5\u00e9\u009a\u008e\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u0086\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e6\u00ae\u00b5\u00e9\u009a\u008e\u00e3\u0082\u0092\u00e7\u009c\u0081\u00e7\u0095\u00a5\u00e3\u0081\u0097\u00e3\u0081\u00aa\u00e3\u0081\u0084\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ae\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e6\u0080\u00a7\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00ab\u0096\u00e3\u0081\u0098\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u00be\u008c\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u008c\u00e5\u008a\u00a9\u00e9\u0095\u00b7\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e6\u0080\u00a7\u00e3\u0081\u00ae\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e5\u0081\u008f\u00e8\u00a6\u008b\u00e3\u0081\u00ae\u00e8\u00aa\u008d\u00e8\u00ad\u0098\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0081\u009d\u00e3\u0082\u008c\u00e3\u0082\u0092\u00e8\u00ad\u0098\u00e5\u0088\u00a5\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae\u00e7\u00b4\u00b9\u00e4\u00bb\u008b AI \u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u00a8\u00e3\u0081\u00af Google \u00e3\u0081\u00ae ML \u00e3\u0081\u00ae\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e7\u00b5\u0084\u00e3\u0081\u00bf \u00e5\u008c\u0085\u00e6\u008b\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa ML \u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00a6\u00e3\u0083\u0089\u00e3\u0081\u00ae Python Notebook \u00e8\u00a6\u0081\u00e7\u00b4\u0084 \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00a8\u00e3\u0081\u009d\u00e3\u0082\u008c\u00e3\u0082\u0092\u00e6\u0095\u0099\u00e3\u0081\u0088\u00e3\u0082\u008b Google \u00e3\u0082\u00a8\u00e3\u0082\u00ad\u00e3\u0082\u00b9\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e7\u00b4\u00b9\u00e4\u00bb\u008b\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 Google \u00e3\u0081\u008c\u00e8\u0087\u00aa\u00e7\u00a4\u00be\u00e3\u0081\u00ae\u00e4\u00bc\u0081\u00e6\u00a5\u00ad\u00e6\u0088\u00a6\u00e7\u0095\u00a5\u00e3\u0081\u00af AI \u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e9\u009a\u009b\u00e3\u0081\u00ae\u00e6\u0084\u008f\u00e5\u0091\u00b3\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e9\u009a\u009b\u00e3\u0081\u00ae\u00e6\u0084\u008f\u00e5\u0091\u00b3\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google \u00e3\u0081\u008c\u00e9\u0095\u00b7\u00e5\u00b9\u00b4\u00e3\u0081\u00ab\u00e3\u0082\u008f\u00e3\u0081\u009f\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e8\u0093\u0084\u00e7\u00a9\u008d\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u008d\u00e3\u0081\u009f\u00e7\u00b5\u0084\u00e7\u00b9\u0094\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e3\u0083\u008e\u00e3\u0082\u00a6\u00e3\u0083\u008f\u00e3\u0082\u00a6\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u00b7\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0083\u00a0\u00e3\u0081\u008c\u00e3\u0083\u0087\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00ab\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e3\u0081\u00be\u00e3\u0081\u00be\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e5\u0081\u008f\u00e3\u0081\u00a3\u00e3\u0081\u009f\u00e5\u0088\u00a4\u00e5\u00ae\u009a\u00e3\u0082\u0092\u00e5\u0087\u00ba\u00e5\u008a\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e7\u0090\u0086\u00e7\u0094\u00b1\u00e3\u0081\u00a8 ML \u00e3\u0082\u0092\u00e8\u00a3\u00bd\u00e5\u0093\u0081\u00e3\u0081\u00ab\u00e7\u00b5\u0084\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0082\u0080\u00e9\u009a\u009b\u00e3\u0081\u00ae\u00e6\u00b3\u00a8\u00e6\u0084\u008f\u00e7\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00a7\u00e4\u00bd\u00bf\u00e3\u0081\u0086\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e7\u0092\u00b0\u00e5\u00a2\u0083\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b Cloud Datalab \u00e3\u0082\u0092\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e4\u00b8\u008a\u00e3\u0081\u0092\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 ", "Apache Spark is the de-facto standard for large scale data processing. This is the first course of a series of courses towards the IBM Advanced Data Science Specialization. We strongly believe that is is crucial for success to start learning a scalable data science platform since memory and CPU constraints are to most limiting factors when it comes to building advanced machine learning models.\n\nIn this course we teach you the fundamentals of Apache Spark using python and pyspark. We'll introduce Apache Spark in the first two weeks and learn how to apply it to compute basic exploratory and data pre-processing tasks in the last two weeks. Through this exercise you'll also be introduced to the most fundamental statistical measures and data visualization technologies.\n\nThis gives you enough knowledge to take over the role of a data engineer in any modern environment. But it gives you also the basis for advancing your career towards data science. \n\nPlease have a look at the full specialization curriculum:\nhttps://www.coursera.org/specializations/advanced-data-science-ibm\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  To find out more about IBM digital badges follow the link ibm.biz/badging.\n\n\nAfter completing this course, you will be able to:\n\u00e2\u0080\u00a2\tDescribe how basic statistical measures, are used to reveal  patterns within the data \n\u00e2\u0080\u00a2\tRecognize data characteristics, patterns, trends, deviations or inconsistencies, and potential outliers.\n\u00e2\u0080\u00a2\tIdentify useful techniques for working with big data such as dimension reduction and feature selection methods \n\u00e2\u0080\u00a2\tUse advanced tools and charting libraries to:\n      o\timprove efficiency of analysis of big-data with partitioning and parallel analysis \n      o\tVisualize the data in an number of 2D and 3D formats (Box Plot, Run Chart, Scatter Plot, Pareto Chart, and Multidimensional Scaling)\n\nFor successful completion of the course, the following prerequisites are recommended: \n\u00e2\u0080\u00a2\tBasic programming skills in python\n\u00e2\u0080\u00a2\tBasic math\n\u00e2\u0080\u00a2\tBasic SQL (you can get it easily from https://www.coursera.org/learn/sql-data-science if needed)\n\nIn order to complete this course, the following technologies will be used:\n(These technologies are introduced in the course as necessary so no previous knowledge is required.)\n\u00e2\u0080\u00a2\tJupyter notebooks (brought to you by IBM Watson Studio for free)\n\u00e2\u0080\u00a2\tApacheSpark (brought to you by IBM Watson Studio for free)\n\u00e2\u0080\u00a2\tPython\n\nThis course takes four weeks, 4-6h per week Introduction the course and grading environment Tools that support BigData solutions Scaling Math for Statistics on Apache Spark Data Visualization of Big Data    ", "Welcome to the Reinforcement Learning course. \n\nHere you will find out about:\n\n- foundations of RL methods: value/policy iteration, q-learning, policy gradient, etc.\n--- with math & batteries included\n\n- using deep neural networks for RL tasks\n--- also known as \"the hype train\"\n\n- state of the art RL algorithms\n--- and how to apply duct tape to them for practical problems.\n\n- and, of course, teaching your neural network to play games\n--- because that's what everyone thinks RL is about. We'll also use it for seq2seq and contextual bandits.\n\nJump in. It's gonna be fun!\n\nDo you have technical problems? Write to us: coursera@hse.ru Intro: why should i care? At the heart of RL: Dynamic Programming Model-free methods Approximate Value Based Methods Policy-based methods Exploration In this module we gonna define and \"taste\" what reinforcement learning is about. We'll also learn one simple algorithm that can solve reinforcement learning problems with embarrassing efficiency. This week we'll consider the reinforcement learning formalisms in a more rigorous, mathematical way.  You'll learn how to effectively compute the return your agent gets for a particular action - and how to pick best actions based on that return. This week we'll find out how to apply last week's ideas to the real world problems: ones where you don't have a perfect model of your environment. This week we'll learn to scale things even farther up by training agents based on neural networks. We spent 3 previous modules working on the value-based methods: learning state values, action values and whatnot. Now's the time to see an alternative approach that doesn't require you to predict all future rewards to learn something. In this final week you'll learn how to build better exploration strategies with a focus on contextual bandit setup. In honor track, you'll also learn how to apply reinforcement learning to train structured deep learning models.", "Everyday across the world, thousands of businesses are victimized by fraud.  Who commits these bad acts?  Why? And, how? In this course we are going to help you answer the questions: who commits fraud, why and how.  We\u00e2\u0080\u0099ll also help you develop skills for catching them. The Accidental Fraudster The Predator Fraudster  Big Data, Benford's Law and Financial Analytics Cyber-Crime and Money Laundering: Contemporary Tools and Techniques. Whistleblowing Who are \"accidental\" fraudsters?  Learn who accidental fraudsters are, the basic elements of fraud, and how devastating the costs of fraud are. What is a \"preditor\" fraudster? How do you protect your organization against a preditor fraudster?  Learn how internal control concepts and other techniques can help you detect and prevent preditor fraudsters. What is \"big data\"?  Learn how data analysis, Benford analysis and other tools can help you identify fraudulent activities. Money laundering.  This week\u00e2\u0080\u0099s session will introduce you to the objectives and \n\nstages of money laundering as well as the basic techniques used.  Everything you need to know about whistleblowing.  Learn the importance of \n\nwhistleblowing and the difficulty of being a whistleblower.  ", "In this course, you will learn about several algorithms that can learn near optimal policies based on trial and error interaction with the environment---learning from the agent\u00e2\u0080\u0099s own experience. Learning from actual experience is striking because it requires no prior knowledge of the environment\u00e2\u0080\u0099s dynamics, yet can still attain optimal behavior. We will cover intuitively simple but powerful Monte Carlo methods, and temporal difference learning methods including Q-learning. We will wrap up this course investigating how we can get the best of both worlds: algorithms that can combine model-based planning (similar to dynamic programming) and temporal difference updates to radically accelerate learning.\n\nBy the end of this course you will be able to:\n \n- Understand Temporal-Difference learning and Monte Carlo as two strategies for estimating value functions from sampled experience\n- Understand the importance of exploration, when using sampled experience rather than dynamic programming sweeps within a model\n- Understand the connections between Monte Carlo and Dynamic Programming and TD. \n- Implement and apply the TD algorithm, for estimating value functions\n- Implement and apply Expected Sarsa and Q-learning (two TD methods for control) \n- Understand the difference between on-policy and off-policy control\n- Understand planning with simulated experience (as opposed to classic planning strategies)\n- Implement a model-based approach to RL, called Dyna, which uses simulated experience \n- Conduct an empirical study to see the improvements in sample efficiency when using Dyna Welcome to the Course!   Monte Carlo Methods for Prediction & Control Temporal Difference Learning Methods for Prediction  Temporal Difference Learning Methods for Control  Planning, Learning & Acting Welcome to the second course in the Reinforcement Learning Specialization: Sample-Based Learning Methods, brought to you by the University of Alberta, Onlea, and Coursera. In this pre-course module, you'll be introduced to your instructors, and get a flavour of what the course has in store for you. Make sure to introduce yourself to your classmates in the \"Meet and Greet\" section! This week you will learn how to estimate value functions and optimal policies, using only sampled experience from the environment. This module represents our first step toward incremental learning methods that learn from the agent\u00e2\u0080\u0099s own interaction with the world, rather than a model of the world. You will learn about on-policy and off-policy methods for prediction and control, using Monte Carlo methods---methods that use sampled returns. You will also be reintroduced to the exploration problem, but more generally in RL, beyond bandits.  This week, you will learn about one of the most fundamental concepts in reinforcement learning: temporal difference (TD) learning. TD learning combines some of the features of both Monte Carlo and Dynamic Programming (DP) methods. TD methods are similar to Monte Carlo methods in that they can learn from the agent\u00e2\u0080\u0099s interaction with the world, and do not require knowledge of the model. TD methods are similar to DP methods in that they bootstrap, and thus can learn online---no waiting until the end of an episode. You will see how TD can learn more efficiently than Monte Carlo, due to bootstrapping. For this module, we first focus on TD for prediction, and discuss TD for control in the next module. This week, you will implement TD to estimate the value function for a fixed policy, in a simulated domain.  This week, you will learn about using temporal difference learning for control, as a generalized policy iteration strategy. You will see three different algorithms based on bootstrapping and Bellman equations for control: Sarsa, Q-learning and Expected Sarsa. You will see some of the differences between the methods for on-policy and off-policy control, and that Expected Sarsa is a unified algorithm for both. You will implement Expected Sarsa and Q-learning, on Cliff World.  Up until now, you might think that learning with and without a model are two distinct, and in some ways, competing strategies: planning with Dynamic Programming verses sample-based learning via TD methods. This week we unify these two strategies with the Dyna architecture. You will learn how to estimate the model from data and then use this model to generate hypothetical experience (a bit like dreaming) to dramatically improve sample efficiency compared to sample-based methods like Q-learning. In addition, you will learn how to design learning systems that are robust to inaccurate models. ", "Want to make sense of the volumes of data you have collected?  Need to incorporate data-driven decisions into your process?  This course provides an overview of machine learning techniques to explore, analyze, and leverage data.  You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems.\n\nAt the end of the course, you will be able to:\n\u00e2\u0080\u00a2\tDesign an approach to leverage data using the steps in the machine learning process.\n\u00e2\u0080\u00a2\tApply machine learning techniques to explore and prepare data for modeling.\n\u00e2\u0080\u00a2\tIdentify the type of machine learning problem in order to apply the appropriate set of techniques.\n\u00e2\u0080\u00a2\tConstruct models that learn from data using widely available open source tools.\n\u00e2\u0080\u00a2\tAnalyze big data problems using scalable machine learning algorithms on Spark.\n\nSoftware Requirements: \nCloudera VM, KNIME, Spark Welcome Introduction to Machine Learning with Big Data Data Exploration Data Preparation Classification Evaluation of Machine Learning Models Regression, Cluster Analysis, and Association Analysis       ", "In this course, you will analyze and apply essential design principles to your Tableau visualizations. This course assumes you understand the tools within Tableau and have some knowledge of the fundamental concepts of data visualization. You will define and examine the similarities and differences of exploratory and explanatory analysis as well as begin to ask the right questions about what\u00e2\u0080\u0099s needed in a visualization. You will assess how data and design work together, including how to choose the appropriate visual representation for your data, and the difference between effective and ineffective visuals. You will apply effective best practice design principles to your data visualizations and be able to illustrate examples of strategic use of contrast to highlight important elements. You will evaluate pre-attentive attributes and why they are important in visualizations. You will exam the importance of using the \"right\" amount of color and in the right place and be able to apply design principles to de-clutter your data visualization. Getting Started in Effective and Ineffective Visuals Visual Perception and Cognitive Load Design Best Practices and Exploratory Analysis Design for Understanding Welcome to this first module where we are going to start you off with background information about how the human brain perceives the world and then you will discover effective and ineffective visuals. By the end of this module, you will be able to recognize how the brain relates to visual design. You will know the difference between cognitive versus perceptual design. You will learn the various visualization options offered by Tableau and some of their advantages and disadvantages. You will discuss why how good ethical practices play in designing visualizations. You will also start to examine ineffective visualizations and learn how to improve them. Welcome to this second module. This module will explore specific data visualization concepts that apply the concepts you learned about how the human brain works from the last module. In this module, you will be able to define cognitive load and what clutter means from a visualization perspective. You will be able to visually illustrate the principles of visual perception and use contrast to enhance your visualizations. You will be able to define and use pre-attentive attributes like color to make effective visualizations. In this module, we revisit some of the concepts introduced from the previous module. You will be able to apply Gestalt Principles and leverage pre-attentive attributes in your visualizations. You will examine the role of accessibility and aesthetics play in your creations. Also, you will be able to define the ideas of exploratory and explanatory analysis and be able to normalize your data and identify outliers. Finally, you will be introduced to a challenging concept and construct a control chart to set you up to perform more advanced exploratory analysis. Making sense of large, multi-dimensional data sets can be a challenge for anyone. Your task as a designer is to make good decisions about encoding, arranging, and presenting data to reveal meaningful patterns and stories for your audiences. After completing this module, you will be able to design your visualizations for a target audience and with purpose. You will be able to identify the connection to between data, relationships and good visual design. You will implement additional design tools and tips into your visualizations. \n", "This course aims to help you to draw better statistical inferences from empirical research. First, we will discuss how to correctly interpret p-values, effect sizes, confidence intervals, Bayes Factors, and likelihood ratios, and how these statistics answer different questions you might be interested in. Then, you will learn how to design experiments where the false positive rate is controlled, and how to decide upon the sample size for your study, for example in order to achieve high statistical power. Subsequently, you will learn how to interpret evidence in the scientific literature given widespread publication bias, for example by learning about p-curve analysis. Finally, we will talk about how to do philosophy of science, theory construction, and cumulative science, including how to perform replication studies, why and how to pre-register your experiment, and how to share your results following Open Science principles. \n\nIn practical, hands on assignments, you will learn how to simulate t-tests to learn which p-values you can expect, calculate likelihood ratio's and get an introduction the binomial Bayesian statistics, and learn about the positive predictive value which expresses the probability published research findings are true. We will experience the problems with optional stopping and learn how to prevent these problems by using sequential analyses. You will calculate effect sizes, see how confidence intervals work through simulations, and practice doing a-priori power analyses. Finally, you will learn how to examine whether the null hypothesis is true using equivalence testing and Bayesian statistics, and how to pre-register a study, and share your data on the Open Science Framework.\n\nAll videos now have Chinese subtitles. More than 10.000 learners have enrolled so far! Introduction + Frequentist Statistics Likelihoods & Bayesian Statistics Multiple Comparisons, Statistical Power, Pre-Registration Effect Sizes Confidence Intervals, Sample Size Justification, P-Curve analysis Philosophy of Science & Theory Open Science Final Exam        This module contains a practice exam and a graded exam. Both quizzes cover content from the entire course. We recommend making these exams only after you went through all the other modules.", "This course will introduce the learner to network analysis through tutorials using the NetworkX library. The course begins with an understanding of what network analysis is and motivations for why we might model phenomena as networks. The second week introduces the concept of connectivity and network robustness. The third week will explore ways of measuring the importance or centrality of a node in a network. The final week will explore the evolution of networks over time and cover models of network generation and the link prediction problem. \n\nThis course should be taken after: Introduction to Data Science in Python, Applied Plotting, Charting & Data Representation in Python, and Applied Machine Learning in Python. Why Study Networks and Basics on NetworkX Network Connectivity Influence Measures and Network Centralization Network Evolution Module One introduces you to different types of networks in the real world and why we study them. You'll learn about the basic elements of networks, as well as different types of networks. You'll also learn how to represent and manipulate networked data using the NetworkX library. The assignment will give you an opportunity to use NetworkX to analyze a networked dataset of employees in a small company. In Module Two you'll learn how to analyze the connectivity of a network based on measures of distance, reachability, and redundancy of paths between nodes. In the assignment, you will practice using NetworkX to compute measures of connectivity of a network of email communication among the employees of a mid-size manufacturing company.  In Module Three, you'll explore ways of measuring the importance or centrality of a node in a network, using measures such as Degree, Closeness, and Betweenness centrality, Page Rank, and Hubs and Authorities. You'll learn about the assumptions each measure makes, the algorithms we can use to compute them, and the different functions available on NetworkX to measure centrality. In the assignment, you'll practice choosing the most appropriate centrality measure on a real-world setting. In Module Four, you'll explore the evolution of networks over time, including the different models that generate networks with realistic features, such as the Preferential Attachment Model and Small World Networks. You will also explore the link prediction problem, where you will learn useful features that can predict whether a pair of disconnected nodes will be connected in the future. In the assignment, you will be challenged to identify which model generated a given network. Additionally, you will have the opportunity to combine different concepts of the course by predicting the salary, position, and future connections of the employees of a company using their logs of email exchanges. \n", ">>> By enrolling in this course you agree to the End User License Agreement as set out in the FAQ.  Once enrolled you can access the license in the Resources area <<<\n\nThis course, Applied Artificial Intelligence with DeepLearning, is part of the IBM Advanced Data Science Certificate which IBM is currently creating and gives you easy access to the invaluable insights into Deep Learning models used by experts in Natural Language Processing, Computer Vision, Time Series Analysis, and many other disciplines. We\u00e2\u0080\u0099ll learn about the fundamentals of Linear Algebra and Neural Networks. Then we introduce the most popular DeepLearning Frameworks like Keras, TensorFlow, PyTorch, DeepLearning4J and Apache SystemML. Keras and TensorFlow are making up the greatest portion of this course. We learn about Anomaly Detection, Time Series Forecasting, Image Recognition and Natural Language Processing by building up models using Keras on real-life examples from IoT (Internet of Things), Financial Marked Data, Literature or Image Databases. Finally, we learn how to scale those artificial brains using Kubernetes, Apache Spark and GPUs.\n\nIMPORTANT: THIS COURSE ALONE IS NOT SUFFICIENT TO OBTAIN THE \"IBM Watson IoT Certified Data Scientist certificate\". You need to take three other courses where two of them are currently built. The Specialization will be ready late spring, early summer 2018\n\nUsing these approaches, no matter what your skill levels in topics you would like to master, you can change your thinking and change your life. If you\u00e2\u0080\u0099re already an expert, this peep under the mental hood will give your ideas for turbocharging successful creation and deployment of DeepLearning models. If you\u00e2\u0080\u0099re struggling, you\u00e2\u0080\u0099ll see a structured treasure trove of practical techniques that walk you through what you need to do to get on track. If you\u00e2\u0080\u0099ve ever wanted to become better at anything, this course will help serve as your guide.\n\nPrerequisites: Some coding skills are necessary. Preferably python, but any other programming language will do fine. Also some basic understanding of math (linear algebra) is a plus, but we will cover that part in the first week as well.\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  To find out more about IBM digital badges follow the link ibm.biz/badging. Introduction to deep learning DeepLearning Frameworks DeepLearning Applications Scaling and Deployment    ", "This 1-week, accelerated on-demand course builds upon Google Cloud Platform Big Data and Machine Learning Fundamentals. Through a combination of video lectures, demonstrations, and hands-on labs, you'll learn how to build streaming data pipelines using Google Cloud Pub/Sub and Dataflow to enable real-time decision making. You will also learn how to build dashboards to render tailored output for various stakeholder audience.\n\nPrerequisites:\n\u00e2\u0080\u00a2 Google Cloud Platform Big Data and Machine Learning Fundamentals (or equivalent experience)\n\u00e2\u0080\u00a2 Some knowledge of Java\n\nObjectives:\n\u00e2\u0080\u00a2 Understand use-cases for real-time streaming analytics\n\u00e2\u0080\u00a2 Use Google Cloud PubSub asynchronous messaging service to manage data events\n\u00e2\u0080\u00a2 Write streaming pipelines and run transformations where necessary\n\u00e2\u0080\u00a2 Get familiar with both sides of a streaming pipeline: production and consumption\n\u00e2\u0080\u00a2 Interoperate Dataflow, BigQuery and Cloud Pub/Sub for real-time streaming and analysis\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Module 1: Architecture of Streaming Analytics Pipelines Module 2: Ingesting Variable Volumes Module 3: Implementing Streaming Pipelines Module 4: Streaming analytics and dashboards Module 5: Handling Throughput and Latency Requirements     ", "Welcome to Introduction to Statistics & Data Analysis in Public Health!\n\nThis course will teach you the core building blocks of statistical analysis - types of variables, common distributions, hypothesis testing - but, more than that, it will enable you to take a data set you've never seen before, describe its keys features, get to know its strengths and quirks, run some vital basic analyses and then formulate and test hypotheses based on means and proportions. You'll then have a solid grounding to move on to more sophisticated analysis and take the other courses in the series. You'll learn the popular, flexible and completely free software R, used by statistics and machine learning practitioners everywhere. It's hands-on, so you'll first learn about how to phrase a testable hypothesis via examples of medical research as reported by the media. Then you'll work through a data set on fruit and vegetable eating habits: data that are realistically messy, because that's what public health data sets are like in reality. There will be mini-quizzes with feedback along the way to check your understanding. The course will sharpen your ability to think critically and not take things for granted: in this age of uncontrolled algorithms and fake news, these skills are more important than ever.\n\nPrerequisites\n\nSome formulae are given to aid understanding, but this is not one of those courses where you need a mathematics degree to follow it. You will need only basic numeracy (for example, we will not use calculus) and familiarity with graphical and tabular ways of presenting results. No knowledge of R or programming is assumed. Introduction to Statistics in Public Health Types of Variables, Common Distributions and Sampling Introduction to R and RStudio Hypothesis Testing in R Statistics has played a critical role of in public health research and practice, and you\u00e2\u0080\u0099ll start by looking at two examples: one from eighteenth century London and the other by the United Nations. The first task in carrying out a research study is to define the research question and express it as a testable hypothesis. With examples from the media, you\u00e2\u0080\u0099ll see what does and does not work in this regard, giving you a chance to define a research question from some real news stories. This module will introduce you to some of the key building blocks of knowledge in statistical analysis: types of variables, common distributions and sampling. You\u00e2\u0080\u0099ll see the difference between \u00e2\u0080\u009cwell-behaved\u00e2\u0080\u009d data distributions, such as the normal and the Poisson, and real-world ones that are common in public health data sets. Now it\u00e2\u0080\u0099s time to get started with the powerful and completely free statistical software R and its popular interface RStudio. With the example of fruit and vegetable consumption, you\u00e2\u0080\u0099ll learn how to download R, import the data set and run essential descriptive analyses to get to know the variables. Having learned how to define a research question and testable hypothesis earlier in the course, you\u00e2\u0080\u0099ll learn how to apply hypothesis testing in R and interpret the result. As all medical knowledge is derived from a sample of patients, random and other kinds of variation mean that what you measure on that sample, such as the average body mass index, is not necessarily the same as in the population as a whole. It\u00e2\u0080\u0099s essential that you incorporate this uncertainty in your estimate of average BMI when presenting it. This involves the calculation of a p value and confidence interval, fundamental concepts in statistical analysis. You\u00e2\u0080\u0099ll see how to do this for averages and proportions.", "Accounting Analytics explores how financial statement data and non-financial metrics can be linked to financial performance. \u00c2\u00a0In this course, taught by Wharton\u00e2\u0080\u0099s acclaimed accounting professors, you\u00e2\u0080\u0099ll learn how data is used to assess what drives financial performance and to forecast future financial scenarios. While many accounting and financial organizations deliver data, accounting analytics deploys that data to deliver insight, and this course will explore the many areas in which accounting data provides insight into other business areas including consumer behavior predictions, corporate strategy, risk management, optimization, and more. By the end of this course, you\u00e2\u0080\u0099ll understand how financial data and non-financial data interact to forecast events, optimize operations, and determine strategy. This course has been designed to help you make better business decisions about the emerging roles of accounting analytics, so that you can apply what you\u00e2\u0080\u0099ve learned to make your own business decisions and create strategy using financial data.\u00c2\u00a0 Ratios and Forecasting Earnings Management Big Data and Prediction Models Linking Non-financial Metrics to Financial Performance The topic for this week is ratio analysis and forecasting. Since ratio analysis involves financial statement numbers, I\u00e2\u0080\u0099ve included two optional videos that review financial statements and sources of financial data, in case you need a review. We will do a ratio analysis of a single company during the module. First, we\u00e2\u0080\u0099ll examine the company's strategy and business model, and then we'll look at the DuPont analysis. Next, we\u00e2\u0080\u0099ll analyze profitability and turnover ratios followed by an analysis of the liquidity ratios for the company. Once we've put together all the ratios, we can use them to forecast future financial statements. (If you\u00e2\u0080\u0099re interested in learning more, I\u00e2\u0080\u0099ve included another optional video, on valuation). By the end of this week, you\u00e2\u0080\u0099ll be able to do a ratio analysis of a company to identify the sources of its competitive advantage (or red flags of potential trouble),  and then use that information to forecast its future financial statements.   This week we are going to examine \"earnings management\", which is the practice of trying to intentionally bias financial statements to look better than they really should look. Beginning with an overview of earnings management, we\u00e2\u0080\u0099ll cover means, motive, and opportunity: how managers actually make their earnings look better, their incentives for manipulating earnings, and how they get away with it. Then, we will investigate red flags for two different forms of revenue manipulation. Manipulating earnings through aggressive revenue recognition practices is the most common reason that companies get in trouble with government regulators for their accounting practices. Next, we will discuss red flags for manipulating earnings through aggressive expense recognition practices, which is the second most common reason that companies get in trouble for their accounting practices. By the end of this module, you\u00e2\u0080\u0099ll know how to spot earnings management and get a more accurate picture of earnings, so that you\u00e2\u0080\u0099ll be able to catch some bad guys in finance reporting! This week, we\u00e2\u0080\u0099ll use big data approaches to try to detect earnings management. Specifically, we're going to use prediction models to try to predict how the financial statements would look if there were no manipulation by the manager. First, we\u00e2\u0080\u0099ll look at Discretionary Accruals Models, which try to model the non-cash portion of earnings or \"accruals,\" where managers are making estimates to calculate revenues or expenses. Next, we'll talk about Discretionary Expenditure Models, which try to model the cash portion of earnings. Then we'll look at Fraud Prediction Models, which try to directly predict what types of companies are likely to commit frauds. Finally, we\u00e2\u0080\u0099ll explore something called Benford's Law, which examines the frequency with which certain numbers appear. If certain numbers appear more often than dictated by Benford's Law, it's an indication that the financial statements were potentially manipulated. These models represent the state of the art right now, and are what academics use to try to detect and predict earnings management. By the end of this module, you'll have a very strong tool kit that will help you try to detect financial statements that may have been manipulated by managers. Linking non-financial metrics to financial performance is one of the most important things we do as managers, and also one of the most difficult. We need to forecast future financial performance, but we have to take non-financial actions to influence it. And we must be able to accurately predict the ultimate impact on financial performance of improving non-financial dimensions. In this module, we\u00e2\u0080\u0099ll examine how to uncover which non-financial performance measures predict financial results through asking fundamental questions, such as: of the hundreds of non-financial measures, which are the key drivers of financial success? How do you rank or weight non-financial measures which don\u00e2\u0080\u0099t share a common denominator?  What performance targets are desirable? Finally, we\u00e2\u0080\u0099ll look at some comprehensive examples of how companies have used accounting analytics to show how investments in non-financial dimensions pay off in the future, and finish with some important organizational issues that commonly arise using these models. By the end of this module, you\u00e2\u0080\u0099ll know how predictive analytics can be used to determine what you should be measuring, how to weight very, very different performance measures when trying to analyze potential financial results, how to make trade-offs between short-term and long-term objectives, and how to set performance targets for optimal financial performance.", "This course introduces simple and multiple linear regression models. These models allow you to assess the relationship between variables in a data set and a continuous response variable. Is there a relationship between the physical attractiveness of a professor and their student evaluation scores? Can we predict the test score for a child based on certain characteristics of his or her mother? In this course, you will learn the fundamental theory behind linear regression and, through data examples, learn to fit, examine, and utilize regression models to examine relationships between multiple variables, using the free statistical software R and RStudio. About Linear Regression and Modeling Linear Regression More about Linear Regression Multiple Regression Final Project This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Linear Regression and Modeling. Please take several minutes to browse them through. Thanks for joining us in this course! In this week we\u00e2\u0080\u0099ll introduce linear regression. Many of you may be familiar with regression from reading the news, where graphs with straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.  Welcome to week 2! In this week, we will look at outliers, inference in linear regression and variability partitioning. Please use this week to strengthen your understanding on linear regression. Don't forget to post your questions, concerns and suggestions in the discussion forum! In this week, we\u00e2\u0080\u0099ll explore multiple regression, which allows us to model numerical response variables using multiple predictors (numerical and categorical). We will also cover inference for multiple linear regression, model selection, and model diagnostics. Hope you enjoy! In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "This course introduces you to the basic biology of modern genomics and the experimental tools that we use to measure it. We'll introduce the Central Dogma of Molecular Biology and cover how next-generation sequencing can be used to measure DNA, RNA, and epigenetic patterns. You'll also get an introduction to the key concepts in computing and data science that you'll need to understand how data from next-generation sequencing experiments are generated and analyzed.  \n\nThis is the first course in the Genomic Data Science Specialization. Overview Measurement Technology Computing Technology Data Science Technology In this Module, you can expect to study topics of \"Just enough molecular biology\", \"The genome\", \"Writing a DNA sequence\", \"Central dogma\", \"Transcription\", \"Translation\", and \"DNA structure and modifications\". In this module, you'll learn about polymerase chain reaction, next generation sequencing, and applications of sequencing. The lectures for this module cover a few basic topics in computing technology. We'll go over the foundations of computer science, algorithms, memory and data structures, efficiency, software engineering, and computational biology software. In this module on Data Science Technology, we'll be covering quite a lot of information about how to handle the data produced during the sequencing process. We'll cover reproducibility, analysis, statistics, question types, the central dogma of inference, analysis code, testing, prediction, variation, experimental design, confounding, power, sample size, correlation, causation, and degrees of freedom.", "In this course, you'll get a big-picture view of using SQL for big data, starting with an overview of data, database systems, and the common querying language (SQL). Then you'll learn the characteristics of big data and SQL tools for working on big data platforms. You'll also install an exercise environment (virtual machine) to be used through the specialization courses, and you'll have an opportunity to do some initial exploration of databases and tables in that environment.\n\nBy the end of the course, you will be able to\n\u00e2\u0080\u00a2 distinguish operational from analytic databases, and understand how these are applied in big data;\n\u00e2\u0080\u00a2 understand how database and table design provides structures for working with data;\n\u00e2\u0080\u00a2 appreciate how differences in volume and variety of data affects your choice of an appropriate database system;\n\u00e2\u0080\u00a2 recognize the features and benefits of SQL dialects designed to work with big data systems for storage and analysis; and \n\u00e2\u0080\u00a2\u00c2\u00a0explore databases and tables in a big data platform.\n\nTo use the hands-on environment for this course, you need to download and install a virtual machine and the software on which to run it. Before continuing, be sure that you have access to a computer that meets the following hardware and software requirements:\n\u00e2\u0080\u00a2\u00c2\u00a0Windows, macOS, or Linux operating system (iPads and Android tablets will not work)\n\u00e2\u0080\u00a2 64-bit operating system (32-bit operating systems will not work)\n\u00e2\u0080\u00a2 8 GB RAM or more\n\u00e2\u0080\u00a2\u00c2\u00a025GB free disk space or more\n\u00e2\u0080\u00a2 Intel VT-x or AMD-V virtualization support enabled (on Mac computers with Intel processors, this is always enabled;\non Windows and Linux computers, you might need to enable it in the BIOS)\n\u00e2\u0080\u00a2 For Windows XP computers only: You must have an unzip utility such as 7-Zip or WinZip installed (Windows XP\u00e2\u0080\u0099s built-in unzip utility will not work) Data and Databases Relational Databases and SQL Big Data SQL Tools for Big Data Analysis Introduction to the Hands-On Environment In this week, you'll get an overview of this Specialization and of Course 1. Then you'll learn about database systems and the distinction between operational and analytic databases.    ", "The simple spreadsheet is one of the most powerful data analysis tools that exists, and it\u00e2\u0080\u0099s available to almost anyone. Major corporations and small businesses alike use spreadsheet models to determine where key measures of their success are now, and where they are likely to be in the future. But in order to get the most out of a spreadsheet, you have the know-how to use it. This course is designed to give you an introduction to basic spreadsheet tools and formulas so that you can begin harness the power of spreadsheets to map the data you have now and to predict the data you may have in the future. Through short, easy-to-follow demonstrations, you\u00e2\u0080\u0099ll learn how to use Excel or Sheets so that you can begin to build models and decision trees in future courses in this Specialization. \nBasic familiarity with, and access to, Excel or Sheets is required. Spreadsheets: A Tool for Thinking with Numbers From Spreadsheet to Model Addressing Uncertainty and Probability in Models  Simulation and Optimization This module was designed to introduce you to the history of spreadsheets, their basic capabilities, and how they can be used to create models. You'll learn the different types of data used in spreadsheets, spreadsheet notations for mathematical operations, common built-in formulas and functions, conditional expressions, relative and absolute references, and how to identify and correct circular references. By the end of this module, you'll understand the context of spreadsheets, be able to navigate a spreadsheet, use built-in formulas and functions in spreadsheets, create your own simple formulas, and identify and correct common errors so you can put spreadsheets to work for you. In this module, you'll move from spreadsheet to model, so you can begin to create your own models that reflect real-world events. You'll learn how to organize and lay out model elements, as well as the types of objective functions and their use. You'll also learn what-if analysis and scenarios, sensitivity analysis, and other classic models. By the end of this module, you'll be able to design a spreadsheet reflecting assumptions, decision variables, and outcomes, create a basic cashflow model, evaluate a small business opportunity, conduct what-if analysis, identify key variables using sensitivity analysis, and linear programming models and deterministic models. This module was designed to introduce you to how you can use spreadsheets to address uncertainty and probability. You'll learn about random variables, probability distributions, power, exponential, and log functions in model formulas, models for calculating probability trees and decision trees, how to use regression tools to make predictions, as well as multiple regression. By the end of this module, you'll be able to measure correlations between variables using spreadsheet statistical functions, understand the results of functions that calculate correlations, use regression tools to make predictions, and improve forecasts with multiple regression. In this module, you'll learn to use spreadsheets to implement Monte Carlo simulations as well as linear programs for optimization. You'll examine the purpose of Monte Carlo simulations, how to implement Monte Carlo simulations in spreadsheets, the types of problems you can address with linear programs and how to implement those linear programs in spreadsheets. By the end of this module, you'll be able to model uncertainty and risk in spreadsheets, and use Excel's solver to optimize resources to reach a desired outcome.  You'll also be able to identify the similarities and differences between Excel and Sheets, and be prepared for the next course in the Business and Financial Modeling Specialization.", "This course will provide you a foundational understanding of machine learning models (logistic regression, multilayer perceptrons, convolutional neural networks, natural language processing, etc.) as well as demonstrate how these models can solve complex problems in a variety of industries, from medical diagnostics to image recognition to text prediction. In addition, we have designed practice exercises that will give you hands-on experience implementing these data science models on data sets. These practice exercises will teach you how to implement machine learning algorithms with TensorFlow, open source libraries used by leading tech companies in the machine learning field (e.g., Google, NVIDIA, CocaCola, eBay, Snapchat, Uber and many more). Simple Introduction to Machine Learning Basics of Model Learning Image Analysis with Convolutional Neural Networks Introduction to Natural Language Processing Introduction to Reinforcement Learning The focus of this module is to introduce the concepts of machine learning with as little mathematics as possible. We will introduce basic concepts in machine learning, including logistic regression, a simple but widely employed machine learning (ML) method.  Also covered is multilayered perceptron (MLP), a fundamental neural network. The concept of deep learning is discussed, and also related to simpler models.  In this module we will be discussing the mathematical basis of learning deep networks. We\u00e2\u0080\u0099ll first work through how we define the issue of learning deep networks as a minimization problem of a mathematical function. After defining our mathematical goal, we will introduce validation methods to estimate real-world performance of the learned deep networks. We will then discuss how gradient descent, a classical technique in optimization, can be used to achieve this mathematical goal. Finally, we will discuss both why and how stochastic gradient descent is used in practice to learn deep networks. This week will cover model training, as well as transfer learning and fine-tuning. In addition to learning the fundamentals of a CNN and how it is applied, careful discussion is provided on the intuition of the CNN, with the goal of providing a conceptual understanding. This week will cover the application of neural networks to natural language processing (NLP), from simple neural models to the more complex. The fundamental concept of word embeddings is discussed, as well as how such methods are employed within model learning and usage for several NLP applications. A wide range of neural NLP models are also discussed, including recurrent neural networks, and specifically long short-term memory (LSTM) models. This week will cover Reinforcement Learning, a fundamental concept in machine learning  that is concerned with taking suitable actions to maximize rewards in a particular situation. After learning the initial  steps of Reinforcement Learning, we'll move to Q Learning, as well as Deep Q Learning. We'll discuss the difference between the concepts of Exploration and Exploitation and why they are important. ", "By the end of this course, learners will understand what computer vision is, as well as its mission of making computers see and interpret the world as humans do, by learning core concepts of the field and receiving an introduction to human vision capabilities. They are equipped to identify some key application areas of computer vision and understand the digital imaging process. The course covers crucial elements that enable computer vision: digital signal processing, neuroscience and artificial intelligence. Topics include color, light and image formation; early, mid- and high-level vision; and mathematics essential for computer vision. Learners will be able to apply mathematical techniques to complete computer vision tasks. \n\nThis course is ideal for anyone curious about or interested in exploring the concepts of computer vision. It is also useful for those who desire a refresher course in mathematical concepts of computer vision. Learners should have basic programming skills and experience (understanding of for loops, if/else statements), specifically in MATLAB (Mathworks provides the basics here: https://www.mathworks.com/learn/tutorials/matlab-onramp.html). Learners should also be familiar with the following: basic linear algebra (matrix vector operations and notation), 3D co-ordinate systems and transformations, basic calculus (derivatives and integration) and basic probability (random variables).  \n\nMaterial includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing computer vision programs through online labs using MATLAB* and supporting toolboxes.\n\nThis is the first course in the Computer Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a video overview at https://youtu.be/OfxVUSCPXd0.\n\n * A free license to install MATLAB for the duration of the course is available from MathWorks. Computer Vision Overview Color, Light, & Image Formation Low-, Mid- & High-Level Vision Mathematics for Computer Vision In this module, we will discuss what computer vision is, the fields related to it, the history and key milestones of it, and some of its applications. In this module, we will discuss color, light sources, pinhole and digital cameras, and image formation. In this module, we will discuss the three-level paradigm of computer vision that was proposed by David Marr.  We will also discuss low, mid, and high level vision. In this lecture, we will discuss the Mathematics used in Computer Vision, which includes linear algebra, calculus, probability, and much more.", "Welcome to this course on Data Analytics for Lean Six Sigma. \n\nIn this course you will learn data analytics techniques that are typically useful within Lean Six Sigma improvement projects. At the end of this course you are able to analyse and interpret data gathered within such a project. You will be able to use Minitab to analyse the data. I will also briefly explain what Lean Six Sigma is.\n\nI will emphasize on use of data analytics tools and the interpretation of the outcome. I will use many different examples from actual Lean Six Sigma projects to illustrate all tools. I will not discuss any mathematical background. \n\nThe setting we chose for our data example is a Lean Six Sigma improvement project. However data analytics tools are very widely applicable. So you will find that you will learn techniques that you can use in a broader setting apart from improvement projects. \n\nI hope that you enjoy this course and good luck!\nDr. Inez Zwetsloot & the IBIS UvA team Data and Lean Six Sigma Understanding and visualizing data Using probability distributions Introduction to testing Testing: numerical Y and categorical X Testing: numerical Y and numerical Y Testing: categorical Y This module introduces Lean Six Sigma and shows you where data and data analytics have their place within the DMAIC framework.  It also introduces the software package Minitab. This package is used throughout the videos for data analytics. It is not mandatory to use this package. I just really like it! This module explains how to visualize data. It discusses visualizing single variables as well as visualizing two variables. You will learn to select the appropriate graph. For this it is essential to first learn the distinction between numerical and categorical data.  In  this module on using probability distributions, you will learn how to quantify uncertainty. Furthermore you will learn to answer an important business question:  \u00e2\u0080\u009cwhat percentage of products or cases meet our specifications?\". You will learn to model your CTQ and influence factor(s) and to use a decision tree to select the appropriate tool for data based testing of this model. Furthermore, causality is introduced. In this module on statistical testing, you will learn how to establish relationship between a numerical Y variable (the CTQ) and categorical influence factors (the X variables).  What is the relation between the length of stay and the age of a patient? In this module you will learn to answers these types of questions using statistical tests to relate a numerical CTQ (the Y variable) to a numerical influence factor (the X variable). Finally you will learn how to test a relationship between a Y and a X variable whenever your Y variable (the CTQ) is a categorical variable. ", "This course aims at providing an introductory and broad overview of the field of ML with the focus on applications on Finance. Supervised Machine Learning methods are used in the capstone project to predict bank closures. Simultaneously, while this course can be taken as a separate course, it serves as a preview of topics that are covered in more details in subsequent modules of the specialization Machine Learning and Reinforcement Learning in Finance.\nThe goal  of Guided Tour of Machine Learning in Finance is to get a sense of what Machine Learning is, what it is for and in how many different financial problems it can be applied to.\n\nThe course is designed for three categories of students:\nPractitioners working at financial institutions such as banks, asset management firms or hedge funds\nIndividuals interested in applications of ML for personal day trading\nCurrent full-time students pursuing a degree in Finance, Statistics, Computer Science, Mathematics, Physics, Engineering or other related disciplines who want to learn about practical applications of ML in Finance  \n\nExperience with Python (including numpy, pandas, and IPython/Jupyter notebooks), linear algebra, basic probability theory and basic calculus is necessary to complete assignments in this course. Artificial Intelligence & Machine Learning Mathematical Foundations of Machine Learning Introduction to Supervised Learning Supervised Learning in Finance    ", "Biostatistics is the application of statistical reasoning to the life sciences, and it is the key to unlocking the data gathered by researchers and the evidence presented in the scientific literature. In this course, we'll focus on the use of statistical measurement methods within the world of public health research. Along the way, you'll be introduced to a variety of methods and measures, and you'll practice interpreting data and performing calculations on real data from published studies.  Topics include summary measures, visual displays, continuous data, sample size, the normal distribution, binary data, the element of time, and the Kaplan-Meir curve. The Role of Statistics in Public Health Research Continuous Data Measures The Normal Distribution Binary Data Dealing with the Element of Time Course Project Within this learning module, you will learn how statistics are used within public heath research. Topics covered include study design types, data types, and data summarization. You will complete a practice quiz before completing a graded quiz.  Module two involves several lectures, a practice quiz and a graded quiz. Topics include summary statistics, visual displays, role of sample size, and continuous data.  The focus of this module is on normal distribution. Topics covered include defining the standard normal distribution, and application of principles of normal distribution to sample data. There is a practice quiz where you can test your knowledge before completing the graded quiz.  Module four covers binary data and its significance.  In addition to lectures, you will complete a practice quiz and a graded quiz.  In module five, you will explore how time is defined and studied in relation to data and learn about the Kaplan-Meir curve. In addition to a practice quiz, you will complete a graded quiz and project.   During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about asthma medication and the other about self-administration of injectable contraception. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies.", "Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data.\n\nAfter completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects. Welcome to Graph Analytics Introduction to Graphs Graph Analytics Graph Analytics Techniques Computing Platforms for Graph Analytics Meet your instructor, Amarnath Gupta and learn about the course objectives. Welcome! This week we will get a first exposure to graphs and their use in everyday life.  By the end of the module you will be able to create a graph applying core mathematical properties of graphs, and identify the kinds of analysis questions one might be able to ask of such a graph.  We hope the you will be inspired as to how graphical representations might enable you to answer new Big Data problems!  Welcome to the 4th module in the Graph Analytics course. Last week, we got a glimpse of a number of graph properties and why they are important. This week we will use those properties for analyzing graphs using a free and powerful graph analytics tool called Neo4j. We will demonstrate how to use Cypher, the query language of Neo4j, to perform a wide range of analyses on a variety of graph networks.  In the last two modules we have learned about graph analytics and graph data management. This week we will study how they come together. There are programming models and software frameworks created specifically for graph analytics.  In this module we'll give an introductory tour of these models and frameworks.  We will learn to implement what you learned in Week 2 and build on it using GraphX and Giraph.   ", "This course builds on the theory and foundations of marketing analytics and focuses on practical application by demystifying the use of data in marketing and helping you realize the power of visualizing data with artful use of numbers found in the digital space.\n\nThis course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu. Course Overview and Marketing Analytics Process Data Collection Data Analysis Data Visualization In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment. The orientation also helps you obtain the technical skills required for the course. Lesson 1 will lay out the Marketing Analysis Process (MAP), a step-by-step procedure for conducting thorough and insightful digital data analyses. Each step of MAP will be introduced with key activities. In this module, you will learn the steps of the Marketing Analytics Process, unstructured data, three primary methods for collecting web-based data and tools analysis can use to collect, store, and analyze data. As a result of these activities, you will learn extensively about planning for and collecting data for your analysis projects. In this module, you will learn about data collection, data review, data biases, and data management tools. As a result of this module\u00e2\u0080\u0099s activities, you will have an in-depth understanding of structured data and know how to ensure your data is error-free and ready for analysis. In this module, you will learn how to coax insights from data that can be used to direct marketing decisions and the effective communication of data insights to stakeholders. As a result of this module\u00e2\u0080\u0099s activities, you will gain hands-on experience in data analysis and learn to make more memorable presentations and data visualizations.", "In this course you will be introduced to the basic ideas behind the qualitative research in social science. You will learn about data collection, description, analysis and interpretation in qualitative research. Qualitative research often involves an iterative process. We will focus on the ingredients required for this process: data collection and analysis.\nYou won't learn how to use qualitative methods by just watching video's, so we put much stress on collecting data through observation and interviewing and on analysing and interpreting the collected data in other assignments.\nObviously, the most important concepts in qualitative research will be discussed, just as we will discuss quality criteria, good practices, ethics, writing some methods of analysis, and mixing methods.\nWe hope to take away some prejudice, and enthuse many students for qualitative research. Philosophy of Qualitative Research Observation Good Practices & Criteria Qualitative Interviewing Qualitative Analysis Writing, mixing & ethics Catch up week Exam week Welcome to the first week of the course. We start with an introduction, followed by two lessons on the  Philosophy of Qualitative Research. In the first module we discussed the philosophy of qualitative research, explaining some basic notions and general philosophical approaches. In this second module we'll discuss observation as an important method within qualitative research. What types of observation are there? How do we observe? And how do we analyse and describe our data?  What makes qualitative research 'good' is a rather difficult question. Different criteria are suggested, but within the field of qualitative research there is not much agreement on these criteria. However, there is quite some agreement on what good practices of qualitative research are. In this module we will start in lesson 1 with a  discussion of  good practices of qualitative research. In this module we'll look at what a qualitative interview entails by trying to define it and by discussing different forms of interviewing behaviour.  In previous modules we discussed how you should observe a social situation or conduct a qualitative interview. Now we will focus on what to do with your data, by discussing qualitative analysis. In this module you will try to do a qualitative analysis by interpreting your observed data and try to code it.  In this module I will discuss ideas on writing in qualitative research, I will discuss mixing methods  and talk about the ethical issues you should consider.  In this module there's no new material. The only requirement in this module is that you finish up the final peer review assignment. This is the final module, where you can apply everything you've learned up until now in the final exam. Please note that you can only take the final exam once every day, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others taking this exam. Good luck! Once you've taken the exam, please consider doing the other courses in our specialisation track. I hope it was an enjoyable experience. If it was, please consider joining in with the Massive Open Online Research by my colleague Christian Br\u00c3\u00b6er. Thanks for all your hard work, feedback and interpretations, the course team and your fellow learners really appreciate it!", "This course is intended to be an introduction to machine learning for non-technical business professionals. There is a lot of hype around machine learning and many people are concerned that in order to use machine learning in business, you need to have a technical background. For reasons that are covered in this course, that's not the case. In actuality, your knowledge of your business is far more important than your ability to build an ML model from scratch.\n\nBy the end of this course, you will have learned how to:\n\u00e2\u0080\u00a2 Formulate machine learning solutions to real-world problems\n\u00e2\u0080\u00a2 Identify whether the data you have is sufficient for ML\n\u00e2\u0080\u00a2 Carry a project through various ML phases including training, evaluation, and deployment\n\u00e2\u0080\u00a2 Perform AI responsibly and avoid reinforcing existing bias\n\u00e2\u0080\u00a2 Discover ML use cases\n\u00e2\u0080\u00a2 Be successful at ML\n\nYou'll need a desktop web browser to run this course's interactive labs via Qwiklabs and Google Cloud Platform.\n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<< Introduction What is Machine Learning? Employing ML Discovering ML Use Cases How to be successful at ML Summary This module reviews the learning objectives for the course and introduces technology that will be important for completing labs. This module defines what machine learning is, provides examples of how businesses are using it, contextualizes recent advances in machine learning, and reviews how artificial intelligence raises important ethical questions. This module reviews how to do machine learning, including how to label data, train and evaluate models and avoid reinforcing bias. This module reviews broad categories of ML use cases in order to jump start your ideation. This module reviews what your business must do in order to be successful at ML, including how to acquire data, how to appropriately govern that data, and how to create a culture of innovation. This module reviews the content in the course.", "This course describes Bayesian statistics, in which one's inferences about parameters or hypotheses are updated as evidence accumulates. You will learn to use Bayes\u00e2\u0080\u0099 rule to transform prior probabilities into posterior probabilities, and be introduced to the underlying theory and perspective of the Bayesian paradigm. The course will apply Bayesian methods to several practical problems, to show end-to-end Bayesian analyses that move from framing the question to building models to eliciting prior probabilities to implementing in R (free statistical software) the final posterior distribution. Additionally, the course will introduce credible regions, Bayesian comparisons of means and proportions, Bayesian regression and inference using multiple models, and discussion of Bayesian prediction.\n\nWe assume learners in this course have background knowledge equivalent to what is covered in the earlier three courses in this specialization: \"Introduction to Probability and Data,\" \"Inferential Statistics,\" and \"Linear Regression and Modeling.\" About the Specialization and the Course The Basics of Bayesian Statistics Bayesian Inference Decision Making Bayesian Regression Perspectives on Bayesian Applications Data Analysis Project This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Bayesian Statistics. Please take several minutes read this information. Thanks for joining us in this course! <p>Welcome! Over the next several weeks, we will together explore Bayesian statistics. <p>In this module, we will work with conditional probabilities, which is the probability of event B given event A. Conditional probabilities are very important in medical decisions. By the end of the week, you will be able to solve problems using Bayes' rule, and update prior probabilities.</p><p>Please use the learning objectives and practice quiz to help you learn about Bayes' Rule, and apply what you have learned in the lab and on the quiz.  In this week, we will discuss the continuous version of Bayes' rule and show you how to use it in a conjugate family, and discuss credible intervals. By the end of this week, you will be able to understand and define the concepts of prior, likelihood, and posterior probability and identify how they relate to one another. In this module, we will discuss Bayesian decision making, hypothesis testing, and Bayesian testing. By the end of this week, you will be able to make optimal decisions based on Bayesian statistics and compare multiple hypotheses using Bayes Factors.  This week, we will look at Bayesian linear regressions and model averaging, which allows you to make inferences and predictions using several models. By the end of this week, you will be able to implement Bayesian model averaging, interpret Bayesian multiple linear regression and understand its relationship to the frequentist linear regression approach.  This week consists of interviews with statisticians on how they use Bayesian statistics in their work, as well as the final project in the course. In this module you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. ", "This is the second of a two-course sequence introducing the fundamentals of Bayesian statistics. It builds on the course Bayesian Statistics: From Concept to Data Analysis, which introduces Bayesian methods through use of simple conjugate models. Real-world data often require more sophisticated models to reach realistic conclusions. This course aims to expand our \u00e2\u0080\u009cBayesian toolbox\u00e2\u0080\u009d with more general models, and computational techniques to fit them. In particular, we will introduce Markov chain Monte Carlo (MCMC) methods, which allow sampling from posterior distributions that have no analytical solution. We will use the open-source, freely available software R (some experience is assumed, e.g., completing the previous course in R) and JAGS (no experience required). We will learn how to construct, fit, assess, and compare Bayesian statistical models to answer scientific questions involving continuous, binary, and count data. This course combines lecture videos, computer demonstrations, readings, exercises, and discussion boards to create an active learning experience. The lectures provide some of the basic mathematical development, explanations of the statistical modeling process, and a few basic modeling techniques commonly used by statisticians. Computer demonstrations provide concrete, practical walkthroughs. Completion of this course will give you access to a wide range of Bayesian analytical tools, customizable to your data. Statistical modeling and Monte Carlo estimation Markov chain Monte Carlo (MCMC) Common statistical models Count data and hierarchical modeling Capstone project Statistical modeling, Bayesian modeling, Monte Carlo estimation Metropolis-Hastings, Gibbs sampling, assessing convergence Linear regression, ANOVA, logistic regression, multiple factor ANOVA Poisson regression, hierarchical modeling Peer-reviewed data analysis project", "This course introduces an overview of financial analytics. You will learn why, when, and how to apply financial analytics in real-world situations. You will explore techniques to analyze time series data and how to evaluate the risk-reward trade off expounded in modern portfolio theory. While most of the focus will be on the prices, returns, and risk of corporate stocks, the analytical techniques can be leverages in other domains. Finally, a short introduction to algorithmic trading concludes the course.\n\nAfter completing this course, you should be able to understand time series data, create forecasts, and determine the efficacy of the estimates. Also, you will be able to create a portfolio of assets using actual stock price data while optimizing risk and reward. Understanding financial data is an important skill as an analyst, manager, or consultant. Course Introduction Module 1: Introduction to Financial Analytics and Time Series Data Module 2: Performance Measures and Holt-Winters Model Module 3: Stationarity and ARIMA Model Module 4: Modern Portfolio Theory and Intro to Algorithmic Trading In this course, we will introduce a number of financial analytic techniques. You will learn why, when, and how to apply financial analytics in real-world situations. We will explore techniques to analyze time series data and how to evaluate the risk-reward trade off expounded in modern portfolio theory. While most of the focus will be on the prices, returns, and risks of corporate stocks, the analytical techniques can be leveraged in other domains. Finally, a short introduction to algorithmic trading concludes the course. In this module, we will introduce an overview of financial analytics. Students will learn why, when, and how to apply financial analytics in real-world situations. We will explore techniques to analyze time series data and how to evaluate the risk-reward trade off expounded in modern portfolio theory. While most of our focus will be on the prices, returns, and risks of corporate stocks, the analytical techniques can be leveraged in other domains. Finally, a short introduction to algorithmic trading concludes the course. We will introduce analytical methods to analyze time series data to build forecasting models and support decision-making. Students will learn how to analyze financial data that is usually presented as time series data. Topics include forecasting performance  measures, moving average, exponential smoothing methods, and the Holt-Winters method. In this module, we will begin with stationarity, the first and necessary step in analyzing time series data. Students will learn how to identify if a time series is stationary or not and know how to make nonstationary data become stationary. Next, we will study a basic forecasting model: ARIMA. Students will learn how to build an ARIMA forecasting model using R. We will introduce some basic measurements of modern portfolio theory. Students will understand about risk and returns, how to balance them, and how to evaluate an investment portfolio.", "Once you\u00e2\u0080\u0099ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions?  In this course, you will experience various data genres and management tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples.  Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources.\n\nAt the end of this course, you will be able to:\n * Recognize different data elements in your own work and in everyday life problems\n * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design\n * Identify the frequent data operations required for various types of data\n * Select a data model to suit the characteristics of your data \n * Apply techniques to handle streaming data\n * Differentiate between a traditional Database Management System and a Big Data Management System\n * Appreciate why there are so many data management systems\n * Design a big data information system for an online game company\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking \u00e2\u0080\u009cAbout This Mac.\u00e2\u0080\u009d Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Introduction to Big Data Modeling and Management Big Data Modeling Big Data Modeling (Part 2) Working With Data Models Big Data Management: The \"M\" in DBMS Designing a Big Data Management System for an Online Game Welcome to this course on big data modeling and management. Modeling and managing data is a central focus of all big data projects. In these lessons we introduce you to the concepts behind big data modeling and management and set the stage for the remainder of the course.  Modeling big data depends on many factors including data structure, which operations may be performed on the data, and what constraints are placed on the models. In these lessons you will learn the details about big data modeling and you will gain the practical skills you will need for modeling your own big data projects. These lessons continue to shed light on big data modeling with specific approaches including vector space models, graph data models, and more.  Data models deal with many different types of data formats. Streaming data is becoming ubiquitous, and working with streaming data requires a different approach from working with static data. In these lessons you will gain practical hands-on experience working with different forms of streaming data including weather data and twitter feeds.  Managing big data requires a different approach to database management systems because of the wide variation in data structure which does not lend itself to traditional DBMSs. There are many applications available to help with big data management. In these lessons we introduce you to some of these applications and provide insight into how and when they might be appropriate for your own big data management challenges.  In these lessons we give you the opportunity to learn about big data modeling and management using a fictitious online game called \"Catch the Pink Flamingo\". ", "We have all heard the phrase \u00e2\u0080\u009ccorrelation does not equal causation.\u00e2\u0080\u009d  What, then, does equal causation?  This course aims to answer that question and more!  \n\nOver a period of 5 weeks, you will learn how causal effects are defined, what assumptions about your data and models are necessary, and how to implement and interpret some popular statistical methods.  Learners will have the opportunity to apply these methods to example data in R (free statistical software environment).\n\nAt the end of the course, learners should be able to:\n1.  Define causal effects using potential outcomes\n2.  Describe the difference between association and causation\n3.  Express assumptions with causal graphs\n4.  Implement several types of causal inference methods (e.g. matching, instrumental variables, inverse probability of treatment weighting)\n5.  Identify which causal assumptions are necessary for each type of statistical method\n\nSo join us.... and discover for yourself why modern statistical methods for estimating causal effects are indispensable in so many fields of study! Welcome and Introduction to Causal Effects Confounding and Directed Acyclic Graphs (DAGs) Matching and Propensity Scores Inverse Probability of Treatment Weighting (IPTW) Instrumental Variables Methods This module focuses on defining causal effects using potential outcomes. A key distinction is made between setting/manipulating values and conditioning on variables. Key causal identifying assumptions are also introduced. This module introduces directed acyclic graphs. By understanding various rules about these graphs, learners can identify whether a set of variables is sufficient to control for confounding. An overview of matching methods for estimating causal effects is presented, including matching directly on confounders and matching on the propensity score. The ideas are illustrated with data analysis examples in R. Inverse probability of treatment weighting, as a method to estimate causal effects, is introduced. The ideas are illustrated with an IPTW data analysis in R. This module focuses on causal effect estimation using instrumental variables in both randomized trials with non-compliance and in observational studies. The ideas are illustrated with an instrumental variables analysis in R.", ">>> By enrolling in this course you agree to the End User License Agreement as set out in the FAQ.  Once enrolled you can access the license in the Resources area <<<\n\nThis course, Advanced Machine Learning and Signal Processing, is part of the IBM Advanced Data Science Specialization which IBM is currently creating and gives you easy access to the invaluable insights into Supervised and Unsupervised Machine Learning Models used by experts in many field relevant disciplines. We\u00e2\u0080\u0099ll learn about the fundamentals of Linear Algebra to understand how machine learning modes work. Then we introduce the most popular Machine Learning Frameworks for python Scikit-Learn and SparkML. SparkML is making up the greatest portion of this course since scalability is key to address performance bottlenecks. We learn how to tune the models in parallel by evaluating hundreds of different parameter-combinations in parallel. We\u00e2\u0080\u0099ll continuously use a real-life example from IoT (Internet of Things), for exemplifying the different algorithms. For passing the course you are even required to create your own vibration sensor data using the accelerometer sensors in your smartphone. So you are actually working on a self-created, real dataset throughout the course.\n\nIf you choose to take this course and earn the Coursera course certificate, you will also earn an IBM digital badge.  To find out more about IBM digital badges follow the link ibm.biz/badging. Setting the stage Supervised Machine Learning Unsupervised Machine Learning Digital Signal Processing in Machine Learning    ", "The ability to understand and apply Business Statistics is becoming increasingly important in the industry. A good understanding of Business Statistics is a requirement to make correct and relevant interpretations of data. Lack of knowledge could lead to erroneous decisions which could potentially have negative consequences for a firm. This course is designed to introduce you to Business Statistics. We begin with the notion of descriptive statistics, which is summarizing data using a few numbers. Different categories of descriptive measures are introduced and discussed along with the Excel functions to calculate them. The notion of probability or uncertainty is introduced along with the concept of a sample and population data using relevant business examples. This leads us to various statistical distributions along with their Excel functions which are then used to model or approximate business processes. You get to apply these descriptive measures of data and various statistical distributions using easy-to-follow Excel based examples which are demonstrated throughout the course.\n\nTo successfully complete course assignments, students must have access to Microsoft Excel. \n________________________________________\nWEEK 1\nModule 1: Basic Data Descriptors\nIn this module you will get to understand, calculate and interpret various descriptive or summary measures of data. These descriptive measures summarize and present data using a few numbers. Appropriate Excel functions to do these calculations are introduced and demonstrated.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tCategories of descriptive data\n\u00e2\u0080\u00a2\tMeasures of central tendency, the mean, median, mode, and their interpretations and calculations\n\u00e2\u0080\u00a2\tMeasures of spread-in-data, the range, interquartile-range, standard deviation and variance\n\u00e2\u0080\u00a2\tBox plots\n\u00e2\u0080\u00a2\tInterpreting the standard deviation measure using the rule-of-thumb and Chebyshev\u00e2\u0080\u0099s theorem\n________________________________________\nWEEK 2\nModule 2: Descriptive Measures of Association, Probability, and Statistical Distributions\nThis module presents the covariance and correlation measures and their respective Excel functions. You get to understand the notion of causation versus correlation. The module then introduces the notion of probability and random variables and starts introducing statistical distributions.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tMeasures of association, the covariance and correlation measures; causation versus correlation\n\u00e2\u0080\u00a2\tProbability and random variables; discrete versus continuous data\n\u00e2\u0080\u00a2\tIntroduction to statistical distributions\n________________________________________\nWEEK 3\nModule 3: The Normal Distribution\nThis module introduces the Normal distribution and the Excel function to calculate probabilities and various outcomes from the distribution. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tProbability density function and area under the curve as a measure of probability\n\u00e2\u0080\u00a2\tThe Normal distribution (bell curve), NORM.DIST, NORM.INV functions in Excel\n________________________________________\nWEEK 4\nModule 4: Working with Distributions, Normal, Binomial, Poisson\nIn this module, you'll see various applications of the Normal distribution. You will also get introduced to the Binomial and Poisson distributions. The Central Limit Theorem is introduced and explained in the context of understanding sample data versus population data and the link between the two.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tVarious applications of the Normal distribution\n\u00e2\u0080\u00a2\tThe Binomial and Poisson distributions\n\u00e2\u0080\u00a2\tSample versus population data; the Central Limit Theorem Basic Data Descriptors Descriptive Measures of Association, Probability, and Statistical Distributions The Normal Distribution Working with Distributions (Normal, Binomial, Poisson), Population and Sample Data    ", "This course provides a deep dive into how to create a chatbot using Dialogflow, augment it with Cloud Natural Language API, and operationalize it using Google Cloud tools.\n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<< Welcome to the course Building conversational experiences with Dialogflow Defining Intents and Entities Maintaining context and taking actions Taking your chatbot to production Additional Features Summary This module introduces the course agenda. This module gives an introduction of conversational agents (chatbots), reviews the evolution of, and  challenges designing for rich natural language, conversational agents, and introduces Dialogflow as a tool for such experiences. This module covers the basic building blocks of designing a conversational agent in Dialogflow This module continues to extend your agent's capabilities to be able to understand context and fulfill actions. This module takes a look at how you can operationalize your agent...in other words what you need to do to take your chatbot into production.  This module discusses some of the beta features that you can experiment with you to further enhance your chatbot This module covers a summary of all the topics so far", "We live in an uncertain and complex world, yet we continually have to make decisions in the present with uncertain future outcomes.  Indeed, we should be on the look-out for \"black swans\" - low-probability high-impact events.\n\nTo study, or not to study?  To invest, or not to invest?  To marry, or not to marry?\n\nWhile uncertainty makes decision-making difficult, it does at least make life exciting!  If the entire future was known in advance, there would never be an element of surprise.  Whether a good future or a bad future, it would be a known future.\n\nIn this course we consider many useful tools to deal with uncertainty and help us to make informed (and hence better) decisions - essential skills for a lifetime of good decision-making.\n\nKey topics include quantifying uncertainty with probability, descriptive statistics, point and interval estimation of means and proportions, the basics of hypothesis testing, and a selection of multivariate applications of key terms and concepts seen throughout the course. Dealing with Uncertainty and Complexity in a Chaotic World Quantifying Uncertainty With Probability Describing The World The Statistical Way On Your Marks, Get Set, Infer! To p Or Not To p? Applications      ", "Learn how to model social and economic networks and their impact on human behavior.  How do networks form, why do they exhibit certain patterns, and how does their structure impact diffusion, learning, and other behaviors?   We will bring together models and techniques from economics, sociology, math, physics, statistics and computer science to answer these questions.\n\nThe course begins with some empirical background on social and economic networks, and an overview of concepts used to describe and measure networks. Next, we will cover a set of models of how networks form, including random network models as well as strategic formation models, and some hybrids. We will then discuss a series of models of how networks impact behavior, including contagion, diffusion, learning, and peer influences.\n\nYou can find a more detailed syllabus here:  http://web.stanford.edu/~jacksonm/Networks-Online-Syllabus.pdf \n\nYou can find a short introductory videao here: http://web.stanford.edu/~jacksonm/Intro_Networks.mp4 Introduction, Empirical Background and Definitions   Background, Definitions, and Measures Continued Random Networks Strategic Network Formation Diffusion on Networks Learning on Networks Games on Networks Final Exam Examples of Social Networks and their Impact, Definitions, Measures and Properties: Degrees, Diameters, Small Worlds, Weak and Strong Ties, Degree Distributions Homophily, Dynamics, Centrality Measures: Degree, Betweenness, Closeness, Eigenvector, and Katz-Bonacich. Erdos and Renyi Random Networks: Thresholds and Phase Transitions Poisson Random Networks, Exponential Random Graph Models, Growing Random Networks, Preferential Attachment and Power Laws, Hybrid models of Network Formation. Game Theoretic Modeling of Network Formation, The Connections Model, The Conflict between Incentives and Efficiency, Dynamics, Directed Networks, Hybrid Models of Choice and Chance. Empirical Background, The Bass Model, Random Network Models of Contagion, The SIS model, Fitting a Simulated Model to Data. Bayesian Learning on Networks, The DeGroot Model of Learning on a Network, Convergence of Beliefs, The Wisdom of Crowds, How Influence depends on Network Position.. Network Games, Peer Influences: Strategic Complements and Substitutes, the Relation between Network Structure and Behavior, A Linear Quadratic Game, Repeated Interactions and Network Structures. The description goes here", "This four-module course introduces users to Julia as a first language.  Julia is a high-level, high-performance dynamic programming language developed specifically for scientific computing. This language will be particularly useful for applications in physics, chemistry, astronomy, engineering, data science, bioinformatics and many more. As open source software, you will always have it available throughout your working life. It can also be used from the command line, program files or a new type of interface known as a Jupyter notebook (which is freely available as a service from JuliaBox.com).\n\nJulia is designed to address the requirements of high-performance numerical and scientific computing while also being effective for general-purpose programming. You will be able to access all the available processors and memory, scrape data from anywhere on the web, and have it always accessible through any device you care to use as long as it has a browser.  Join us to discover new computing possibilities. Let's get started on learning Julia.\n\nBy the end of the course you will be able to:\n- Programme using the Julia language by practising through assignments\n- Write your own simple Julia programs from scratch\n- Understand the advantages and capacities of Julia as a computing language\n- Work in Jupyter notebooks using the Julia language\n- Use various Julia packages such as  Plots, DataFrames and Stats\n\nThe course is delivered through video lectures, on-screen demonstrations, quizzes and practical peer-reviewed projects designed to give you an opportunity to work with the packages. Welcome to the course A context for exploring Julia: Working with data Notebooks as Julia Programs Structuring data and functions in Julia A warm welcome to Julia Scientific Programming. Over the next four weeks, we will provide you with an introduction to what Julia can offer. This will allow you to learn the basics of the language, and stimulate your imagination about how you can use Julia in your own context. This is all about you exploring Julia - we can only demonstrate some of the capacity and encourage you to take the first steps. For those of you with a programming background, the course is intended to offer a jumpstart into using this language. If you are a novice or beginner programmer, you should follow along the simple coding but recognising that working through the material will not be sufficient to make you a proficient programmer in four weeks. You could see this as the \u00e2\u0080\u0098first date\u00e2\u0080\u0099 at the beginning of a long and beautiful new relationship. There is so much you will need to learn and discover. Good luck and we hope you enjoy the course! Best wishes, Henri and Juan In our case study we use Julia to store, plot, select and slice data from the Ebola epidemic. Taking real data, we explain how to work in Julia using arrays, and for loops to work with the structures. By the end of this module, you will be able to: create an array from data;  learn to use the logical structures IF  and FOR ; conduct basic array slicing, getting the incidence data and generating total number of cases; use Plots to generate graphs and plot data; and combine the Ebola data outputs to show a plot of disease incidence in several countries. in this week, we demonstrate how it is possible to use Julia in the notebook environment to interpret a model and its fit to the data from the Ebola outbreak. For this, we apply the well-known SIR compartmental model in epidemiology. The SIR model labels three compartments, namely S = number susceptible, I =number infectious, and R =number recovered. By the end of this module, you will be able to: understand the SIR models; describe the basic parameters of an SIR model; plot the model-predicted curve and the data on the same diagram; adjust the parameters of the model so the model-predicted curve is close (or rather as close as you can make it) to the data. As a scientific computing language, Julia has many applications and is particularly well suited to the task of working with data.  In this last module, we will use descriptive statistics as our topic to explore the power of Julia. You should see this week as offering you a chance to further explore concepts introduced in week one and two. You will also be introduced to more efficient ways of managing and visualizing your data. We have also included additional, honors material for those who want to explore further with Julia around functions and collections.   By the end of this module, you will be able to:  1. Practice basic functions in Julia  2.Creating random variables from data point values 3. Build your own  Dataframes 4. Create a variety of data visualisations 5. Conduct statistical tests 6. learn how to export your data. \n", "Case Studies: Finding Similar Documents\n\nA reader is interested in a specific news article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents cover?   \n\nIn this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA).  You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.\n\nLearning Outcomes:  By the end of this course, you will be able to:\n   -Create a document retrieval system using k-nearest neighbors.\n   -Identify various similarity metrics for text data.\n   -Reduce computations in k-nearest neighbor search by using KD-trees.\n   -Produce approximate nearest neighbors using locality sensitive hashing.\n   -Compare and contrast supervised and unsupervised learning tasks.\n   -Cluster documents by topic using k-means.\n   -Describe how to parallelize k-means using MapReduce.\n   -Examine probabilistic clustering approaches using mixtures models.\n   -Fit a mixture of Gaussian model using expectation maximization (EM).\n   -Perform mixed membership modeling using latent Dirichlet allocation (LDA).\n   -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.\n   -Compare and contrast initialization techniques for non-convex optimization objectives.\n   -Implement these techniques in Python. Welcome Nearest Neighbor Search Clustering with k-means Mixture Models Mixed Membership Modeling via Latent Dirichlet Allocation Hierarchical Clustering & Closing Remarks Clustering and retrieval are some of the most high-impact machine learning tools out there.  Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform.  Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading.  We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses.  However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints.  You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces.  You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced. In clustering, our goal is to group the datapoints in our dataset into disjoint sets.  Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by \"topic\".  These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like \"Science\", \"World News\", etc.  Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset.  The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there.  To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework.  You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned. In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information.  In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a \"cluster\" and (2) accounts for uncertainty in assignments of datapoints to clusters via \"soft assignments\".  You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters.  To gain intuition, you will first consider a visually appealing image clustering task.  You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered. The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic.  But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics.  In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis.  You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features.  The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.<p>Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling.  You will be able to implement a Gibbs sampler for LDA by the end of the module. In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.<p>We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset.  Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series.  We then briefly outline some important clustering and retrieval ideas that we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  ", "Get started learning about the fascinating and useful world of geographic information systems (GIS)! In this first course of the specialization GIS, Mapping, and Spatial Analysis, you'll learn about what a GIS is, how to get started with the software yourself, how things we find in the real world can be represented on a map, how we record locations using coordinates, and how we can make a two-dimensional map from a three-dimensional Earth. In the course project, you will create your own GIS data by tracing geographic features from a satellite image for a location and theme of your choice. This course will give you a strong foundation in mapping and GIS that will give you the understanding you need to start working with GIS, and to succeed in the other courses in this specialization.\n\nThis course is for anyone who wants to learn about mapping and GIS. You don't have to have any previous experience - just your curiosity! The course includes both practical software training and explanations of the concepts you need to know to make informed decisions as you start your journey to becoming a GIS analyst.\n\nYou will need a Windows computer with ArcGIS Desktop installed. What is a GIS? Introduction to ArcGIS Mapping the real world with vector and raster data Mapping Locations with Coordinate Systems Flattening the Earth with Map Projections Project: Creating Your Own Data      ", "This course focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner. Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.  The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. This course will focus on literate statistical analysis tools which allow one to publish data analyses in a single document that allows others to easily execute the same analysis to obtain the same results. Week 1: Concepts, Ideas, & Structure Week 2: Markdown & knitr Week 3: Reproducible Research Checklist & Evidence-based Data Analysis Week 4: Case Studies & Commentaries This week will cover the basic ideas of reproducible research since they may be unfamiliar to some of you. We also cover structuring and organizing a data analysis to help make it more reproducible. I recommend that you watch the videos in the order that they are listed on the web page, but watching the videos out of order isn't going to ruin the story.  This week we cover some of the core tools for developing reproducible documents. We cover the literate programming tool knitr and show how to integrate it with Markdown to publish reproducible web documents. We also introduce the first peer assessment which will require you to write up a reproducible data analysis using knitr.  This week covers what one could call a basic check list for ensuring that a data analysis is reproducible. While it's not absolutely sufficient to follow the check list, it provides a necessary minimum standard that would be applicable to almost any area of analysis. This week there are two \ncase studies involving the importance of reproducibility in science for you to watch.", "This course introduces students to the science of business analytics while casting a keen eye toward the artful use of numbers found in the digital space. The goal is to provide businesses and managers with the foundation needed to apply data analytics to real-world challenges they confront daily in their professional lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize data; and utilize data in decision making for their agencies, organizations or clients. Module 1: Pictures You See with Your Brain Module 2: Working Fast and Thinking Slow Module 3: Finding Your Data Story Module 4: Getting Your Story Across This module, will help you earn the foundation needed to become a good data communicator. After completion of this module, you will know the history of data visualizaiton, understand today\u00e2\u0080\u0099s dataviz tools, make connections with visuals, and evaluate the effectiveness of data visualizations. This module will walk you through various methods to access data. The data could be either publicly available or company internal. The module also reviews why it is important for analysts to define clear objectives for their analysis. You will also be introduced to frameworks that help guide digital marketing analysis and their importance to data visualization. In this module, you will learn an approach to finding patterns in data through visualization. You will see how charts can be used to communicate messages that can be conceptual or data driven and declarative or exploratory. Next, we will learn how to use different charting techniques to reveal data patterns. Visual aids play a major role in identifying ideas and, as a result, we will learn how to match the appropriate visual technique to the ideas we hope to expose. Next, we will explore analysis tools. RStudio is one of the most commonly used tools, and we will explore its visual capabilities. Finally, we will discuss a framework that will help analysts effectively plan for data collection, analysis, and, ultimately, visualization.  This module explores telling stories, through data, that connect emotionally with your audience. It will also review examples and figures that make the concept easy to understand. You will learn the major do\u00e2\u0080\u0099s and don\u00e2\u0080\u0099ts of creating dataviz and rules that lead to the clear depiction of your findings. This unit specifically focuses on Dona Wong\u00e2\u0080\u0099s guidelines for good data visualization and charts. The last leg of Module 4 teaches the three tests that help you improve your visualization. In the final step of dataviz execution, you will learn the McCandless Method for presenting visualizations. This five-step process produces the most effective communication of the graphics to your audience.", "This course will provide you with a basic, intuitive and practical introduction into Probability Theory. You will be able to learn how to apply Probability Theory in different scenarios and you will earn a \"toolbox\" of methods to deal with uncertainty in your daily life. \n\nThe course is split in 5 modules. In each module you will first have an easy introduction into the topic, which will serve as a basis to further develop your knowledge about the topic and acquire the \"tools\" to deal with uncertainty. Additionally, you will have the opportunity to complete 5 exercise sessions to reflect about the content learned in each module and start applying your earned knowledge right away. \n\nThe topics covered are: \"Probability\", \"Conditional Probability\", \"Applications\", \"Random Variables\", and \"Normal Distribution\".\n\nYou will see how the modules are taught in a lively way, focusing on having an entertaining and useful learning experience! We are looking forward to see you online! Probability Conditional Probability Application Discrete Random Variables Normal Distribution In this module we will learn about probabilities and perform our first calculations using probability formulas. We want to get comfortable with the idea that probabilities describe the chance of uncertain events occurring. The arrival of new information may lead us to alter our probabilistic assessments of uncertain events. In this module, we will learn how the concept of \"conditional\" probabilities allows us to make these changes correctly. Script of the course can be found here: https://www.coursera.org/learn/introductiontoprobability/resources/qxi9W  We will discuss some fascinating every-day applications of probability. In addition to entertaining examples, we will also review very serious applications from finance and law. In this module we move beyond probabilities and learn about important summary measures such as expected values, variances, and standard deviations. We also learn about the most popular discrete probability distribution, the binomial distribution. We want to get comfortable with the normal distribution. We will discuss what the famous bell curve really represents. And we will learn how easy it is to calculate normal probabilities.", "Want to know how you can improve the accuracy of your machine learning models? What about how to find which data columns make the most useful features? Welcome to Feature Engineering on Google Cloud Platform where we will discuss the elements of good vs bad features and how you can preprocess and transform them for optimal use in your machine learning models.\n\nIn this course you will get hands-on practice choosing features and preprocessing them inside of Google Cloud Platform with interactive labs. Our instructors will walk you through the code solutions which will also be made public for your reference as you work on your own future data science projects.\n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Raw Data to Features Preprocessing and Feature Creation Feature Crosses TF Transform Summary Want to know how you can improve the accuracy of your ML models? What about how to find which data columns make the most useful features? Welcome to Feature Engineering where we will discuss good vs bad features and how you can preprocess and transform them for optimal use in your models. Feature engineering is often the longest and most difficult phase of building your ML project. In the feature engineering process, you start with your raw data and use your own domain knowledge to create features that will make your machine learning algorithms work. In this module we explore what makes a good feature and how to represent them in your ML model. This section of the module covers pre-processing and feature creation which are data processing techniques that can help you prepare a feature set for a machine learning system.\n In traditional machine learning, feature crosses don\u00e2\u0080\u0099t play much of a role, but in modern day ML methods, feature crosses are an invaluable part of your toolkit.In this module, you will learn how to recognize the kinds of problems where feature crosses are a powerful way to help machines learn. \nTensorFlow Transform (tf.Transform) is a library for preprocessing data with TensorFlow. tf.Transform is useful for preprocessing that requires a full pass the data, such as:\n\n- normalizing an input value by mean and stdev\n- integerizing a vocabulary by looking at all input examples for values\n- bucketizing inputs based on the observed data distribution\n\nIn this module we will explore use cases for tf.Transform. Here we recap the major points you learned in each module on Feature Engineering: Selecting Good Features, Preprocessing at Scale, Using Feature Crosses, and Practicing with TensorFlow.", "This course, which is designed to serve as the first course in the Recommender Systems specialization, introduces the concept of recommender systems, reviews several examples in detail, and leads you through non-personalized recommendation using summary statistics and product associations, basic stereotype-based or demographic recommendations, and content-based filtering recommendations. \n\nAfter completing this course, you will be able to compute a variety of recommendations from datasets using basic spreadsheet tools, and if you complete the honors track you will also have programmed these recommendations using the open source LensKit recommender toolkit.  \n\nIn addition to detailed lectures and interactive exercises, this course features interviews with several leaders in research and practice on advanced topics and current directions in recommender systems. Preface Introducing Recommender Systems Non-Personalized and Stereotype-Based Recommenders Content-Based Filtering -- Part I Content-Based Filtering -- Part II Course Wrap-up This brief module introduces the topic of recommender systems (including placing the technology in historical context) and provides an overview of the structure and coverage of the course and specialization. This module introduces recommender systems  in more depth.  It includes a detailed taxonomy of the types of recommender systems, and also includes tours of two systems heavily dependent on recommender technology:  MovieLens and Amazon.com. There is an introductory assessment in the final lesson to ensure that you understand the core concepts behind recommendations before we start learning how to compute them. In this module, you will learn several techniques for non- and lightly-personalized recommendations, including how to use meaningful summary statistics, how to compute product association recommendations, and how to explore using demographics as a means for light personalization.  There is both an assignment (trying out these techniques in a spreadsheet) and a quiz to test your comprehension.   The next topic in this course is content-based filtering, a technique for personalization based on building a profile of personal interests.  Divided over two weeks, you will learn and practice the basic techniques for content-based filtering and then explore a variety of advanced interfaces and content-based computational techniques being used in recommender systems.   The assessments for content-based filtering include an assignment where you compute three types of profile and prediction using a spreadsheet and a quiz on the topics covered.  The assignment is in three parts -- a written assignment, a video intro, and a \"quiz\" where you provide answers from your work to be automatically graded. We close this course with a set of mathematical notation that will be helpful as we move forward into a wider range of recommender systems (in later courses in this specialization).  ", "Computer Vision is one of the most exciting fields in Machine Learning and AI. It has applications in many industries such as self-driving cars, robotics, augmented reality, face detection in law enforcement agencies. In this beginner-friendly course you will understand about computer vision, and will learn about its various applications across many industries.\n\nAs part of this course you will utilize Python, Watson AI, and OpenCV to process images and interact with image classification models. You will also build, train, and test your own custom image classifiers. \n\nThis is a hands-on course and involves several labs and exercises. All the labs will be performed on the Cloud and you will be provided access to a Cloud environment completely free of charge. At the end of the course, you will create your own computer vision web app and deploy it to the Cloud.\n\nThis course does not require any prior Machine Learning or Computer Vision experience, however some knowledge of Python programming language is necessary. Introduction to Computer Vision Image Classification with IBM Watson  Custom Classifiers with Watson Visual Recognition Image Processing using IBM Watson and Python Face Detection and Image Processing using OpenCV and Python Project: Building a Web-Based Computer Vision App This short module will introduce you to the field of computer vision. We will cover the course expectations and highlight how computer vision is making an impact across various industries. You will also be introduced to the tools used in computer vision. In this module, we start by introducing the topic of image classification. We will then talk about Watson Visual Recognition, an industry-leading service provided by IBM that is used for image classification. Finally, you will create your own instance of Watson Visual Recognition and will use it to upload images and perform image classification. Let's get started! This module delves deeper into IBM Watson's image classifiers. You will learn to create a custom classifier in your Visual Recognition Instance and will train and test your custom classifier to classify dog images into different breeds. This week, you will learn about image processing and face detection. You will then use Python and the Watson Visual Recognition API to perform image classification. This week, you will learn how to use the Haar Cascade classifiers for detecting eyes and faces in images. You will then do a variety of hands-on labs that will teach you how to perform license plate recognition using the Tesseract OCR, colour quantization, image compression, and image processing. While completing these labs, you will also learn how to use the OpenCV package in Python  In the final week of this course, you will build a computer vision app that you will deploy on the web. For the project, you will create a custom classifier, train it and connect it to your app. You will then deploy your app to the cloud for making it accessible to anyone around the globe!", "Leveraging the visualizations you created in the previous course, Visual Analytics with Tableau, you will create dashboards that help you identify the story within your data, and you will discover how to use Storypoints to create a powerful story to leave a lasting impression with your audience.\n\nYou will balance the goals of your stakeholders with the needs of your end-users, and be able to structure and organize your story for maximum impact. Throughout the course you will apply more advanced functions within Tableau, such as hierarchies, actions and parameters to guide user interactions.  For your final project, you will create a compelling narrative to be delivered in a meeting, as a static report, or in an interactive display online. Planning and Preproduction: Aligning your Audience, Stakeholders, and Data Key Metrics, Indicators, and Decision Triggers Dashboard and Storytelling with Data Tell the Story of Your Data Welcome to the first module of this course! In the following modules, you will learn and work with concepts, tips, and techniques to help you explore data, identify meaningful findings, and then explain them through the power of data visualization and storytelling. In this module, you will be able to determine the who, what, why, and how of the story and discover the importance of planning before you start. You will be able to interview your stakeholders and assess your audience to find the right story in the data. By the end of this module, you will be able to define what a story is and build a basic framework for presenting your story. Let's get started! Welcome to Module 2. In this module, you will identify the key metrics that will provide the answers to your business question. You will develop an understanding of the types of ways KPIs can be visualized. You will create calculated fields for KPIs to build a figure that will be used to measure progress in the data.  By the end of this module, you should be able to set thresholds and create alerts to trigger a decision.  We will also discuss the topic of quality and constraints of the data. Welcome to Module 3.  In this module, you will go from learning Tableau's six best practices for dashboard design. By the end of this module, you should be able to apply hierarchies, actions, filters, and parameters within Tableau. You will also review five videos associated with the topic and discover how to uncover the story in the data and be able to frame your story.       Welcome to Module 4. Although this course focuses on Tableau, we will look at a wider range of examples and techniques to help you become a better data storyteller. By the end of this module, you should be able to leverage concepts and techniques designed to help you become a more focused and compelling storyteller with data as the foundation. We will discuss ways to avoid unintentionally creating false narratives with good data. You will also learn about what neuroscience research tells us about stories, audience engagement, and decision drivers. You will use structural story elements to help improve the relatability of the story and explore ways design and textual elements can affect the emotional tone of a story.  Lastly, you will learn how to frame and format the data story based on your design checklist.  Let's get started!              ", "In this course, we will explore basic principles behind using data for estimation and for assessing theories. We will analyze both categorical data and quantitative data, starting with one population techniques and expanding to handle comparisons of two populations. We will learn how to construct confidence intervals. We will also use sample data to assess whether or not a theory about the value of a parameter is consistent with the data. A major focus will be on interpreting inferential results appropriately.  \n\nAt the end of each week, learners will apply what they\u00e2\u0080\u0099ve learned using Python within the course environment. During these lab-based sessions, learners will work through tutorials focusing on specific case studies to help solidify the week\u00e2\u0080\u0099s statistical concepts, which will include further deep dives into Python libraries including Statsmodels, Pandas, and Seaborn. This course utilizes the Jupyter Notebook environment within Coursera. WEEK 1 - OVERVIEW & INFERENCE PROCEDURES WEEK 2 - CONFIDENCE INTERVALS WEEK 3 - HYPOTHESIS TESTING WEEK 4 - LEARNER APPLICATION In this first week, we\u00e2\u0080\u0099ll review the course syllabus and discover the various concepts and objectives to be mastered in weeks to come. You\u00e2\u0080\u0099ll be introduced to inference methods and some of the research questions we\u00e2\u0080\u0099ll discuss in the course, as well as an overall framework for making decisions using data, considerations for how you make those decisions, and evaluating errors that you may have made.\nOn the Python side, we\u00e2\u0080\u0099ll review some high level concepts from the first course in this series, Python\u00e2\u0080\u0099s statistics landscape, and walk through intermediate level Python concepts. All of the course information on grading, prerequisites, and expectations are on the course syllabus and you can find more information on our Course Resources page. In this second week, we will learn about estimating population parameters via confidence intervals. You will be introduced to five different types of population parameters, assumptions needed to calculate a confidence interval for each of these five parameters, and how to calculate confidence intervals. Quizzes  will appear throughout the week to test your understanding. In addition, you\u00e2\u0080\u0099ll learn how to create confidence intervals in Python. In week three, we\u00e2\u0080\u0099ll learn how to test various hypotheses - using the five different analysis methods covered in the previous week. We\u00e2\u0080\u0099ll discuss the importance of various factors and assumptions with hypothesis testing and learn to interpret our results. We will also review how to distinguish which procedure is appropriate for the research question at hand.  Quizzes and a peer assessment will appear throughout the week to test your understanding. In the final week of this course, we will walk through several examples and case studies that illustrate applications of the inferential procedures discussed in prior weeks. Learners will see examples of well-formulated research questions related to the study designs and data sets that we have discussed thus far, and via both confidence interval estimation and formal hypothesis testing, we will formulate inferential responses to those questions.", "This course provides a rigorous introduction to the R programming language, with a  particular focus on using R for software development in a data science setting. Whether you are part of a data science team or working individually within a community of developers, this course will give you the knowledge of R needed to make useful contributions in those settings. As the first course in the Specialization, the course provides the essential foundation of R needed for the following courses. We cover basic R concepts and language fundamentals, key concepts like tidy data and related \"tidyverse\" tools, processing and manipulation of complex and large datasets, handling textual data, and basic data science tasks. Upon completing this course, learners will have fluency at the R console and will be able to create tidy datasets from a wide range of possible data sources. Basic R Language Basic R Language: Lesson Choices Data Manipulation Data Manipulation: Lesson Choices Text Processing, Regular Expression, & Physical Memory Text Processing, Regular Expression, & Physical Memory: Lesson Choices Large Datasets In this module, you'll learn the basics of R, including syntax, some tidy data principles and processes, and how to read data into R.    During this module, you'll learn to summarize, filter, merge, and otherwise manipulate data in R, including working through the challenges of dates and times.   During this module, you'll learn to use R tools and packages to deal with text and regular expressions. You'll also learn how to manage and get the most from your computer's physical memory when working in R.  Choice 1: Get credit while using swirl | Choice 2: Get credit by providing a code from swirl In this final module, you'll learn how to overcome the challenges of working with large datasets both in memory and out as well as how to diagnose problems and find help.", "In this third course of the specialization, we\u00e2\u0080\u0099ll drill deeper into the tools Tableau offers in the areas of charting, dates, table calculations and mapping. We\u00e2\u0080\u0099ll explore the best choices for charts, based on the type of data you are using. We\u00e2\u0080\u0099ll look at specific types of charts including scatter plots, Gantt charts, histograms, bullet charts and several others, and we\u00e2\u0080\u0099ll address charting guidelines. We\u00e2\u0080\u0099ll define discrete and continuous dates, and examine when to use each one to explain your data.  You\u00e2\u0080\u0099ll learn how to create custom and quick table calculations and how to create parameters. We\u00e2\u0080\u0099ll also introduce mapping and explore how Tableau can use different types of geographic data, how to connect to multiple data sources and how to create custom maps. Getting Started and Charting Dates Table Calculations Mapping In this module, you will explore the topic of charting in Tableau. By now you should already be well versed in how to change colors, shapes, and sizes of charts, so we are going to practice and demonstrate that skill more. You will be able to explain what the Tableau Tooltip does and when to use it. You will be able to discuss the various guidelines for choosing the right chart for your data. You will also create a chart using Tableau. This module highlights the important topic of dates within Tableau. You will be able to differentiate between discrete and continuous dates and when to use each. You will be able to use date hierarchies and use the date field to better customize your charts. You will be able to convert between discrete and continuous dates and know when and why you want to switch from one to the other. You will create dates using calculated fields. In this module, you will focus on table calculations. You will be able to create new calculated fields to allow you to compare fields, apply aggregations, and more. You will be able use quick table calculations and create new calculated fields. You will be able to customize them and apply filters and parameters to your table calculations.  In this final module, we will go more in depths about maps within Tableau. You will be able to connect to a different data sources and customize your maps by changing colors, shapes, and sizes. You will be able to custom geocode a map and create Tableau maps with geographic data that is not recognized by Tableau. You will also be able to create dual layer maps and showcase how to overlay maps on top of one another.", "A data product is the production output from a statistical analysis. Data products automate complex analysis tasks or use technology to expand the utility of a data informed model, algorithm or inference. This course covers the basics of creating data products using Shiny, R packages, and interactive graphics. The course will focus on the statistical fundamentals of creating a data product that can be used to tell a story about data to a mass audience. Course Overview Shiny, GoogleVis, and Plotly R Markdown and Leaflet R Packages Swirl and Course Project  In this overview module, we'll go over some information and resources to help you get started and succeed in the course.  Now we can turn to the first substantive lessons. In this module, you'll learn how to develop basic applications and interactive graphics in shiny, compose interactive HTML graphics with GoogleVis, and prepare data visualizations with Plotly. During this module, we'll learn how to create R Markdown files and embed R code in an Rmd. We'll also explore Leaflet and use it to create interactive annotated maps. In this module, we'll dive into the world of creating R packages and practice developing an R Markdown presentation that includes a data visualization built using Plotly. Week 4 is all about the Course Project, producing a Shiny Application and reproducible pitch.", "Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. Text data are unique in that they are usually generated directly by humans rather than a computer system or sensors, and are thus especially valuable for discovering knowledge about people\u00e2\u0080\u0099s opinions and preferences, in addition to many other kinds of knowledge that we encode in text. \n\nThis course will cover search engine technologies, which play an important role in any data mining applications involving text data for two reasons. First, while the raw data may be large for any particular problem, it is often a relatively small subset of the data that are relevant, and a search engine is an essential tool for quickly discovering a small subset of relevant text data in a large text collection. Second, search engines are needed to help analysts interpret any patterns discovered in the data by allowing them to examine the relevant original text data to make sense of any discovered pattern. You will learn the basic concepts, principles, and the major techniques in text retrieval, which is the underlying science of search engines. Orientation Week 1 Week 2 Week 3 Week 4 Week 5 Week 6 You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. During this week's lessons, you will learn of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model.  In this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query.  In this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling. In this week's lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model.  In this week's lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines work, including web crawling, web indexing, and how links between web pages can be leveraged to score web pages.  In this week's lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called filtering systems), including content-based recommendation/filtering and collaborative filtering. You will also have a chance to review the entire course.", "This course will empower you with the skills to scale data science and machine learning (ML) tasks on Big Data sets using Apache Spark. Most real world machine learning work involves very large data sets that go beyond the CPU, memory and storage limitations of a single computer. \n\nApache Spark is an open source framework that leverages cluster computing and distributed storage to process extremely large data sets in an efficient and cost effective manner. Therefore an applied knowledge of working with Apache Spark is a great asset and potential differentiator for a Machine Learning engineer.\n\nAfter completing this course, you will be able to:\n- gain a practical understanding of Apache Spark, and apply it to solve machine learning problems involving both small and big data\n- understand how parallel code is written, capable of running on thousands of CPUs. \n- make use of large scale compute clusters to apply machine learning algorithms on Petabytes of data using Apache SparkML Pipelines. \n- eliminate out-of-memory errors generated by traditional machine learning frameworks when data doesn\u00e2\u0080\u0099t fit in a computer's main memory\n- test thousands of different ML models in parallel to find the best performing one \u00e2\u0080\u0093 a technique used by many successful Kagglers\n- (Optional) run SQL statements on very large data sets using Apache SparkSQL and the Apache Spark DataFrame API.\n\nEnrol now to learn the machine learning techniques for working with Big Data that have been successfully applied by companies like Alibaba, Apple, Amazon, Baidu, eBay, IBM, NASA, Samsung, SAP, TripAdvisor, Yahoo!, Zalando and many others.\n\nNOTE: You will practice running machine learning tasks hands-on on an Apache Spark cluster provided by IBM at no charge during the course which you can continue to use afterwards.\n\nPrerequisites:\n- basic python programming\n- basic machine learning (optional introduction videos are provided in this course as well)\n- basic SQL skills for optional content\n\nThe following courses are recommended before taking this class (unless you already have the skills)\nhttps://www.coursera.org/learn/python-for-applied-data-science or similar\nhttps://www.coursera.org/learn/machine-learning-with-python or similar\nhttps://www.coursera.org/learn/sql-data-science for optional lectures Week 1: Introduction Week 2: Scaling Math for Statistics on Apache Spark Week 3: Introduction to Apache SparkML Week 4: Supervised and Unsupervised learning with SparkML This is an introduction to Apache Spark. You'll learn how Apache Spark internally works and how to use it for data processing. RDD, the low level API is introduced in conjunction with parallel programming / functional programming. Then, different types of data storage solutions are contrasted. Finally, Apache Spark SQL and the optimizer Tungsten and Catalyst are explained.  Applying basic statistical calculations using the Apache Spark RDD API in order to experience how parallelization in Apache Spark works Understand the concept of machine learning pipelines in order to understand how Apache SparkML works programmatically Apply Supervised and Unsupervised Machine Learning tasks using SparkML", "This is the second course in the Data Warehousing for Business Intelligence specialization. Ideally, the courses should be taken in sequence.\n\nIn this course, you will learn exciting concepts and skills for designing data warehouses and creating data integration workflows. \nThese are fundamental skills for data warehouse developers and administrators. You will have hands-on experience for data warehouse design and use open source products for manipulating pivot tables and creating data integration workflows.You will also gain conceptual background about maturity models, architectures, multidimensional models, and management practices, providing an organizational perspective about data warehouse development. If you are currently a business or information technology professional and want to become a data warehouse designer or administrator, this course will give you the knowledge and skills to do that. By the end of the course, you will have the design experience, software background, and organizational context that prepares you to succeed with data warehouse development projects.  \n\nIn this course, you will create data warehouse designs and data integration workflows that satisfy the business intelligence needs of organizations. When you\u00e2\u0080\u0099re done with this course, you\u00e2\u0080\u0099ll be able to:\n   * Evaluate an organization for data warehouse maturity and business architecture alignment;\n   * Create a data warehouse design and reflect on alternative design methodologies and design goals;\n   * Create data integration workflows using prominent open source software;\n   * Reflect on the role of change data, refresh constraints, refresh frequency trade-offs, and data quality goals in data integration process design; and\n   * Perform operations on pivot tables to satisfy typical business analysis requests using prominent open source software Data Warehouse Concepts and Architectures Multidimensional Data Representation and Manipulation Multidimensional Data Representation and Manipulation: Lesson Choices Data Warehouse Design Practices and Methodologies Data Integration Concepts, Processes,\u000band Techniques  Architectures, Features, and \u000bDetails of Data Integration Tools Module 1 introduces the course and covers concepts that provide a context for the remainder of this course. In the first two lessons, you\u00e2\u0080\u0099ll understand the objectives for the course and know what topics and assignments to expect. In the remaining lessons, you will learn about historical reasons for development of data warehouse technology, learning effects, business architectures, maturity models, project management issues, market trends, and employment opportunities. This informational module will ensure that you have the background for success in later modules that emphasize details and hands-on skills.You should also read about the software requirements in the lesson at the end of module 1. I recommend that you try to install the software this week before assignments begin in week 2. Now that you have the informational context for data warehouse development, you\u00e2\u0080\u0099ll start using data warehouse tools! In module 2, you will learn about the multidimensional representation of a data warehouse used by business analysts. You\u00e2\u0080\u0099ll apply what you\u00e2\u0080\u0099ve learned in practice and graded problems using WebPivotTable or Pivot4J, open source tools for manipulating pivot tables. At the end of this module, you will have solid background to communicate and assist business analysts who use a multidimensional representation of a data warehouse. After completing this module, you should proceed to module 3 to complete an assignment and quiz with either WebPivotTable or Pivot4J. Because Pivot4J can be difficult to install, I recommend completing the assignment and quiz using WebPivotTable. Choice 1 and 2: If completing the WebPivotTable assignment (choice 1), you should also complete the WebPivotTable quiz (choice 2). | Choice 3 and 4: If completing the Pivot4J assignment (choice 3), you should also complete the Pivot4J quiz (choice 4). Due to potential difficulty with installing Pivot4J, I recommend that you complete the WebPivotTable assignment and quiz. This module emphasizes data warehouse design skills. Now that you understand the multidimensional representation used by business analysts, you are ready to learn about data warehouse design using a relational database. In practice, the multidimensional representation used by business analysts must be derived from a data warehouse design using a relational DBMS.You will learn about design patterns, summarizability problems, and design methodologies. You will apply these concepts to mini case studies about data warehouse design. At the end of the module, you will have created data warehouse designs based on data sources and business needs of hypothetical organizations. Module 4 extends your background about data warehouse development. After learning about schema design concepts and practices, you are ready to learn about data integration processing to populate and refresh a data warehouse. The informational background in module 4 covers concepts about data sources, data integration processes, and techniques for pattern matching and inexact matching of text. Module 4 provides a context for the software skills that you will learn in module 5.  Module 5 extends your background about data integration from module 4. Module 5 covers architectures, features, and details about data integration tools to complement the conceptual background in module 4. You will learn about the features of two open source data integration tools, Talend Open Studio and Pentaho Data Integration. You will use Pentaho Data Integration in guided tutorial in preparation for a graded assignment involving Pentaho Data Integration.", "This course covers the design, acquisition, and analysis of Functional Magnetic Resonance Imaging (fMRI) data.\n\nA book related to the class can be found here: https://leanpub.com/principlesoffmri Week 1 Week 2 Week 3 Week 4 This week we will introduce fMRI, and talk about data acquisition and reconstruction.  This week we will discuss the fMRI signal, experimental design and pre-processing.  This week we will discuss the General Linear Model (GLM). The description goes here", "By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nWelcome to the Coursera course, Industrial Internet of Things (IoT) on Google Cloud Platform (GCP) brought to you by the Google Cloud team. I\u00e2\u0080\u0099m Catherine Gamboa and I\u00e2\u0080\u0099m going to be your guide.\n\nThis course covers the entire Industrial IoT network architecture from sensors and devices to analysis. The course discusses sensors and devices but the focus is on the cloud side.  You'll learn about the importance of scaling, device communication, and processing streaming data. The course uses simulated devices in the labs to allow you to concentrate on learning the cloud side of IIoT.  The course is a little different than most Coursera courses because there is very little video. Most of the learning is done with short readings, quizzes, and labs.  \n\nThis course takes about two weeks to complete, 11-12 hours of work with 6 of those hours spent in labs.  By the end of this course, you\u00e2\u0080\u0099ll be able to: create a streaming data pipeline, to create registries with Cloud IoT Core, topics and subscriptions with Cloud Pub/Sub, store data on Google Cloud Storage, query the data in BigQuery, and gain data insights with Dataprep.  You'll learn and practice these skills in 7 labs.  Then you'll have an opportunity to test yourself in an optional capstone lab using simulated devices or Cloud IoT Core Inspector. Welcome to Industrial IoT on GCP Foundations of GCP Architecture Sensors, Devices, and Communication Google Cloud IoT Platform Creating IoT Data Pipelines Analyzing Data with BigQuery Analyzing IoT Dataprep and Data Studio Optional Capstone Project        ", "This course empowers learners to develop image processing programs and leverage MATLAB functionalities to implement sophisticated image applications. It provides a rich explanation of the fundamentals of computer vision\u00e2\u0080\u0099s lower- and mid-level tasks by examining several principle approaches and their historical roots. By the end of the course, learners are prepared to analyze images in frequency domain. Topics include image filters, image features and matching, and image segmentation.   \n\nThis course is ideal for anyone curious about or interested in exploring the concepts of computer vision. It is also useful for those who desire a refresher course in mathematical concepts of computer vision. Learners should have basic programming skills and experience (understanding of for loops, if/else statements), specifically in MATLAB (Mathworks provides the basics here: https://www.mathworks.com/learn/tutorials/matlab-onramp.html).  Learners should also be familiar with the following: basic linear algebra (matrix vector operations and notation), 3D co-ordinate systems and transformations, basic calculus (derivatives and integration) and basic probability (random variables).   \n  \nMaterial includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing computer vision programs through online labs using MATLAB* and supporting toolboxes.\n\nThis is the second course in the Computer Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a video overview at https://youtu.be/OfxVUSCPXd0. \n\n * A free license to install MATLAB for the duration of the course is available from MathWorks. Image Processing Fundamentals Image Filters Image Features & Matching Image Segmentation In this module, we will discuss the basics and applications of digital image processing, including intensity transformations and color image processing. In this module, we will discuss image filtering as well as some advanced image processing methods. In this module, we will discuss image features, feature matching, texture matching, and how to create a panorama. This module describes what image segmentation is and provides information on the different techniques used to perform image segmentation.", "This introductory course is for SAS software users who perform statistical analyses using SAS/STAT software. The focus is on t tests, ANOVA, and linear regression, and includes a brief introduction to logistic regression. Course Overview and Data Setup Introduction and Review of Concepts ANOVA and Regression More Complex Linear Models Model Building and Effect Selection Model Post-Fitting for Inference Model Building for Scoring and Prediction Categorical Data Analysis In this module you learn about the course and the data you analyze in this course. Then you set up the data you need to do the practices in the course.  In this module you learn about the models required to analyze different types of data and the difference between explanatory vs predictive modeling. Then you review fundamental statistical concepts, such as the sampling distribution of a mean, hypothesis testing, p-values, and confidence intervals. After reviewing these concepts, you apply one-sample and two-sample t tests to data to confirm or reject preconceived hypotheses. In this module you learn to use graphical tools that can help determine which predictors are likely or unlikely to be useful. Then you learn to augment these graphical explorations with correlation analyses that describe linear relationships between potential predictors and our response variable. After you determine potential predictors, tools like ANOVA and regression help you assess the quality of the relationship between the response and predictors. In this module you expand the one-way ANOVA model to a two-factor analysis of variance and then extend simple linear regression to multiple regression with two predictors. After you understand the concepts of two-way ANOVA and multiple linear regression with two predictors, you'll have the skills to fit and interpret models with many variables. In this module you explore several tools for model selection. These tools help limit the number of candidate models so that you can choose an appropriate model that's based on your expertise and research priorities. In this module you learn to verify the assumptions of the model and diagnose problems that you encounter in linear regression. You learn to examine residuals, identify outliers that are numerically distant from the bulk of the data, and identify influential observations that unduly affect the regression model. Finally, you learn to diagnose collinearity to avoid inflated standard errors and parameter instability in the model. In this module you learn how to transition from inferential statistics to predictive modeling. Instead of using p-values, you learn about assessing models using honest assessment. After you choose the best performing model, you learn about ways to deploy the model to predict new data. In this module you look for associations between predictors and a binary response using hypothesis tests. Then you build a logistic regression model and learn about how to characterize the relationship between the response and predictors. Finally, you learn how to use logistic regression to build a model, or classifier, to predict unknown cases.", "This course will cover the major techniques for mining and analyzing text data to discover interesting patterns, extract useful knowledge, and support decision making, with an emphasis on statistical approaches that can be generally applied to arbitrary text data in any natural language with no or minimum human effort. \n\nDetailed analysis of text data requires understanding of natural language text, which is known to be a difficult task for computers. However, a number of statistical approaches have been shown to work well for the \"shallow\" but robust analysis of text data for pattern finding and knowledge discovery. You will learn the basic concepts, principles, and major algorithms in text mining and their potential applications. Orientation Week 1 Week 2 Week 3 Week 4 Week 5 Week 6 You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. During this module, you will learn the overall course design, an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations).    During this module, you will learn more about word association mining with a particular focus on mining the other basic form of word association (i.e., syntagmatic relations), and start learning topic analysis with a focus on techniques for mining one topic from text.  During this module, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.  During this module, you will learn text clustering, including the basic concepts, main clustering techniques, including probabilistic approaches and similarity-based approaches, and how to evaluate text clustering. You will also start learning text categorization, which is related to text clustering, but with pre-defined categories that can be viewed as pre-defining clusters.    During this module, you will continue learning about various methods for text categorization, including multiple methods classified under discriminative classifiers, and you will also learn sentiment analysis and opinion mining, including a detailed introduction to a particular technique for sentiment classification (i.e., ordinal regression).  During this module, you will continue learning about sentiment analysis and opinion mining with a focus on Latent Aspect Rating Analysis (LARA), and you will learn about techniques for joint mining of text and non-text data, including contextual text mining techniques for analyzing topics in text in association with various context information such as time, location, authors, and sources of data. You will also see a summary of the entire course.", "No doubt working with huge data volumes is hard, but to move a mountain, you have to deal with a lot of small stones. But why strain yourself? Using  Mapreduce and Spark you tackle the issue partially, thus leaving some space for high-level tools. Stop  struggling to make your big data workflow productive and efficient,  make use of the tools we are offering you.\n \nThis course will teach you how to:\n- Warehouse your data efficiently using Hive, Spark SQL and Spark DataFframes. \n- Work with large graphs, such as social graphs or networks. \n- Optimize your Spark applications for maximum performance.\n\nPrecisely, you will master your knowledge in:\n- Writing and executing Hive & Spark SQL queries;\n- Reasoning how the queries are translated into actual execution primitives (be it MapReduce jobs or Spark transformations);\n- Organizing your data in Hive to optimize disk space usage and execution times;\n- Constructing Spark DataFrames and using them to write ad-hoc analytical jobs easily;\n- Processing large graphs with Spark GraphFrames;\n- Debugging, profiling and optimizing Spark application performance.\n \nStill in doubt? Check this out. Become a data ninja by taking this course!\n\nSpecial thanks to:\n- Prof. Mikhail Roytberg, APT dept., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the road.\n- Oleg Sukhoroslov (PhD, Senior Researcher at IITP RAS), who has been teaching  MapReduce, Hadoop and friends since 2008. Now he is leading the infrastructure team.\n- Oleg Ivchenko (PhD student APT dept., MIPT), Pavel Akhtyamov (MSc. student at APT dept., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl State University), superbrains who have developed and now maintain the infrastructure used for practical assignments in this course.\n- Asya Roitberg, Eugene Baulin, Marina Sudarikova. These people never sleep to babysit this course day and night, to make your learning experience productive, smooth and exciting. Welcome to the Second Course: Big Data Analysis Big Data SQL: Hive Big Data SQL: Hive (practice week) Spark SQL and Spark Dataframe Graph Analysis from Big Data Perspective PageRank and Recent Advances Spark Internals and Optimization       ", "Useful quantitative models help you to make informed decisions both in situations in which the factors affecting your decision are clear, as well as in situations in which some important factors are not clear at all. In this course, you can learn how to create quantitative models to reflect complex realities, and how to include in your model elements of risk and uncertainty. You\u00e2\u0080\u0099ll also learn the methods for creating predictive models for identifying optimal choices; and how those choices change in response to changes in the model\u00e2\u0080\u0099s assumptions. You\u00e2\u0080\u0099ll also learn the basics of the measurement and management of risk. By the end of this course, you\u00e2\u0080\u0099ll be able to build your own models with your own data, so that you can begin making data-informed decisions. You\u00e2\u0080\u0099ll also be prepared for the next course in the Specialization. Week 1: Modeling Decisions in Low Uncertainty Settings Week 2: Risk and Reward: Modeling High Uncertainty Settings Week 3: Choosing Distributions that Fit Your Data Week 4: Balancing Risk and Reward Using Simulation This module is designed to teach you how to analyze settings with low levels of uncertainty, and how to identify the best decisions in these settings. You'll explore the optimization toolkit, learn how to build an algebraic model using an advertising example, convert the algebraic model to a spreadsheet model, work with Solver to discover the best possible decision, and examine an example that introduces a simple representation of risk to the model. By the end of this module, you'll be able to build an optimization model, use Solver to uncover the optimal decision based on your data, and begin to adjust your model to account for simple elements of risk. These skills will give you the power to deal with large models as long as the actual uncertainty in the input values is not too high. What if uncertainty is the key feature of the setting you are trying to model? In this module, you'll learn how to create models for situations with a large number of variables. You'll examine high uncertainty settings, probability distributions, and risk, common scenarios for multiple random variables, how to incorporate risk reduction, how to calculate and interpret correlation values, and how to use scenarios for optimization, including sensitivity analysis and the efficient frontier. By the end of this module, you'll be able to identify and use common models of future uncertainty to build scenarios that help you optimize your business decisions when you have multiple variables and a higher degree of risk.  When making business decisions, we often look to the past to make predictions for the future. In this module, you'll examine commonly used distributions of random variables to model the future and make predictions. You'll learn how to create meaningful data visualizations in Excel, how to choose the the right distribution for your data, explore the differences between discrete distributions and continuous distributions, and test your choice of model and your hypothesis for goodness of fit. By the end of this module, you'll be able to represent your data using graphs, choose the best distribution model for your data, and test your model and your hypothesis to see if they are the best fit for your data. This module is designed to help you use simulations to enabling compare different alternatives when continuous distributions are used to describe uncertainty. Through an in-depth examination of the simulation toolkit, you'll learn how to make decisions in high uncertainty settings where random inputs are described by continuous probability distributions. You'll also learn how to run a simulation model, analyze simulation output, and compare alternative decisions to decide on the most optimal solution.  By the end of this module, you'll be able to make decisions and manage risk using simulation, and more broadly, to make successful business decisions in an increasing complex and rapidly evolving business world.", "This class provides an introduction to the Python programming language and the iPython notebook. This is the third course in the Genomic Big Data Science Specialization from Johns Hopkins University. Week One Week Two Week Three Week Four This week we will have an overview of Python and take the first steps towards programming. In this module, we'll be taking a look at Data Structures and Ifs and Loops. In this module, we have a long three-part lecture on Functions as well as a 10-minute look at Modules and Packages. In this module, we have another long three-part lecture, this time about Communicating with the Outside, as well as a final lecture about Biopython.", "This one-week course describes the process of analyzing data and how to manage that process. We describe the iterative nature of data analysis and the role of stating a sharp question, exploratory data analysis, inference, formal statistical modeling, interpretation, and communication. In addition, we will describe how to direct analytic activities within a team and to drive the data analysis process towards coherent and useful results. \n\nThis is a focused course designed to rapidly get you up to speed on the process of data analysis and how it can be managed. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know how to\u00e2\u0080\u00a6.\n\n1. Describe the basic data analysis iteration\n2. Identify different types of questions and translate them to specific datasets\n3. Describe different types of data pulls\n4. Explore datasets to determine if data are appropriate for a given question\n5. Direct model building efforts in common data analyses\n6. Interpret the results from common data analyses\n7. Integrate statistical findings to form coherent data analysis presentations\n\nCommitment: 1 week of study, 4-6 hours\n\nCourse cover image by fdecomite. Creative Commons BY https://flic.kr/p/4HjmvD Managing Data Analysis Welcome to Managing Data Analysis! This course is one module, intended to be taken in one week. The course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials that expand on the lecture. I'm excited to have you in the class and look forward to your contributions to the learning community. If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center. Good luck as you get started, and I hope you enjoy the course!", "Welcome to the art and science of machine learning. In this data science course you will learn the essential skills of ML intuition, good judgment and experimentation to finely tune and optimize your ML models for the best performance.  \n\nIn this course you will learn the many knobs and levers involved in training a model. You will first manually adjust them to see their effects on model performance. Once familiar with the knobs and levers, otherwise known as hyperparameters, you will learn how to tune them in an automatic way using Cloud Machine Learning Engine on Google Cloud Platform.\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction The Art of ML Hyperparameter Tuning A pinch of science The science of neural networks Embeddings Custom Estimator Summary Course overview highlighting the key objectives and modules. First, you will learn about aspects of Machine Learning that require some intuition, good judgment and experimentation. We call it the Art of ML. You will learn the many knobs and levers involved in training a model. You will manually adjust them to see their effects on model performance. \n In this course you will learn about The Art of Machine Learning. We will review aspects of machine learning that require intuition, judgment and experimentation to find the right balance and what\u00e2\u0080\u0099s good enough (spoiler alert: it's never perfect!). \n In this module you will learn how to differentiate between parameters and hyperparameters. Then we\u00e2\u0080\u0099ll discuss traditional grid search approach and learn how to think beyond it with smarter algorithms. Finally you\u00e2\u0080\u0099ll learn how Cloud ML engine makes it so convenient to automate hyperparameter tuning. \n In this module, we will start to introduce the science along with the art of machine learning. We\u00e2\u0080\u0099re first going to talk about how to perform regularization for sparsity so that we can have simpler, more concise models. Then we\u00e2\u0080\u0099re going to talk about logistic regression and learning how to determine performance.\n In this module we will now be diving deep into the science, specifically with neural networks. In this module, you will learn how to use embeddings to manage sparse data, to make machine learning models that use sparse data consume less memory and train faster. Embeddings are also a way to do dimensionality reduction, and in that way, make models simpler and more generalizable.\n In this module we will go beyond using canned estimators by writing a custom estimator. By writing a custom estimator, you will be able to gain greater control over the model function itself. Review the key concepts we covered in the Art and Science of ML course. ", "Science is undergoing a data explosion, and astronomy is leading the way. Modern telescopes produce terabytes of data per observation, and the simulations required to model our observable Universe push supercomputers to their limits. To analyse this data scientists need to be able to think computationally to solve problems. In this course you will investigate the challenges of working with large datasets: how to implement algorithms that work; how to use databases to manage your data; and how to learn from your data with machine learning tools. The focus is on practical skills - all the activities will be done in Python 3, a modern programming language used throughout astronomy.\n\nRegardless of whether you\u00e2\u0080\u0099re already a scientist, studying to become one, or just interested in how modern astronomy works \u00e2\u0080\u0098under the bonnet\u00e2\u0080\u0099, this course will help you explore astronomy: from planets, to pulsars to black holes.\n\nCourse outline:\nWeek 1: Thinking about data\n- Principles of computational thinking\n- Discovering pulsars in radio images\n\nWeek 2: Big data makes things slow\n- How to work out the time complexity of algorithms\n- Exploring the black holes at the centres of massive galaxies\n\nWeek 3: Querying data using SQL\n- How to use databases to analyse your data\n- Investigating exoplanets in other solar systems\n\nWeek 4: Managing your data\n- How to set up databases to manage your data\n- Exploring the lifecycle of stars in our Galaxy\n\nWeek 5: Learning from data: regression\n- Using machine learning tools to investigate your data\n- Calculating the redshifts of distant galaxies\n\nWeek 6: Learning from data: classification\n- Using machine learning tools to classify your data\n- Investigating different types of galaxies\n\nEach week will also have an interview with a data-driven astronomy expert.\n\nNote that some knowledge of Python is assumed, including variables, control structures, data structures, functions, and working with files. Thinking about data Big data makes things slow Querying your data Managing your data Learning from data: regression Learning from data: classification This module introduces the idea of computational thinking, and how big data can make simple problems quite challenging to solve. We use the example of calculating the median and mean stack of a set of radio astronomy images to illustrate some of the issues you encounter when working with large datasets.  In this module we explore the idea of scaling your code. Some algorithms scale well as your dataset increases, but others become impossibly slow. We look at some of the reason for this, and use the example of cross-matching astronomical catalogues to demonstrate what kind of improvements you can make.  Most large astronomy projects use databases to manage their data. In this module we introduce SQL - the language most commonly used to query databases. We use SQL to query the NASA Exoplanet database and investigate the habitability of planets in other solar systems. This module introduces the basic principles of setting up databases. We look at how to set up new tables, and then how to combine Python and SQL to get the best out of both approaches. We use these tools to explore the life of stars in a stellar cluster.\n This module introduces the idea of machine learning. We look at standard methodology for running machine learning experiments, and then apply this to calculating redshifts of distant galaxies using decision trees for regression.  In this final module we explore the limitations of decision tree classifiers. We then look at ensemble classifiers, using the random forest algorithm to classify images of galaxies into different types.", "At the end of the course, you will be able to:\n\n*Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking \u00e2\u0080\u009cAbout This Mac.\u00e2\u0080\u009d Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Welcome to Big Data Integration and Processing Retrieving Big Data (Part 1) Retrieving Big Data (Part 2) Big Data Integration Processing Big Data Big Data Analytics using Spark Learn By Doing: Putting MongoDB and Spark to Work Welcome to the third course in the Big Data Specialization. This week you will be introduced to basic concepts in big data integration and processing. You will be guided through installing the Cloudera VM, downloading the data sets to be used for this course, and learning how to run the Jupyter server.  This module covers the various aspects of data retrieval and relational querying. You will also be introduced to the Postgres database.  This module covers the various aspects of data retrieval for NoSQL data, as well as data aggregation and working with data frames. You will be introduced to MongoDB and Aerospike, and you will learn how to use Pandas to retrieve data from them. In this module you will be introduced to data integration tools including Splunk and Datameer, and you will gain some practical insight into how information integration processes are carried out.  This module introduces Learners to big data pipelines and workflows as well as processing and analysis of big data using Apache Spark.  In this module, you will go deeper into big data processing by learning the inner workings of the Spark Core. You will be introduced to two key tools in the Spark toolkit: Spark MLlib and GraphX.  In this module you will get some practical hands-on experience applying what you learned about Spark and MongoDB to analyze Twitter data. ", "\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u00ad\u00b4\u00e5\u008f\u00b2\u00e3\u0082\u0092\u00e7\u009a\u00ae\u00e5\u0088\u0087\u00e3\u0082\u008a\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0083\u008b\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u00ab \u00e3\u0083\u008d\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u00af\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0081\u008c\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0081\u00a7\u00e3\u0081\u0086\u00e3\u0081\u00be\u00e3\u0081\u008f\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e7\u0090\u0086\u00e7\u0094\u00b1\u00e3\u0082\u0092\u00e3\u0081\u0094\u00e7\u00b4\u00b9\u00e4\u00bb\u008b\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ac\u00a1\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e6\u0095\u0099\u00e5\u00b8\u00ab\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0082\u0092\u00e8\u00a8\u00ad\u00e5\u00ae\u009a\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e5\u008b\u00be\u00e9\u0085\u008d\u00e9\u0099\u008d\u00e4\u00b8\u008b\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e9\u0081\u00a9\u00e5\u0088\u0087\u00e3\u0081\u00aa\u00e8\u00a7\u00a3\u00e6\u00b1\u00ba\u00e7\u00ad\u0096\u00e3\u0082\u0092\u00e8\u00a6\u008b\u00e3\u0081\u00a4\u00e3\u0081\u0091\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0081\u00ab\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e5\u008c\u0096\u00e3\u0081\u008c\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008b\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0082\u0082\u00e5\u0090\u00ab\u00e3\u0081\u00be\u00e3\u0082\u008c\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e5\u00ae\u009f\u00e9\u00a8\u0093\u00e3\u0081\u00ab\u00e5\u00af\u00be\u00e5\u00bf\u009c\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e7\u00b9\u00b0\u00e3\u0082\u008a\u00e8\u00bf\u0094\u00e3\u0081\u0097\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a7\u00a3\u00e8\u00aa\u00ac\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e7\u009b\u00ae\u00e6\u00a8\u0099:\n\u00e3\u0083\u0087\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u0097 \u00e3\u0083\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u008c\u00e6\u00b3\u00a8\u00e7\u009b\u00ae\u00e3\u0082\u0092\u00e9\u009b\u0086\u00e3\u0082\u0081\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e7\u0090\u0086\u00e7\u0094\u00b1\u00e3\u0082\u0092\u00e7\u009f\u00a5\u00e3\u0082\u008b\n\u00e6\u0090\u008d\u00e5\u00a4\u00b1\u00e9\u0096\u00a2\u00e6\u0095\u00b0\u00e3\u0081\u00a8\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e6\u008c\u0087\u00e6\u00a8\u0099\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u008a\u00e3\u0082\u0088\u00e3\u0081\u00b3\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a7\u00e7\u0099\u00ba\u00e7\u0094\u009f\u00e3\u0081\u0097\u00e3\u0081\u008c\u00e3\u0081\u00a1\u00e3\u0081\u00aa\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0082\u0092\u00e8\u00bb\u00bd\u00e6\u00b8\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e5\u0086\u008d\u00e7\u008f\u00be\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e3\u0081\u00aa\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab \u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e7\u0094\u00a8\u00e3\u0080\u0081\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e7\u0094\u00a8\u00e3\u0080\u0081\u00e3\u0083\u0086\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e7\u0094\u00a8\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092 \u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096 \u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e5\u008c\u0096\u00e3\u0081\u00a8\u00e3\u0082\u00b5\u00e3\u0083\u00b3\u00e3\u0083\u0097\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00ef\u00bc\u0088ML\u00ef\u00bc\u0089\u00e3\u0081\u00ae\u00e5\u009f\u00ba\u00e7\u00a4\u008e\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0082\u0092\u00e7\u00bf\u0092\u00e5\u00be\u0097\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e5\u0088\u0086\u00e9\u0087\u008e\u00e3\u0081\u00a7\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e7\u0094\u00a8\u00e8\u00aa\u009e\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0080\u0081Google \u00e3\u0081\u00ae ML \u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u008a\u00e3\u0083\u00bc\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e9\u009d\u00a2\u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0092\u00e3\u0083\u00b3\u00e3\u0083\u0088\u00e3\u0082\u0084\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0080\u0081\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0082\u0084\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0082\u0092\u00e7\u00bf\u0092\u00e5\u00be\u0097\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e7\u008b\u00ac\u00e8\u0087\u00aa\u00e3\u0081\u00ae ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ab\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0082\u0082\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e4\u00b8\u00bb\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e3\u0082\u00bf\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00ef\u00bc\u0088ML\u00ef\u00bc\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e6\u009c\u0080\u00e6\u0096\u00b0\u00e6\u008a\u0080\u00e8\u00a1\u0093\u00e3\u0081\u00ab\u00e8\u0087\u00b3\u00e3\u0082\u008b\u00e3\u0081\u00be\u00e3\u0081\u00a7\u00e3\u0081\u00ae ML \u00e3\u0081\u00ae\u00e6\u00ad\u00b4\u00e5\u008f\u00b2\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e6\u00a4\u009c\u00e8\u00a8\u00bc\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u008d\u00e3\u0081\u00aa\u00e3\u0081\u008c\u00e3\u0082\u0089\u00e3\u0080\u0081ML \u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u008a\u00e3\u0083\u00bc\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e3\u0082\u00ad\u00e3\u0083\u00a3\u00e3\u0083\u00aa\u00e3\u0082\u00a2\u00e3\u0082\u00a2\u00e3\u0083\u0083\u00e3\u0083\u0097\u00e3\u0082\u0092\u00e5\u009b\u00b3\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00ef\u00bc\u0088ML\u00ef\u00bc\u0089\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e5\u00b0\u0091\u00e3\u0081\u0097\u00e9\u0081\u0095\u00e3\u0081\u00a3\u00e3\u0081\u009f\u00e8\u00a7\u0092\u00e5\u00ba\u00a6\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e6\u00a4\u009c\u00e8\u00a8\u008e\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ad\u00a3\u00e8\u00a7\u00a3\u00e7\u008e\u0087\u00e3\u0081\u008c\u00e6\u009c\u0080\u00e3\u0082\u0082\u00e9\u00ab\u0098\u00e3\u0081\u0084\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00ef\u00bc\u0088ML\u00ef\u00bc\u0089\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e9\u0081\u00b8\u00e6\u008a\u009e\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00ae\u00e3\u0081\u008c\u00e9\u0081\u00a9\u00e3\u0081\u0095\u00e3\u0081\u00aa\u00e3\u0081\u0084\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0081\u008c\u00e3\u0080\u0081\u00e3\u0081\u009d\u00e3\u0082\u008c\u00e3\u0081\u00af\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00aa\u00e5\u00a0\u00b4\u00e5\u0090\u0088\u00e3\u0081\u00a7\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0081\u008b\u00e3\u0080\u0082\u00e5\u0089\u008d\u00e5\u009b\u009e\u00e3\u0081\u00ae\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e7\u00a4\u00ba\u00e5\u0094\u0086\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e5\u00af\u00be\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u0090\u008d\u00e5\u00a4\u00b1\u00e6\u008c\u0087\u00e6\u00a8\u0099\u00e3\u0081\u008c 0 \u00e3\u0081\u00a8\u00e3\u0081\u0084\u00e3\u0081\u0086\u00e3\u0081\u00a0\u00e3\u0081\u0091\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e4\u00b8\u0096\u00e7\u0095\u008c\u00e3\u0081\u00ae\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e9\u0081\u00a9\u00e5\u0088\u0087\u00e3\u0081\u00ab\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0081\u00af\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u009b\u00e3\u0082\u0093\u00e3\u0080\u0082 ", "A learner will be able to write an application that leverages multiple Watson AI services (Discovery, Speech to Text, Assistant, and Text to Speech). By the end of the course, they\u00e2\u0080\u0099ll learn best practices of combining Watson services, and how they can build interactive information retrieval systems with Discovery + Assistant. Module 1: Introduction Module 2: Watson Discovery Module 3: Building the Chatbot Module 4: Giving it a Voice Module 5: Deployment Module 6: Project In this module, we'll discuss the course prerequisites, scope, and the technologies that we'll be using. We'll also get you set up for using key Watson services on the IBM Cloud. In this module, you'll learn about Watson Discovery, a great tool to extract insight from large volumes of unstructured data. You'll also learn about how integration between Watson Assistant and Discovery works in principle. In this module, we'll start to create a student advisor chatbot by leveraging Watson Assistant. We'll then use IBM Cloud Functions to integrate it with Watson Discovery. In this module, you'll learn about the various options available to enable interaction with your chatbot via audio rather than textual means. In the labs, you'll work on integrating Watson Assistant with Watson Speech APIs. This module will teach you how to deploy your chatbot to various channels, including Facebook Messenger and Slack. In module 6, you'll put it all together, by using your newfound skills for the creation of a Coursera Student Advisor. This chatbot will leverage at least two Watson services, including Watson Assistant and Watson Discovery.", "In this second MOOC in the Social Marketing Specialization - \"The Importance of Listening\" - you will go deep into the Big Data of social and gain a more complete picture of what can be learned from interactions on social sites. You will be amazed at just how much information can be extracted from a single post, picture, or video. In this MOOC, guest speakers from Social Gist, BroadReader, Lexalytics, Semantria, Radian6, and IBM's Bluemix and Social Media Analytics Tools (SMA) will join Professor Hlavac to take you through the full range of analytics tools and options available to you and how to get the most from them. The best part, most of them will be available to you through the MOOC for free! Those purchasing the MOOC will receive special tools, templates, and videos to enhance your learning experience. In completing this course you will develop a fuller understanding of the data and will be able to increase the effectiveness of your content strategy by making better decisions and spotting crises before they happen! MOOC 2 bonus content in the paid toolkit includes access to Semantria's analytics engine to extract some data on the markets you are developing and have it analyzed. \n\nAs a student in this course, you are being provided the opportunity to access IBM Bluemix\u00c2\u00ae platform-as-a-service trial for up to six months at no-charge with no credit card (up to a $1500 value).\n\nNOTE: By enrolling in this course, given access to IBM's Bluemix technology for one month for free as well as Lexalytics' Semantria tool. For those earning a Course Certificate, you will be given an additional five months of Bluemix and three months of Semantria at no cost with a special key code. By enrolling for a Course Certificate for this MOOC, you are acknowledging that your email will be shared with Lexalytics for the sole and express purpose of generating your individual key code. After the key code has been generated, Lexalytics will delete your email from its records. \n\nAdditional MOOC 2 faculty include: \n* Steve Dodd (SVP Business Development, Effyis - dba BoardReader and Socialgist - Global Social Media Content Access) \n* Seth Redmore (CMO, Lexalytics, Inc.)\n* Chris Gruber (Social Media Analytics Solution Architect, IBM)\n* Russell Beardall (Cloud Architect, IBM) \n* Tom Collinger (Executive Director Spiegel Research Center and Senior Director Distance Learning, Medill Integrated Marketing Communications, Northwestern)\n* Tressie Lieberman (VP Digital Innovation, Taco Bell) Big Data Big Information Big Insights Real-Time in Action In this module, you will learn how big data is collected, standardized, and deployed by organizations into big insights.  In this module, you will learn how to use key social information to drive your social strategy using state of the art analytics systems.   In this module, you will learn how data is transformed into actionable insights for your social marketing programs.  In this module, you will learn how big data and big insights are being used by global businesses and organizations to drive their content strategies and prevent crises from happening. ", "This course allows you to apply the SQL skills taught in \u00e2\u0080\u009cSQL for Data Science\u00e2\u0080\u009d to four increasingly complex and authentic data science inquiry case studies. We'll learn how to convert timestamps of all types to common formats and perform date/time calculations. We'll select and perform the optimal JOIN for a data science inquiry and clean data within an analysis dataset by deduping, running quality checks, backfilling, and handling nulls. We'll learn how to segment and analyze data per segment using windowing functions and use case statements to execute conditional logic to address a data science inquiry. We'll also describe how to convert a query into a scheduled job and how to insert data into a date partition. Finally, given a predictive analysis need, we'll engineer a feature from raw data using the tools and skills we've built over the course. The real-world application of these skills will give you the framework for performing the analysis of an AB test. Data of Unknown Quality Creating Clean Datasets SQL Problem Solving Case Study: AB Testing In this module, you will be able to create trustworthy analysis from a new set of data. You will be able  to coalesce some nulls and identify unreliable data and discover reasons why data might be missing. You will also be able to answer ambiguous questions by defining new metrics. In this module, you will be able to name the main the categories of data types. You will be able to explain how the unfiltered data can be manipulated into a table where you can conduct data analysis. You will be able to discuss why a data warehouse is separate from a production database, and you will be able to use the tools you learned to create your own trustworthy tables.  In this module, you will be able to map out your joins and be able to highlight the level of detail needed for different kinds of questions. You will be able to practice answering data questions, which should help you feel ready to get asked a whole slough of questions, vague questions, ambiguous questions, or even poorly worded questions. Finally, you will develop a strategy for answering all those questions using data. In this module, you will be able to use your SQL skills to set up a basic AB testing system. You will be able to apply hypothesis testing to prove or disprove a hypothesis about how user behavior changed. You will be able to test and interpret the results using a metric or metrics that are tied directly to some business metrics. You will be able to test your SQL skills and give you the base experience you need to learn anything more complicated in terms of AB testing in the future.", "Looking to start a career in Deep Learning? Look no further. This course will introduce you to the field of deep learning and help you answer many questions that people are asking nowadays, like what is deep learning, and how do deep learning models compare to artificial neural networks? You will learn about the different deep learning models and build your first deep learning model using the Keras library.\n\nAfter completing this course, learners will be able to:\n\u00e2\u0080\u00a2\tdescribe what a neural network is, what a deep learning model is, and the difference between them.\n\u00e2\u0080\u00a2\tdemonstrate an understanding of unsupervised deep learning models such as autoencoders and restricted Boltzmann machines.\n\u00e2\u0080\u00a2\tdemonstrate an understanding of supervised deep learning models such as convolutional neural networks and recurrent networks.\n\u00e2\u0080\u00a2\tbuild deep learning models and networks using the Keras library. Introduction to Neural Networks and Deep Learning Artificial Neural Networks Keras and Deep Learning Libraries Deep Learning Models Course Project In this module, you will learn about exciting applications of deep learning and why now is the perfect time to learn deep learning. You will also learn about neural networks and how most of the deep learning algorithms are inspired by the way our brain functions and the neurons process data. Finally, you will learn about how neural networks feed data forward through the network. In this module, you will learn about the gradient descent algorithm and how variables are optimized with respect to a defined function. You will also learn about backpropagation and how neural networks learn and update their weights and biases. Futhermore, you will learn about the vanishing gradient problem. Finally, you will learn about activation functions. In this module, you will learn about the diifferent deep learning libraries namely, Keras, PyTorch, and TensorFlow. You will also learn how to build regression and classification models using the Keras library. In this module, you will learn about the difference between the shallow and deep neural networks. You will also learn about convolutional networks and how to build them using the Keras library. Finally, you will also learn about recurrent neural networks and autoencoders. In this module, you will conclude the course by working on a final assignment where you will use the Keras library to build a regression model and experiment with the depth and the width of the model.", "This course is designed to provide you with an understanding of the role of data and technology in human capital management. Every topic in the course will be covered in the most practical way so that learners get hands-on experience. In the course we use the 4Ts principle: Task, Theory, Technique and Technology so that there is always a connection to organizational performance objectives, an overview of underlying theories and principles, and specific tools which help achieve business objectives.\n\nYou will learn \n\u00e2\u0097\u008f\twhat combination of data, technologies, and tools can be used in people management processes to improve organization\u00e2\u0080\u0099s performance\n\u00e2\u0097\u008f\thow to use some of these tools and how to select the ones that suit your objectives and budget\n\u00e2\u0097\u008f\tto design individual and team development plans and measure its ROI for the organization\n\u00e2\u0097\u008f\thow to figure out the qualities that lead employees to their best performance so you know what to encourage in current and look for in new employees\n\u00e2\u0097\u008f\thow to identify the right channels to recruit your employees or team members\n\u00e2\u0097\u008f\twhat combination of monetary and non-monetary motivation tools work best for your organization \n\u00e2\u0097\u008f\thow to predict what people will leave in the near future and how to make sure some of them stay\n\u00e2\u0097\u008f\thow to measure engagement and make a strong organizational culture improve performance Introduction Performance Culture and Assessments Compensation Motivation and Engagement Workforce Planning and Recruitment Development As the name suggests, this module will introduce the course and its content, as well as give an overview of what triggered the explosion of People Analytics.  In this module, we talk about measuring performance, setting KPIs, organizing performance evaluation and touch upon the issue of biases. In your first peer-reviewed assignment you will get to apply your knowledge as a manager of a sports club. You can get in the right mood by watching (or rewatching) Moneyball.  Culture is a very elusive topic, but it is still measurable and it affects performance a lot. In this module we discuss its various aspects, why it affects performance, and what are the \"enemies\" of performance-enhancing organizational cultures In this module we will talk about various compensation models and you will learn how to do compensation benchmarking. In your second peer-reviewed assignment you will apply the new knowledge to conduct your own benchmarking project.   After discussing how monetary motivation works, we continue with non-monetary motivation and engagement and how they affect performance.  In this module, you will learn about recruitment analytics, assessing candidates, optimizing recruitment channels, etc. Your third peer-reviewed assignment will be baaed on a real-life dataset of a startup which took analytical approach to candidate marketing.   The final module is dedicated to training and development planning and calculating its ROI. We hope you will enjoy the final peer-reviewed assignment, where you will be required to calculate the ROI of this class for you. ", "Welcome to Supply Chain Analytics - an exciting and emerging area that is in high demand! \n\nIn this introductory course to Supply Chain Analytics, I will take you on a journey to this fascinating area of Supply chain analytics where supply chain management meets data analytics. You will learn real life stories and examples on how analytics can apply to various domains of a supply chain, from sell, to move, make and source, to generate a significant social and / or economic impact. You will also learn about job market trend, job requirement and preparation. Lastly, you will master a data analysis technique to assess the efficiency of a supply chain. Supply Chain Analytics is a relatively new and fast developing area. Thus this course is by no mean exhaustive nor should you expect to be an expert upon completion. My goal of this course is to open the eyes of the learners to the impact of Supply Chain Analytics, and hopefully this will encourage you to learn more.\n\nUpon completing this course, you will\n\n1. Understand why analytics is critical to today\u00e2\u0080\u0099s supply chains\n2. See the pain points of a supply chain and how analytics can be applied to address them\n3. Understand the requirement of supply chain analytics jobs, and how to prepare for them\n4. Assess a company\u00e2\u0080\u0099s overall supply chain efficiency quantitatively\n\nI hope you enjoy the course! Welcome! Supply Chain Analytics Essentials Cash Cycle to Measure Supply Chain Efficiency Project Welcome to the exciting world of supply chain analytics! This module introduces you to the professor who is teaching this course. You will learn real life examples on how supply chain management can help a company to achieve a long-term competitive advantage. You will also understand the challenges of managing a supply chain and why analytics is critical to supply chain management.  In this module, you will learn the major domains (functional areas) of a supply chain, and understand the objectives and typical problems in each domain and how analytics may help to address them. You will also learn about supply chain analytics job opportunities, requirement and preparation. In this module, you will learn how a company\u00e2\u0080\u0099s financial performance depends on its supply chain partners (customers and suppliers). You will understand why inventory measures and cash conversion cycle are important indicators of a supply chain's efficiency, and you will master the methods to calculate them. In this last module, you will put your new skills to test. You will analyze a real-life data set, and assess and compare the supply chain efficiency between Apple and Samsung. You will also interpret the meanings of their inventory measures and cash conversion cycles.", "This is the first course in the four-course specialization Python Data Products for Predictive Analytics, introducing the basics of reading and manipulating datasets in Python. In this course, you will learn what a data product is and go through several Python libraries to perform data retrieval, processing, and visualization. \n\nThis course will introduce you to the field of data science and prepare you for the next three courses in the Specialization: Design Thinking and Predictive Analytics for Data Products, Meaningful Predictive Modeling, and Deploying Machine Learning Models. At each step in the specialization, you will gain hands-on experience in data manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization. Week 1:  Introduction to Data Products Week 2: Reading Data in Python Week 3: Data Processing in Python Week 4: Python Libraries and Toolkits Final Project This week, we will go over the syllabus and set you up with the course materials and software. We will introduce you to data products and refresh your memory on Python and Jupyter notebooks. This week, we will learn how to load in datasets from CSV and JSON files. We will also practice manipulating data from these datasets with basic Python commands. This week, our goal is to understand how to clean up a dataset before analyzing it. We will go over how to work with different types of  data, such as strings and dates. In this last week, we will get a sense of common libraries in Python and how they can be useful. We will cover data visualization with numpy and MatPlotLib, and also introduce you to the basics of webscraping with urllib and BeautifulSoup. Create your own Jupyter notebook with a dataset of your own choosing and practice data manipulation. Show off the skills you've learned and the libraries you know about in this project. We hope you enjoyed the course, and best of luck in your future learning!", "Inferential statistics are concerned with making inferences based on relations found in the sample, to relations in the population. Inferential statistics help us decide, for example, whether the differences between groups that we see in our data are strong enough to provide support for our hypothesis that group differences exist in general, in the entire population.\n\nWe will start by considering the basic principles of significance testing: the sampling and test statistic distribution, p-value, significance level, power and type I and type II errors. Then we will consider a large number of statistical tests and techniques that help us make inferences for different types of data and different types of research designs. For each individual statistical test we will consider how it works, for what data and design it is appropriate and how results should be interpreted. You will also learn how to perform these tests using freely available software. \n\nFor those who are already familiar with statistical testing: We will look at z-tests for 1 and 2 proportions,  McNemar's test for dependent proportions, t-tests for 1 mean (paired differences) and 2 means, the Chi-square test for independence, Fisher\u00e2\u0080\u0099s exact test, simple regression (linear and exponential) and multiple regression (linear and logistic), one way and factorial analysis of variance, and non-parametric tests (Wilcoxon, Kruskal-Wallis, sign test,  signed-rank test, runs test). Before we get started... Comparing two groups Categorical association Simple regression Multiple regression Analysis of variance Non-parametric tests Exam time! [formatted text here] In this second module of week 1 we dive right in with a quick refresher on statistical hypothesis testing. Since we're assuming you just completed the course Basic Statistics, our treatment is a little more abstract and we go really fast! We provide the relevant Basic Statistics videos in case you need a gentler introduction. After the refresher we discuss methods to compare two groups on a categorical or quantitative dependent variable. We use different test for independent and dependent groups. In this module we tackle categorical association. We'll mainly discuss the Chi-squared test that allows us to decide whether two categorical variables are related in the population. If two categorical variables are unrelated you would expect that categories of these variables don't 'go together'. You would expect the number of cases in each category of one variable to be proportionally similar at each level of the other variable. The Chi-squared test helps us to compare the actual number of cases for each combination of categories (the joint frequencies) to the expected number of cases if the variables are unrelated. In this module we\u00e2\u0080\u0099ll see how to describe the association between two quantitative variables using simple (linear) regression analysis. Regression analysis allows us to model the relation between two quantitative variables and - based on our sample -decide whether a 'real' relation exists in the population. Regression analysis is more useful than just calculating a correlation coefficient, since it allows us assess how well our regression line fits the data, it helps us to identify outliers and to predict scores on the dependent variable for new cases. In this module we\u00e2\u0080\u0099ll see how we can use more than one predictor to describe or predict a quantitative outcome variable. In the social sciences relations between psychological and social variables are generally not very strong, since outcomes are generally influences by complex processes involving many variables. So it really helps to be able to describe an outcome variable with several predictors, not just to increase the fit of the model, but also to assess the individual contribution of each predictor, while controlling for the others.  In this module we'll discuss analysis of variance, a very popular technique that allows us to compare more than two groups on a quantitative dependent variable. The reason we call it analysis of variance is because we compare two estimates of the variance in the population. If the group means differ in the population then these variance estimates differ. Just like in multiple regression, factorial analysis of variance allows us to investigate the influence of several independent variables. In this module we'll discuss the last topic of this course: Non-parametric tests. Until now we've mostly considered tests that require assumptions about the shape of the distribution (z-tests, t-tests and F-tests). Sometimes those assumptions don't hold. Non-parametric tests require fewer of those assumptions. There are several non-parametric tests that correspond to the parametric z-, t- and F-tests. These tests also come in handy when the response variable is an ordered categorical variable as opposed to a quantitative variable. There are also non-parametric equivalents to the correlation coefficient and some tests that have no parametric-counterparts. In this final module there's no new material to study. We advise you to take some extra time to review the material from the previous modules and to practice for the final exam. We've provided a practice exam that you can take as many times as you like. The final exam is structured exactly like the practice exam, so you know what to expect. Please note that you can only take the final exam twice every seven days, so make sure you are fully prepared. Please follow the honor code and do not communicate or confer with others while taking this exam or after. In the open questions of the exam (i.e. those that are not multiple choice) you should report your answers to 3 decimal places, and use 5 decimal places in your calculations. Good luck!", "In this course, you'll get an in-depth look at the SQL SELECT statement and its main clauses. The course focuses on big data SQL engines Apache Hive and Apache Impala, but most of the information is applicable to SQL with traditional RDBMs as well; the instructor explicitly addresses differences for MySQL and PostgreSQL.\n\nBy the end of the course, you will be able to\n\u00e2\u0080\u00a2 explore and navigate databases and tables using different tools;\n\u00e2\u0080\u00a2 understand the basics of SELECT statements;\n\u00e2\u0080\u00a2 understand how and why to filter results;\n\u00e2\u0080\u00a2 explore grouping and aggregation to answer analytic questions;\n\u00e2\u0080\u00a2 work with sorting and limiting results; and \n\u00e2\u0080\u00a2\u00c2\u00a0combine multiple tables in different ways.\n\nTo use the hands-on environment for this course, you need to download and install a virtual machine and the software on which to run it. Before continuing, be sure that you have access to a computer that meets the following hardware and software requirements:\n\u00e2\u0080\u00a2\u00c2\u00a0Windows, macOS, or Linux operating system (iPads and Android tablets will not work)\n\u00e2\u0080\u00a2 64-bit operating system (32-bit operating systems will not work)\n\u00e2\u0080\u00a2 8 GB RAM or more\n\u00e2\u0080\u00a2\u00c2\u00a025GB free disk space or more\n\u00e2\u0080\u00a2 Intel VT-x or AMD-V virtualization support enabled (on Mac computers with Intel processors, this is always enabled;\non Windows and Linux computers, you might need to enable it in the BIOS)\n\u00e2\u0080\u00a2 For Windows XP computers only: You must have an unzip utility such as 7-Zip or WinZip installed (Windows XP\u00e2\u0080\u0099s built-in unzip utility will not work) Orientation to SQL on Big Data SQL SELECT Essentials Filtering Data Grouping and Aggregating Data Sorting and Limiting Data Combining Data      ", "Data science is a team sport. As a data science executive it is your job to recruit, organize, and manage the team to success. In this one-week course, we will cover how you can find the right people to fill out your data science team, how to organize them to give them the best chance to feel empowered and successful, and how to manage your team as it grows. \n\nThis is a focused course designed to rapidly get you up to speed on the process of building and managing a data science team. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know.\n\n1. The different roles in the data science team including data scientist and data engineer\n2. How the data science team relates to other teams in an organization\n3. What are the expected qualifications of different data science team members\n4. Relevant questions for interviewing data scientists\n5. How to manage the onboarding process for the team\n6. How to guide data science teams to success\n7. How to encourage and empower data science teams\n\nCommitment: 1 week of study, 4-6 hours\n\nCourse cover image by JaredZammit. Creative Commons BY-SA. https://flic.kr/p/5vuWZz Building a Data Science Team Welcome to Building a Data Science Team! This course is one module, intended to be taken in one week.  the course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials and every lecture has a 5 question quiz. You need to get 4 out of 5 or better on the quiz to pass. Overall the quizzes are worth 17% of your grade each, with the exception of the last quiz, which is worth 15%. I'm excited to have you in the class and look forward to your contributions to the learning community. Click Discussions to see forums where you can discuss the course material with fellow students taking the class. Be sure to introduce yourself to everyone in the Meet and Greet forum.If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.Good luck as you get started, and I hope you enjoy the course!  -Jeff", "This course is for business analysts and SAS programmers who want to learn data manipulation techniques using the SAS DATA step and procedures to access, transform, and summarize data. The course builds on the concepts that are presented in the Getting Started with SAS Programming course and is not recommended for beginning SAS software users.\n\nIn this course you learn how to understand and control DATA step processing, create an accumulating column and process data in groups, manipulate data with functions, convert column type, create custom formats, concatenate and merge tables, process repetitive code, and restructure tables. This course addresses Base SAS software.\n\nBefore attending this course, you should be able to write DATA step code to access data, subset rows and columns, compute new columns, and process data conditionally. You should also be able to sort tables using the SORT procedure and\napply SAS formats. Course Overview and Data Setup Controlling DATA Step Processing Summarizing Data Manipulating Data with Functions Creating and Using Custom Formats Combining Tables Processing Repetitive Code Restructuring Tables In this module you get an overview of what you learn in this course and your set up for software and the data you use for activities and practices in the course. In this module, we dig deeper into the DATA step. You learn how the DATA step processes data behind the scenes. Then you use this knowledge to control when and where the DATA step outputs rows to new tables. In this module, you learn new syntax that enables you to alter the default behavior of the DATA step to solve a problem. First you learn to create an accumulating column, or in other words generate a running total.  Then you learn to process data in groups, so you can perform an action when each group begins or ends.    In this module, you learn to use some new functions that enable you to manipulate numeric, date, and character values. In addition, you learn to use functions that change a column from one data type to another. In this module, you learn to create and use custom formats to enhance the way your data is displayed in a table or report. In this module, we take a comprehensive look at combining tables by using the DATA step. You learn to concatenate tables, merge tables, and identify matching and nonmatching rows. In this module, you learn to save time by taking advantage of iterative processing with DO loops. First you learn to create an iterative DO Loop, then you learn to create conditional DO loops. In this module, you learn techniques that can be used to transpose or restructure a table. First you learn to restructure data with the DATA step. Then you learn to restructure data by using the TRANSPOSE procedure.", "The course aims at helping students to be able to solve practical ML-amenable problems that they may encounter in real life that include: (1) understanding where the problem one faces lands on a general landscape of available ML methods, (2) understanding which particular ML approach(es) would be most appropriate for resolving the problem, and (3) ability to successfully implement a solution, and assess its performance.  \nA learner with some or no previous knowledge of Machine Learning (ML)  will get to know main algorithms of Supervised and Unsupervised Learning, and Reinforcement Learning, and will be able to use ML open source Python packages to design, test, and implement ML algorithms in Finance.\nFundamentals of Machine Learning in Finance will provide more at-depth view of supervised, unsupervised, and reinforcement learning, and end up in a project on using unsupervised learning for implementing a simple portfolio trading strategy.\n\nThe course is designed for three categories of students:\nPractitioners working at financial institutions such as banks, asset management firms or hedge funds\nIndividuals interested in applications of ML for personal day trading\nCurrent full-time students pursuing a degree in Finance, Statistics, Computer Science, Mathematics, Physics, Engineering or other related disciplines who want to learn about practical applications of ML in Finance  \n\nExperience with Python (including numpy, pandas, and IPython/Jupyter notebooks), linear algebra, basic probability theory and basic calculus is necessary to complete assignments in this course. Fundamentals of Supervised Learning in Finance Core Concepts of Unsupervised Learning, PCA & Dimensionality Reduction Data Visualization & Clustering Sequence Modeling and Reinforcement Learning \n \n  ", "Learner Outcomes: After taking this course, you will be able to:\n- Utilize various Application Programming Interface (API) services to collect data from different social media sources such as YouTube, Twitter, and Flickr.\n- Process the collected data - primarily structured - using methods involving correlation, regression, and classification to derive insights about the sources and people who generated that data.\n- Analyze unstructured data - primarily textual comments - for sentiments expressed in them.\n- Use different tools for collecting, analyzing, and exploring social media data for research and development purposes.\n\nSample Learner Story: Data analyst wanting to leverage social media data.\nIsabella is a Data Analyst working as a consultant for a multinational corporation. She has experience working with Web analysis tools as well as marketing data. She wants to now expand into social media arena, trying to leverage the vast amounts of data available through various social media channels. Specifically, she wants to see how their clients, partners, and competitors view their products/services and talk about them. She hopes to build a new workflow of data analytics that incorporates traditional data processing using Web and marketing tools, as well as newer methods of using social media data.\n\nSample Job Roles requiring these skills: \n- Social Media Analyst\n- Web Analyst\n- Data Analyst\n- Marketing and Public Relations \n\nFinal Project Deliverable/ Artifact: The course will have a series of small assignments or mini-projects that involve data collection, analysis, and presentation involving various social media sources using the techniques learned in the class. Introduction to Data Analytics Collecting and Extracting Social Media Data Data Analysis, Visualization, and Exploration Case Studies In this first unit of the course, several concepts related to social media data and data analytics are introduced. We start by first discussing two kinds of data - structured and unstructured. Then look at how structured data, the primary focus of this course, is analyzed and what one could gain by doing such analysis. Finally, we briefly cover some of the visualizations for exploring and presenting data.Make sure to go through the material for this unit in the sequence it's provided. First, watch the four short videos, then take the practice test, followed by the two quizzes. Finally, read the documents about installation and configuration of Python and R. This is very important - before proceeding to the next units, make sure you have installed necessary tools, and also learned how to install new packages/libraries for them. The course expects students to have programming experience in Python and R. In this unit we will see how to collect data from Twitter and YouTube. The unit will start with an introduction to Python programming. Then we will use a Python script, with a little editing, to extract data from Twitter. A similar exercise will then be done with YouTube. In both the cases, we will also see how to create developer accounts and what information to obtain to use the data collection APIs.\n\nOnce again, make sure to go item-by-item in the order provided. Before beginning this unit, ensure that you have all the right tools (Python, R, Anaconda) ready and configured. The lessons depend on them and also your ability to install required packages. In this unit, we will focus on analyzing and visualizing the data from various social media services. We will first use the data collected before from YouTube to do various statistics analyses such as correlation and regression. We will then introduce R - a platform for doing statistical analysis. Using R, then we will analyze a much larger dataset obtained from Yelp.\n\nMake sure you have covered the material in the previous units before proceeding with this. That means, having all the tools (Anaconda, Python, and R) as well as various packages installed. We will also need new packages this time, so make sure you know how to install them to your Python or R. If needed, please review some basic concepts in statistics - specifically, correlation and regression - before or during working on this unit. In the final unit of this course, we will work on two case studies - both using Twitter and focusing on unstructured data (in this case, text). The first case study will involve doing sentiment analysis with Python. The second case study will take us through basic text mining application using R. We wrap up the unit with a conclusion of what we did in this course and where to go next for further learning and exploration.", "This course will prepare you to complete all parts of the Clinical Data Science Specialization. In this course you will learn how clinical data are generated, the format of these data, and the ethical and legal restrictions on these data. You will also learn enough SQL and R programming skills to be able to complete the entire Specialization - even if you are a beginner programmer. While you are taking this course you will have access to an actual clinical data set and a free, online computational environment for data science hosted by our Industry Partner Google Cloud. \n\nAt the end of this course you will be prepared to embark on your clinical data science education journey, learning how to take data created by the healthcare system and improve the health of tomorrow's patients. Welcome to the Clinical Data Science Specialization Introduction: Clinical Data Tools: SQL Tools: R and the Tidyverse Learn what clinical data science is all about and get access to the free technology environment hosted by Google Cloud! Clinical data are complex. Walk through the four-W's of clinical data to understand where they come from and what they look like.  Develop basic skills in SQL (Structured Query Language) and query the real clinical data set used in the Clinical Data Science Specialization. Learn how to use the tidyverse to implement your Clinical Data Science Workflow in R. ", "Who is this course for?  \nThis course is designed for students, business analysts, and data scientists who want to apply statistical knowledge and techniques to business contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a business role. \n\nYou will find this course exciting and rewarding if you already have a background in statistics, can use R or another programming language and are familiar with databases and data analysis techniques such as regression, classification, and clustering.\nHowever, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to play more freely with data and explore new features and statistical functions in R.\n\nWith this course, you\u00e2\u0080\u0099ll have a first overview on Strategic Business Analytics topics. We\u00e2\u0080\u0099ll discuss a wide variety of applications of Business Analytics. From Marketing to Supply Chain or Credit Scoring and HR Analytics, etc. We\u00e2\u0080\u0099ll cover many different data analytics techniques, each time explaining how to be relevant for your business.\n\nWe\u00e2\u0080\u0099ll pay special attention to how you can produce convincing, actionable, and efficient insights. We'll also present you with different data analytics tools to be applied to different types of issues.\nBy doing so, we\u00e2\u0080\u0099ll help you develop four sets of skills needed to leverage value from data: Analytics, IT, Business and Communication. \n\nBy the end of this MOOC, you should be able to approach a business issue using Analytics by (1) qualifying the issue at hand in quantitative terms, (2) conducting relevant data analyses, and (3) presenting your conclusions and recommendations in a business-oriented, actionable and efficient way.\n\nPrerequisites : 1/ Be able to use R or to program 2/ To know the fundamentals of databases, data analysis (regression, classification, clustering)\n\nWe give credit to Pauline Glikman, Albane Gaubert, Elias Abou Khalil-Lanvin (Students at ESSEC BUSINESS SCHOOL) for their contribution to this course design. Introduction to Strategic Business Analytics Finding groups within Data Factors leading to events Predictions and Forecasting Recommendation production and prioritization  In this module, we will introduce you to the course and instructional approach. You will learn that Strategic Business Analytics relies on four distinct skills: IT, Analytics, Business and Communication. In this module, you will learn how identifying groups of observations enables you to improve business efficiency. You will then learn to create those groups in a business-oriented and actionable way. We will use examples to illustrate various concepts. The assessments will also provide you with opportunities to replicate these examples. In this module, you will learn why using rigorous statistical methods to understand the relationship between different events is crucial. \n\nWe\u00e2\u0080\u0099ll cover two examples: first, using a credit scoring example, you will learn how to derive information about what makes an individual more or less likely to have a strong credit score? Then, in a second example drawn from HR Analytics, you will learn to estimate what makes an employee more or less likely to leave the company. \nAs usual, we invite you to replicate those examples thanks to the recital \nand to use the assessments provided at the end of the module to strengthen your understanding of these concepts.  In this module you will learn more about the importance of forecasting the future.\n\nYou will learn through examples from various sectors: first, using the previous examples of credit scoring and HR Analytics, you will learn to predict what will happen. Then, you will be introduced to predictive maintenance using survival analysis via a case discussion. Finally, we\u00e2\u0080\u0099ll discuss seasonality in the context of the first example discussed in this MOOC: using analytics for managing your supply chain and logistics better. \n So far, you\u00e2\u0080\u0099ve learnt to use Business Analytics to glean important information relevant to the success of your business. In this module, you\u00e2\u0080\u0099ll learn more about how to present your Business Analytics work to a business audience. This module is also important for your final capstone project presentation.You\u00e2\u0080\u0099ll learn that it is important to find an angle, and tell a story.Instead of presenting a list of results that are not connected to each other, you will learn to take your audience by the hand and steer it to the recommendations you want to conclude on.You\u00e2\u0080\u0099ll learn to structure your story and your slides, and master the most used visualization tips and tricks. The assessment at the end of this module will provide an opportunity for you to practice these methods and to prepare the first step of the capstone project.", "This course is designed to show you how use quantitative models to transform data into better business decisions. You\u00e2\u0080\u0099ll learn both how to use models to facilitate decision-making and also how to structure decision-making for optimum results. Two of Wharton\u00e2\u0080\u0099s most acclaimed professors will show you the step-by-step processes of modeling common business and financial scenarios, so you can significantly improve your ability to structure complex problems and derive useful insights about alternatives. Once you\u00e2\u0080\u0099ve created models of existing realities, possible risks, and alternative scenarios, you can determine the best solution for your business or enterprise, using the decision-making tools and techniques you\u00e2\u0080\u0099ve learned in this course. Evaluation Criteria: Net Present Value Evaluating Projects Expressing Business Strategies in Financial Terms New Product Value This module was designed to introduce you to the many potential criteria for selecting investment projects, and to explore the most effective of these criteria: Net Present Value (NPV). Through the use of concrete examples, you'll learn the key components of Net Present Value, including the time value of money and the cost of capital, the main utility of NPV, and why it is ultimately more accurate and useful for evaluating projects than other commonly used criteria. By the end of this module, you'll be able to explain why net present value analysis is the appropriate criteria for choosing whether to accept or reject a project, and why other criteria, such as IRR, payback, ROI, etc. may not lead to decisions which maximize value. In this module, you'll learn how to evaluate a project with emphasis on analyzing the incremental after-tax cash flows associated with the project. You'll work through a concrete example using alternative scenarios to test the effectiveness of this method. You'll also learn why only future cash flows are relevant, why to ignore financial costs, include all incidental effects, remember working capital requirements, consider the effect of taxes, forget sunk costs, remember opportunity costs, use expected cash flows, and perform sensitivity analysis.  By the end of this module, you'll be able to evaluate projects more thoroughly and effectively, with emphasis on how to model the change in the company\u00e2\u0080\u0099s after-tax cash flows, so that you can make more profitable decisions. This module was designed to give you the opportunity to learn how business activities, transactions and events are translated into financial statements, including balance sheets, income statements, and cash flow statements. You'll also learn how these three statements are linked to each other, and how balance sheets and income statements can help forecast the future cash flow statements. By the end of this module, you'll be able to explain how accounting systems translate business activities into financial terms, and how to use this to better forecast future cash flows, so that you can express your business strategies in these financial terms, and show \"the bottom line\" for your proposed plan of action. In this module, you'll apply what you\u00e2\u0080\u0099ve been learning to an analysis of a new product venture. You\u00e2\u0080\u0099ll learn how to map out a plan of the business activities, transactions and events that need to happen to implement the new venture, including their timing. You'll also learn how to set up a spreadsheet to help with forecasts, and to re-calculate things automatically as we re-think our plans. You'll see how to forecast out the implied financial statements, and calculate the Net Present Value (NPV). By the end of this module, you'll be able to use spreadsheets to explore different risks a venture may face, and analyze the implications of these scenarios for NPV, so that you can make the most profitable, data-driven decision possible.", "We will learn computational methods -- algorithms and data structures -- for analyzing DNA sequencing data. We will learn a little about DNA, genomics, and how DNA sequencing is used.  We will use Python to implement key algorithms and data structures and to analyze real genomes and DNA sequencing datasets. DNA sequencing, strings and matching Preprocessing, indexing and approximate matching Edit distance, assembly, overlaps Algorithms for assembly This module we begin our exploration of algorithms for analyzing DNA sequencing data. We'll discuss DNA sequencing technology, its past and present, and how it works.\n In this module, we learn useful and flexible new algorithms for solving the exact and approximate matching problems.  We'll start by learning Boyer-Moore, a fast and very widely used algorithm for exact matching This week we finish our discussion of read alignment by learning about algorithms that solve both the edit distance problem and related biosequence analysis problems, like global and local alignment. In the last module we began our discussion of the assembly problem and we saw a couple basic principles behind it.  In this module, we'll learn a few ways to solve the alignment problem.", "In this course, we will expand our exploration of statistical inference techniques by focusing on the science and art of fitting statistical models to data. We will build on the concepts presented in the Statistical Inference course (Course 2) to emphasize the importance of connecting research questions to our data analysis methods. We will also focus on various modeling objectives, including making inference about relationships between variables and generating predictions for future observations.\n\nThis course will introduce and explore various statistical modeling techniques, including linear regression, logistic regression, generalized linear models, hierarchical and mixed effects (or multilevel) models, and Bayesian inference techniques. All techniques will be illustrated using a variety of real data sets, and the course will emphasize different modeling approaches for different types of data sets, depending on the study design underlying the data (referring back to Course 1, Understanding and Visualizing Data with Python).\n\nDuring these lab-based sessions, learners will work through tutorials focusing on specific case studies to help solidify the week\u00e2\u0080\u0099s statistical concepts, which will include further deep dives into Python libraries including Statsmodels, Pandas, and Seaborn. This course utilizes the Jupyter Notebook environment within Coursera. WEEK 1 - OVERVIEW & CONSIDERATIONS FOR STATISTICAL MODELING WEEK 2 - FITTING MODELS TO INDEPENDENT DATA WEEK 3 - FITTING MODELS TO DEPENDENT DATA WEEK 4: Special Topics We begin this third course of the Statistics with Python specialization with an overview of what is meant by \u00e2\u0080\u009cfitting statistical models to data.\u00e2\u0080\u009d In this first week, we will introduce key model fitting concepts, including the distinction between dependent and independent variables, how to account for study designs when fitting models, assessing the quality of model fit, exploring how different types of variables are handled in statistical modeling, and clearly defining the objectives of fitting models. In this second week, we\u00e2\u0080\u0099ll introduce you to the basics of two types of regression: linear regression and logistic regression.  You\u00e2\u0080\u0099ll get the chance to think about how to fit models, how to assess how well those models fit, and to consider how to interpret those models in the context of the data.  You\u00e2\u0080\u0099ll also learn how to implement those models within Python. In the third week of this course, we will be building upon the modeling concepts discussed in Week 2. Multilevel and marginal models will be our main topic of discussion, as these models enable researchers to account for dependencies in variables of interest introduced by study designs. We\u00e2\u0080\u0099ll be covering why and when we fit these alternative models, likelihood ratio tests, as well as fixed effects and their interpretations.  In this final week, we introduce special topics that extend the curriculum from previous weeks and courses further. We will cover a broad range of topics such as various types of dependent variables, exploring sampling methods and whether or not to use survey weights when fitting models, and in-depth case studies utilizing Bayesian techniques to derive insights from data. You\u00e2\u0080\u0099ll also have the opportunity to apply Bayesian techniques in Python.", "This course will expose you to the transformation taking place, throughout the world, in the way that products are being designed and manufactured. The transformation is happening through digital manufacturing and design (DM&D) \u00e2\u0080\u0093 a shift from paper-based processes to digital processes in the manufacturing industry. By the end of this course, you\u00e2\u0080\u0099ll understand what DMD is and how it is impacting careers, practices and processes in companies both large and small.  \n\nYou will gain an understanding of and appreciation for the role that technology is playing in this transition. The technology we use every day \u00e2\u0080\u0093 whether it is communicating with friends and family, purchasing products or streaming entertainment \u00e2\u0080\u0093 can benefit design and manufacturing, making companies and workers more competitive, agile and productive. Discover how this new approach to making products makes companies more responsive, and employees more involved and engaged, as new career paths in advanced manufacturing evolve.\n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the first course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA The Big Picture Components of the Paradigm The purpose of this module is to introduce learners to the factors and trends motivating the transition from the current state of manufacturing to a DMD model. Details of individual lessons in this module are provided below. The purpose of this module is to introduce learners to the multiple components that integrate to create a future manufacturing enterprise (i.e., a digital link between design and production, leveraging data analytics to identify opportunities for increased quality and efficiency, interconnected and transparent machines, production facilities, and supply chains).  Details of individual lessons in this module are provided below.", "We are always using experiments to improve our lives, our community, and our work. Are you doing it efficiently? Or are you (incorrectly) changing one thing at a time and hoping for the best? \n\nIn this course, you will learn how to plan efficient experiments - testing with many variables. Our goal is to find the best results using only a few experiments. A key part of the course is how to optimize a system.\n\nWe use simple tools: starting with fast calculations by hand, then we show how to use FREE software. \n\nThe course comes with slides, transcripts of all lectures, subtitles (English, Spanish and Portuguese; some Chinese and French), videos, audio files, source code, and a free textbook. You get to keep all of it, all freely downloadable.\n\nThis course is for anyone working in a company, or wanting to make changes to their life, their community, their neighbourhood. You don't need to be a statistician or scientist! There's something for everyone in here. \n\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\nOver 1000 people have completed this online course. What have prior students said about this course?\n\n\"This definitely is one of the most fruitful courses I have participated at Coursera, considering the takeaways and implementations! And so far I finished 12 [courses].\"\n\n\"Excelente curso, flexible y con suficiente material did\u00c3\u00a1ctico f\u00c3\u00a1cilmente digerible y c\u00c3\u00b3modo. No importa si se tiene pocas bases matem\u00c3\u00a1ticas o estad\u00c3\u00adsticas, el curso proporciona casi toda explicaci\u00c3\u00b3n necesaria para un entendimiento alto.\"\n\n\"I wish I had enrolled in your course years ago -- it would have saved us a lot of time in optimizing experimental conditions.\" Jason Eriksen, 3 Jan 2017\n\n\"Interesting and developing both analytical and creative thinking. The lecturer took care to bring lots of real live examples which are fun to analyze.\" 20 February 2016.\n\n\"... love your style of presentation, and the examples you took from everyday life to explain things. It is very difficult to make such a mathematical course accessible and comprehensible to this wide a variety of people!\"\n\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af\u00e2\u008e\u00af Introduction Analysis of experiments by hand Using computer software to analyze experiments Getting more information, with fewer experiments Response surface methods (RSM) to optimize any system Wrap-up and future directions We perform experiments all the time, so let's learn some terminology that we will use throughout the course. We show plenty of examples, and see how to analyze an experiment. We end by pointing out: \"how not to run an experiment\". The focus is on manual calculations. Why? Because you have to understand the most basic building blocks of efficient experiments. We look at systems with 2 and 3 variables (factors). Don't worry; the computer will do the work in the next module. Now we use free software to do the work for us. You can even run the software through a website (without installing anything special). We look at systems with 2, 3 and 4 factors. Most importantly we focus on the software interpretation. This is where the course gets tough and rough, but real. The quiz at the end if a tough one, so take it several times to be sure you have mastered the material - that's all that matters - understanding. We want to do as few experiments as possible, while still learning the most we can. Feel free to skip to module 5, which is the crucial learning from the whole course. You can come back here later. In module 4 we show how to do *practical* experiments that practitioners use everyday. We learn about important safeguards to ensure that we are not mislead by Mother Nature. This is the goal we've been working towards: how to optimize any system. We start gently. We optimize a system with 1 factor and we also show why optimizing one factor at a time is misleading. We spend several videos to show how to optimize a system with 2 variables. We close up the course and point out the next steps you might follow to extend what you have learned here.", "Spatial (map) is considered as a core infrastructure of modern IT world, which is substantiated by business transactions of major IT companies such as Apple, Google, Microsoft, Amazon, Intel, and Uber, and even motor companies such as Audi, BMW, and Mercedes. Consequently, they are bound to hire more and more spatial data scientists.  Based on such business trend, this course is designed to present a firm understanding of spatial data science to the learners, who would have a basic knowledge of data science and data analysis, and eventually to make their expertise differentiated from other nominal data scientists and data analysts.  Additionally, this course could make learners realize the value of spatial big data and the power of open source software's to deal with spatial data science problems.\n\nThis course will start with defining spatial data science and answering why spatial is special from three different perspectives - business, technology, and data in the first week.  In the second week, four disciplines related to spatial data science - GIS, DBMS, Data Analytics, and Big Data Systems, and the related open source software's - QGIS, PostgreSQL, PostGIS, R, and Hadoop tools are introduced together.  During the third, fourth, and fifth weeks, you will learn the four disciplines one by one from the principle to applications.  In the final week, five real world problems and the corresponding solutions are presented with step-by-step procedures in environment of open source software's. Understanding Spatial Data Science Solution Structures of Spatial Data Science Problems Geographic Information System (GIS) Spatial DBMS and Big Data Systems Spatial Data Analytics  Practical Applications of Spatial Data Science The first module of \"Spatial Data Science and Applications\" is entitled to \"Understanding of Spatial Data Science.\"  This module is composed of four lectures.  The first lecture \"Introduction to spatial data  science\" was designed to give learners a solid concept of spatial data science in comparison with science, data science, and spatial data science. For Learner's better understanding, examples of spatial data science problems are also presented.  The second, third, and fourth lectures focuses on \"what is spatial special? - unique aspects of spatial data science from three perspectives of business, technology, and data, respectively.   In the second lecture, learners will learn five reasons why major IT companies are serious about spatial data, in other words, maps.   The third lecture will allow learners to understand four issues of dealing with spatial data, including DBMS problems, topology, spatial indexing, and spatial big data problems. The fourth lecture will allow learners to understand another four issues of spatial data including spatial autocorrelation, map projection, uncertainty, and modifiable areal unit problem. The second module is entitled to \"Solution Structures of Spatial Data Science Problems\", which is composed of four lectures and will give learners an overview of academic subjects, software tools, and their combinations for the solution structures of spatial data science problems. The first lecture, \"Four Disciplines for Spatial Data Science and Applications\" will introduce four academic disciplines related to spatial data science, which are Geographic Information System (GIS), Database Management System (DBMS), Data Analytics, and Big Data Systems.  The second lecture \"Open Source Software's\" will introduce open source software's in the four related disciplines, QGIS for GIS, PostgreSQL and PostGIS for DBMS, R for Data Analytics, Hadoop and Hadoop-based solutions for Big Data System, which will be used throughout this course.  The third lecture \"Spatial Data Science Problems\" will present six solution structures, which are different combinations of GIS, DBMS, Data Analytics, and Big Data Systems. The solution structures are related to the characteristics of given problems, which are the data size, the number of users, level of analysis, and main focus of problems.  The fourth lecture \"Spatial Data vs. Spatial Big Data\" will make learner have a solid understanding of spatial data and spatial big data in terms of similarity and differences.  Additionally, the value of spatial big data will be discussed. The third module is \"Geographic Information System (GIS)\", which is one of the four disciplines for spatial data science.  GIS has five layers, which are spatial reference framework, spatial data model, spatial data acquisition systems, spatial data analysis, and geo-visualization.  This module is composed of six lecture.  The first lecture \"Five Layers of GIS\" is an introduction to the third module.  The rest of the lectures will cover the five layers of GIS, one by one.  The second lecture  \"Spatial Reference Framework\" will make learners understand, first, a series of formulation steps of physical earth, geoid, ellipsoid, datum, and map projections, second, coordinate transformation between different map projections.  The third lecture \"Spatial Data Models\" will teach learners how to represent spatial reality in two spatial data models - vector model and raster model.  The fourth lecture \"Spatial Data Acquisition Systems\" will cover topics on how and where to acquire spatial data and how to produce your own spatial data. The fifth lecture \"Spatial Data Analysis\", will make learners to have brief taste of how to extract useful and valuable information from spatial data. More advanced algorithms for spatial analysis will be covered in the fifth module.  In the sixth lecture \"Geovisualization and Information Delivery\", learners will understand powerful aspects as well as negative potentials of cartographic representations as a communication media of spatial phenomenon.     The fourth module is entitled to \"Spatial DBMS and Big Data Systems\", which covers two disciplines related to spatial data science, and will make learners understand how to use DBMS and Big Data Systems to manage spatial data and spatial big data.  This module is composed of six lectures.  The first two lectures will cover DBMS and Spatial DBMS, and the rest of the lectures will cover Big Data Systems.  The first lecture \"Database Management System (DBMS)\" will introduce powerful functionalities of DBMS and related features, and limitations of conventional Relational DBMS for spatial data.  The second lecture \"Spatial DBMS\" focuses on the difference of spatial DBMS from conventional DBMS, and new features to manage spatial data.  The third lecture will give learners a brief overview of Big Data Systems and the current paradigm - MapReduce.  The fourth lecture will cover Hadoop MapReduce, Hadoop Distributed File System (HDFS), Hadoop YARN, as an implementation of MapReduce paradigm, and also will present the first example of spatial big data processing using Hadoop MapReduce. The fifth lecture will introduce Hadoop ecosystem and show how to utilize Hadoop tools such as Hive, Pig, Sqoop, and HBase for spatial big data processing.  The last lecture \"Spatial Big Data System\" will introduce two Hadoop tools for spatial big data - Spatial Hadoop and GIS Tools for Hadoop, and review their pros and cons for spatial big data management and processing.  The fifth module is entitled to \"Spatial Data Analytics\", which is one of the four disciplines related to spatial data science.  Spatial Data Analytics could cover a wide spectrum of spatial analysis methods, however, in this module, only some portion of spatial data analysis methods will be covered.  The first lecture is an introduction, in which an overview of Spatial Data Analytics and a list of six topics are given and discussed.  The second lecture \"Proximity and Accessibility\" will make learners realize how spatial data science can be used for business applications, while trade area analysis, supply to demand ratio, Floating Catchment Analysis (FCA), and Gravity-based index of accessibility are introduced and applied to real world problems. The third lecture \"Spatial Autocorrelation\" will give an instruction on how to measure spatial autocorrelation and to apply hypothesis test with Moran's I.  The fourth lecture \"Spatial Interpolation\" will introduce trend surface analysis, inverse distance weighting, and Kriging. Particularly, in-depth explanations regarding Kriging, a de facto standard of spatial interpolation will be presented.  The fifth lecture \"Spatial Categorization\" will make learners understand classification algorithms such as Minimum Distance to Mean (MDM) and Decision Tree (DT), clustering algorithms such as K-Means and DBSCAN with real-world examples.  The sixth lecture \"Hotspot Analysis\" will introduce hotspot analysis and Getis-Ord GI* as the most popular method.  The seventh lecture \"Network Analysis\" will make learners explore the algorithms of geocoding, map matching, and shortest path finding, of which importance is increasing in spatial big data analysis. The sixth module is entitled to \"Practical Applications of Spatial Data Science\", in which five real-world problems are introduced and corresponding solutions are presented with step-by-step procedures in the solution structures and related open source software's, discussed in Module 2.  The first lecture presents an example of Desktop GIS, in which only QGIS is used,  to find the top 5 counties for timberland investment in the southeastern states of the U.S, in which simple differencing of demand and supply is applied to figure out counties of large deficit of timber supply in comparison with timber demand.  In the second lecture, an example of sever GIS, in which QGIS and PostgreSQL/PostGIS are used, will be presented as a solution for a given problem of NYC spatial data center, which required multiple user access and different levels of privileges.  The third lecture presents an example of spatial data analytics, in which QGIS and R are used, to find out any regional factors which contribute to higher or lower disease prevalence in administrative districts, for which spatial autocorrelation analysis is conducted and decision tree analysis is applied.  The fourth lecture is another example of spatial data analytics, to find optimal infiltration routing with network analysis, in which cost surface is produced and Dijkstra's algorithm is used.  The fifth lecture is an example of spatial big data management and analytics, in which QGIS, PostGIS, R, and Hadoop MapReduce are all used, to provide a solution of \"Passenger Finder\", which can guide to the places where more passengers are waiting for taxi cabs.  For the solution, spatial big data, taxi trajectory, are collected, and noise removal and map matching are conducted in Hadoop environment.  Then, a series of spatial data processing and analysis such as spatial join in PostGIS, hotspot analysis in R are conducted in order to provide the solution.  All in all, learners will realize the value of spatial big data and power of the solution structure with combination of four disciplines. ", "This course aims at introducing the fundamental concepts of Reinforcement Learning (RL), and develop use cases for applications of RL for option valuation, trading, and asset management. \n\nBy the end of this course, students will be able to\n- Use reinforcement learning to solve classical problems of Finance such as portfolio optimization, optimal trading, and option pricing and risk management.\n- Practice on valuable examples such as famous Q-learning using financial problems.\n- Apply their knowledge acquired in the course to a simple model for market dynamics that is obtained using reinforcement learning as the course project.\n\nPrerequisites are the courses \"Guided Tour of Machine Learning in Finance\" and \"Fundamentals of Machine Learning in Finance\". Students are expected to know the lognormal process and how it can be simulated. Knowledge of option pricing is not assumed but desirable. MDP and Reinforcement Learning MDP model for option pricing: Dynamic Programming Approach MDP model for option pricing - Reinforcement Learning approach RL and INVERSE RL for Portfolio Stock Trading    ", "Learn to use the tools that are available from the Galaxy Project. This is the second course in the Genomic Big Data Science Specialization. Introduction Galaxy 101 Working with sequence data RNA-seq & Running your own Galaxy This week, we will present some of the research challenges that motivated the development of the Galaxy framework. We will then introduce Galaxy, describe what the Galaxy framework is, and look at different ways you can use it. In this module and the following modules we will start to use Galaxy to perform different types of analysis.  In this module we will be studying sequence data quality control as well as ChIP-Sequence Analysis with MACS. In these final modules, we'll take a look at working with sequence data and RNA-seq and at installing and running your own Galaxy.", "This is the fourth course in the Data Warehouse for Business Intelligence specialization. Ideally, the courses should be taken in sequence.  In this course, you will gain the knowledge and skills for using data warehouses for business intelligence purposes and for working as a business intelligence developer. You\u00e2\u0080\u0099ll have the opportunity to work with large data sets in a data warehouse environment and will learn the use of MicroStrategy's Online Analytical Processing (OLAP) and Visualization capabilities to create visualizations and dashboards. \n\nThe course gives an overview of how business intelligence technologies can support decision making across any number of business sectors. These technologies have had a profound impact on corporate strategy, performance, and competitiveness and broadly encompass  decision support systems, business intelligence systems, and visual analytics. Modules are organized around the business intelligence concepts, tools, and applications, and the use of data warehouse for business reporting and online analytical processing, for creating visualizations and dashboards, and for business performance management and descriptive analytics. Decision Making and Decision Support Systems Business Intelligence Concepts and Platform Capabilities Data Visualization and Dashboard Design  Business Performance Management Systems BI Maturity, Strategy, and Summative Project Module 1 explains the role of computerized support for decision making and its importance. It starts by identifying the different types of decisions managers face, and the process through which they make decisions. It then focuses on decision-making styles, the four stages of Simon\u00e2\u0080\u0099s decision-making process, and common strategies and approaches of decision makers. In the next two lessons, you will learn the role of  Decision Support Systems (DSS), understand its main components, the various DSS types and classification, and how DSS have changed over time. Finally, in lesson 4, we focus on how DSS supports each phase of decision making and summarize the evolution of DSS applications, and on how they have changed over time.  I recommend that you go to Ready Made DSS sites and use some of DSS that are listed for various types of decisions. You will need to install MicroStrategy Desktop to analyze three stand-alone offline dashboards in a peer-evaluated exercise. Now that you understand the conceptual foundation of decision making and DSS, in module 2 we start by defining business intelligence (BI), BI architecture, and its components, and relate them to DSS.  In lesson 2, you will learn the main components of  BI platforms, their capabilities, and understand the competitive landscape of BI platforms. In lesson 3, you will learn the building blocks of business reports, the types of business reports, and the components and structure of business reporting systems. Finally, in lesson 4, you will learn different types of OLAP and their applications and comprehend the differences between OLAP and OLTP. You will need to use MicroStrategy Desktop to create effective and compelling data visualizations to analyze data and acquire insights into business practices in a peer-evaluated exercise.  This module continues on the top job responsibilities of BI analysts by focusing on creating data visualizations and dashboards. You will first learn the importance of data visualization and different types of data that can be visually represented. You will then learn about the types of basic and composite charts. This will help you to determine which visualization is most effective to display data for a given data set and to identify best practices for designing data visualizations. In lesson 3, you will learn the common characteristics of a dashboard, the types of dashboards, and the list attributes of metrics usually included in dashboards. Finally, in lesson 4, you will learn the guidelines for designing dashboard and the common pitfalls of dashboard design. You will need to use MicroStrategy  Desktop Visual Insight to design a dashboard for a Financial Services company in a peer-evaluated exercise. This module focuses on how BI is used for Business Performance Management (BPM). You will learn the main components of BPM as well as the four phases of BPM cycle and how organizations typically deploy BPM. In lesson 2, you will learn the purpose of Performance Measurement System and how organizations need to define the key performance indicators (KPIs) for their performance management system. In lesson 3, you will learn the four balanced scorecards perspectives and the differences between dashboards and scorecards. You will also be able to compare and contrast the benefits of using balanced scorecard versus using Six Sigma in a performance measurement system. Finally, in lesson 4, you will learn the role of visual and business analytics (BA) in BI and how various forms of BA are supported in practice. At the end of the module, you will apply these concepts to create a dashboard, blend it with external datasets, and explore various visualization capabilities to find insights faster in a peer-evaluated exercise. Module 5 covers BI maturity and strategy. You will learn different levels of BI maturity, the factors that impact BI maturity within an organization, and the main challenges and the potential solutions for a pervasive BI maturity within an organization. The last lesson will focus on the critical success factors for implementing a BI strategy, BI framework, and BI implementation targets. Finally, in your summative project, you will use MicroStrategy visual analytics capabilities to analyze KPIs for a fast food company to find the causes for problems .", "Your smartphone, smartwatch, and automobile (if it is a newer model) have AI (Artificial Intelligence) inside serving you every day. In the near future, more advanced \u00e2\u0080\u009cself-learning\u00e2\u0080\u009d capable DL (Deep Learning) and ML (Machine Learning) technology will be used in almost every aspect of your business and industry. So now is the right time to learn what DL and ML is and how to use it in advantage of your company. This course has three parts, where the first part focuses on DL and ML technology based future business strategy including details on new state-of-the-art products/services and open source DL software, which are the future enablers. The second part focuses on the core technologies of DL and ML systems, which include NN (Neural Network), CNN (Convolutional NN), and RNN (Recurrent NN) systems. The third part focuses on four TensorFlow Playground projects, where experience on designing DL NNs can be gained using an easy and fun yet very powerful application called the TensorFlow Playground. This course was designed to help you build business strategies and enable you to conduct technical planning on new DL and ML services and products. Deep Learning Products & Services  Business with Deep Learning & Machine Learning Deep Learning Computing Systems & Software Basics of Deep Learning Neural Networks Deep Learning with CNN & RNN  Deep Learning Project with  TensorFlow Playground For the course \u00e2\u0080\u009cDeep Learning for Business,\u00e2\u0080\u009d the first module is \u00e2\u0080\u009cDeep Learning Products & Services,\u00e2\u0080\u009d which starts with the lecture \u00e2\u0080\u009cFuture Industry Evolution & Artificial Intelligence\u00e2\u0080\u009d that explains past, current, and future industry evolutions and how DL (Deep Learning) and ML (Machine Learning) technology will be used in almost every aspect of future industry in the near future. The following lectures look into the hottest DL and ML products and services that are exciting the business world. First, the \u00e2\u0080\u009cJeopardy!\u00e2\u0080\u009d winning versatile IBM Watson is introduced along with its DeepQA and AdaptWatson systems that use DL technology. Then the Amazon Echo and Echo Dot products are introduced along with the Alexa cloud based DL personal assistant that uses ASR (Automated Speech Recognition) and NLU (Natural Language Understanding) technology. The next lecture focuses on LettuceBot, which is a DL system that plants lettuce seeds with automatic fertilizer and herbicide nozzles control. Then the computer vision based DL blood cells analysis diagnostic system Athelas is introduced followed by the introduction of a classical and symphonic music composing DL system named AIVA (Artificial Intelligence Virtual Artist). As the last topic of module 1, the upcoming Apple watchOS 4 and the HomePod speaker that was presented at Apple's 2017 WWDC (World Wide Developers Conference) is introduced. The second module \u00e2\u0080\u009cBusiness with Deep Learning & Machine Learning\u00e2\u0080\u009d first focuses on various business considerations based on changes to come due to DL (Deep Learning) and ML (Machine Learning) technology in the lecture \u00e2\u0080\u009cBusiness Considerations in the Machine Learning Era.\u00e2\u0080\u009d In the following lecture \u00e2\u0080\u009cBusiness Strategy with Machine Learning & Deep Learning\u00e2\u0080\u009d explains the changes that are needed to be more successful in business, and provides an example of business strategy modeling based on the three stages of preparation, business modeling, and model rechecking & adaptation. The next lecture \u00e2\u0080\u009cWhy is Deep Learning Popular Now?\u00e2\u0080\u009d explains the changes in recent technology and support systems that enable the DL systems to perform with amazing speed, accuracy, and reliability. The last lecture \u00e2\u0080\u009cCharacteristics of Businesses with DL & ML\u00e2\u0080\u009d first explains DL and ML based business characteristics based on data types, followed by DL & ML deployment options, the competitive landscape, and future opportunities are also introduced. The third module \u00e2\u0080\u009cDeep Learning Computing Systems & Software\u00e2\u0080\u009d focuses on the most significant DL (Deep Learning) and ML (Machine Learning) systems and software. Except for the NVIDIA DGX-1, the introduced DL systems and software in this module are not for sale, and therefore, may not seem to be important for business at first glance. But in reality, the companies that created these systems and software are indeed the true leaders of the future DL and ML business era. Therefore, this module introduces the true state-of-the-art level of DL and ML technology. The first lecture introduces the most popular DL open source software TensorFlow, CNTK (Cognitive Toolkit), Keras, Caffe, Theano, and their characteristics. Due to their popularly, strong influence, and diverse capabilities, the following lectures introduce the details of Google TensorFlow and Microsoft CNTK. Next, NVIDIA\u00e2\u0080\u0099s supercomputer DGX-1, that has fully integrated customized DL hardware and software, is introduced. In the following lectures, the most interesting competition of human versus machine is introduced in the Google AlphaGo lecture, and in the ILSVRC (ImageNet Large Scale Visual Recognition Challenge) lecture, the results of competition between cutting edge DL systems is introduced and the winning performance for each year is compared. The module \u00e2\u0080\u009cBasics of Deep Learning Neural Networks\u00e2\u0080\u009d first focuses on explaining the technical differences of AI (Artificial Intelligence), ML (Machine Learning), and DL (Deep Learning) in the first lecture titled \u00e2\u0080\u009cWhat is DL (Deep Learning) and ML (Machine Learning).\u00e2\u0080\u009d In addition, the characteristics of CPUs (Central Processing Units) and GPUs (Graphics Processing Units) used in DL as well as the representative computer performance units of FLOPS (FLoating-Point Operations Per Second) and IPS (Instructions Per Second) are introduced. Next, in the NN (Neural Network) lecture, the biological neuron (nerve cell) and its signal transfer is introduced followed by an ANN (Artificial Neural Network) model of a neuron based on a threshold logic unit and soft output activation functions is introduced. Then the extended NN technologies that uses MLP (Multi-Layer Perceptron), SoftMax, and AutoEncoder are explained. In the last lecture of the module, NN learning based on backpropagation is introduced along with the learning method types, which include supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. The module \u00e2\u0080\u009cDeep Learning with CNN & RNN\u00e2\u0080\u009d focuses on CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) technology that enable DL (Deep Learning). First the lectures introduce how CNNs used in image/video recognition, recommender systems, natural language processing, and games (like Chess and Go) are made possible through processing in the convolutional layer and feature maps. The lecture also introduces how CNNs use subsampling (pooling), LCN (Local Contrast Normalization), dropout, ensemble, and bagging technology to become more efficient, reliable, robust, and accurate. Next, the lectures introduce how DL with RNN is used in speech recognition (as in Apple's Siri, Google\u00e2\u0080\u0099s Voice Search, and Samsung's S Voice), handwriting recognition, sequence data analysis, and program code generation. Then the details of RNN technologies are introduced, which include S2S (Sequence to Sequence) learning, forward RNN, backward RNN, representation techniques, context based projection, and representation with attention. As the last part of the module, the early model of RNN, which is the FRNN (Fully Recurrent NN), and the currently popular RNN model LSTM (Long Short-Term Memory) is introduced. The module \u00e2\u0080\u009cDeep Learning Project with TensorFlow Playground\u00e2\u0080\u009d focuses on four NN (Neural Network) design projects, where experience on designing DL (Deep Learning) NNs can be gained using a fun and powerful application called the TensorFlow Playground. The lectures first teach how to use the TensorFlow Playground, which is followed by guidance on three projects so you can easily buildup experience on using the TensorFlow Playground system. Then in Project 4 a \u00e2\u0080\u009cDL NN Design Challenge\u00e2\u0080\u009d is given, where you will need to make the NN \u00e2\u0080\u009cDeeper\u00e2\u0080\u009d by adding hidden layers and neurons to satisfy the classification objectives. The knowledge you obtained in the lecture of Modules 1~5 will be used in these projects.", "Improving health and healthcare institutions requires understanding of data and creation of interventions at the many levels at which health IT interact and affect the institution. These levels range from the external \u00e2\u0080\u009cworld\u00e2\u0080\u009d in which the institution operates down to the specific technologies. Data scientists find that, when they aim at implementing their models in practice, it is the \u00e2\u0080\u009csocio\u00e2\u0080\u009d components that are both novel to them and mission critical to success. At the end of this course, students will be able to make a quick assessment of a health informatics problem\u00e2\u0080\u0094or a proposed solution\u00e2\u0080\u0094and to determine what is missing and what more needs to be learned.\n\nWho Is This Class For? \nPhysicians, nurses, pharmacists, social workers, and other allied health professionals interested in expanding their understanding of digital health, big data, health information systems, and the unintended consequences of disruptive innovation in the healthcare system. The course is also aimed at those with technical, engineering, or analytics backgrounds who want to understand the nuances of those topics when it comes to healthcare. Overview of Health Informatics World, Organization, Roles, Function Workflow, Information System, Modules Data, Information, Knowledge, Technology In this module, you will be given an introduction to the course and its foundational concepts. After providing examples of health IT in the contexts of patients, providers, and populations\u00e2\u0080\u0094the three contexts we always return to\u00e2\u0080\u0094we articulate the drivers that motivate developments in health IT and informatics. We then provide the core definitions of key terms (like \u00e2\u0080\u009chealth IT\u00e2\u0080\u009d and \u00e2\u0080\u009cinformatics\u00e2\u0080\u009d) and introduce the core framework for your work in this course, the Informatics Stack.  In this module, we start our journey down the Stack to explain the world of informatics and health IT. We explore the top four levels of the Stack (World, Organization, Roles, and Functions), and then proceed to discuss new US medical-care (\u00e2\u0080\u009cWorld\u00e2\u0080\u009d) policies that drive health care Organizations to change practice, so they can accomplish their core Functions. We also discuss the history of health IT in relation to such policies in the past. We begin a course-long discussion of interoperability (which occurs at each level of the Stack), and privacy/confidentiality/security. We end with an explication of methods used to Evaluate whether an IT project has achieved the Organizational goals set for that project.  In this module, we continue the journey, starting with the role of needs, requirements, and specifications. We then turn to how workflow issues are turned into requirements and how information systems, built to satisfy those requirements, are assembled. We close with the cautionary notes of how poorly built systems harm the very workflows they were designed to improve. \n\n  This module concludes our journey with discussions of Data, Information, Knowledge, and Technology. Regarding data, we discuss their sources and types and provide examples.  We go on to explain differences between information and data, and between knowledge and information. Standards are most important at this level, and we discuss the exchange of text and imaging data. Regarding technology, we use the Hype Cycle as a way for you to keep track of what new technologies are doing what and when. We close with a framework for thinking about careers in health IT and informatics.", "An increasing volume of data is becoming available in biomedicine and healthcare, from genomic data, to electronic patient records and data collected by wearable devices. Recent advances in data science are transforming the life sciences, leading to precision medicine and stratified healthcare. \n\nIn this course, you will learn about some of the different types of data and computational methods involved in stratified healthcare and precision medicine.  You will have a hands-on experience of working with such data.  And you will learn from leaders in the field about successful case studies. \n\nTopics include: (i) Sequence Processing, (ii) Image Analysis, (iii) Network Modelling, (iv) Probabilistic Modelling, (v) Machine Learning, (vi) Natural Language Processing, (vii) Process Modelling and (viii) Graph Data.\n\nWatch the course promo video here: http://edin.ac/2pn350P Welcome to the Course WELCOME TO WEEK 2 WELCOME TO WEEK 3 WELCOME TO WEEK 4 WELCOME TO WEEK 5 Join us this week to find out how the course works and to try your hand at programming in Python! This week you will be introduced to Sequence Processing and Medical Image Analysis. Explore the course materials to find out about recent advances in these areas and how they contribute to Precision Medicine! This week you will learn about Probabilistic and Network Modelling, and how they are applied to biomedicine. You will also be introduced to Machine Learning and explore the opportunities it brings to the medical field. This week you will discover how clinical notes and other free-form text can be analysed with the use of Natural Language Processing techniques. You will also find out how Process Modelling can help us understand, stratify and improve healthcare processes. In this final week of the course you will learn how the Graph Data model allows for effective linkage of different data in the life sciences. You will also explore societal, legal and ethical implications of precision medicine and stratified healthcare.", "In this course you will learn how to create models for decision making. We will start with cluster analysis, a technique for data reduction that is very useful in market segmentation. You will then learn the basics of Monte Carlo simulation that will help you model the uncertainty that is prevalent in many business decisions. A key element of decision making is to identify the best course of action. Since businesses problems often have too many alternative solutions, you will learn how optimization can help you identify the best option. What is really exciting about this course is that you won\u00e2\u0080\u0099t need to know a computer language or advanced statistics to learn about these predictive and prescriptive analytic models. The Analytic Solver Platform and basic knowledge of Excel is all you\u00e2\u0080\u0099ll need. Learners participating in assignments will be able to get free access to the Analytic Solver Platform. Data Exploration and Reduction \u00e2\u0080\u0094 Cluster Analysis Dealing with Uncertainty and Analyzing Risk Identifying the Best Options \u00e2\u0080\u0094 Optimization Decision Analytics   At the end of this module students should be able to: 1. Develop a spreadsheet model for an optimization problem 2. Use Excel to solve optimization models 3. Interpret solutions and conduct what-if analysis At the end of this module students should be able to: 1. Given a business situation, apply an appropriate technique to identify the best solution alternatives 2. Formulate and solve models for business problems that requires yes/no decisions and logical constraints 3. Create models that mix techniques and tools such as simulation and optimizationAnalyze and interpret results to make informed decisions", "This course will continue the introduction to Python programming that started with Python Programming Essentials and Python Data Representations.  We'll learn about reading, storing, and processing tabular data, which are common tasks.  We will also teach you about CSV files and Python's support for reading and writing them.  CSV files are a generic, plain text file format that allows you to exchange tabular data between different programs. These concepts and skills will help you to further extend your Python programming knowledge and allow you to process more complex data.\n\nBy the end of the course, you will be comfortable working with tabular data in Python. This will extend your Python programming expertise, enabling you to write a wider range of scripts using Python.\n\nThis course uses Python 3.  While most Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This course uses basic desktop Python development environments, allowing you to run Python programs directly on your computer. Dictionaries Tabular Data and Nested Data Structures Tabular Data and CSV Files Organizing Data This module will teach you about Python's dictionary data type and its capabilities.  Dictionaries are used to map keys to values within programs. This module will teach you about storing tabular data within Python programs using lists and dictionaries. This module will teach you the basics of CSV files and how to read them from Python programs. We will discuss the use of Python's csv module to help you access tabular data in CSV files. This module will teach you how to sort data in Python. You will organize and analyze tabular data.", "This is the third course of the Advanced Machine Learning on GCP specialization. In this course,\nWe will take a look at different strategies for building an image classifier using convolutional neural networks. We'll improve the model's accuracy with augmentation, feature extraction, and fine-tuning hyperparameters while trying to avoid overfitting our data. We will also look at practical issues that arise, for example, when you don\u00e2\u0080\u0099t have enough data and how to incorporate the latest research findings into our models.\n\nYou will get hands-on practice building and optimizing your own image classification models on a variety of public datasets in the labs we\u00e2\u0080\u0099ll work on together.  \n\nPrerequisites: Basic SQL, familiarity with Python and TensorFlow\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Welcome to Image Understanding with TensorFlow on GCP Linear and DNN Models Convolutional Neural Networks (CNNs) Dealing with Data Scarcity Going Deeper Faster Pre-built ML Models for Image Classification Summary In this introductory module you will learn about the rapid growth in high-resolution image data available and the types of applications that it can be applied to. We\u00e2\u0080\u0099ll also cover image data as inputs to your model. \n\n\n We\u00e2\u0080\u0099ll start with a brief introduction where we\u00e2\u0080\u0099ll cover the image dataset you will be using for part of this course. Then we\u00e2\u0080\u0099ll tackle an image classification problem with a linear model in TensorFlow. After that we\u00e2\u0080\u0099ll move onto tackling the same problem using a Deep Neural Network. Lastly, we\u00e2\u0080\u0099ll close with a discussion and application of dropout which is a regularization technique for neural networks to help prevent them from memorizing our training dataset. \n This module will introduce Convolutional Neural Networks or CNNs for short, and get you started with implementing CNNs using TensorFlow. Since 2012, CNN based systems achieved unparalleled performance on tasks like image recognition and even at playing the ancient board game of Go against the top human champions. In this module, we\u00e2\u0080\u0099ll focus on data scarcity, what it is, why it\u00e2\u0080\u0099s important, and, before moving onto building ML models, what you need to do about it. In this module, you will learn how to train deeper, more accurate networks and do such training faster.You will learn about common problems that arise when training deeper networks, and how researchers have been able to address these issues. Welcome to the last module of the Image Classification course. Now that you have build your own image classifiers using linear, DNN, and CNN models with TensorFlow, it\u00e2\u0080\u0099s time to experiment with pre-built image models. In most cases, you will want to try these before investing your time in developing  custom TensorFlow code for a model. In this final module, we will review the core concepts covered in this image classification course. You will recall creating classifiers with linear models, DNNs, DNNs with Dropout,  Convolutional Neural Networks (CNNs), and lastly with pre-built models like the Cloud Vision API and AutoML Vision. ", "Welcome to the second course in the Data Analytics for Business specialization! \n\nThis course will introduce you to some of the most widely used predictive modeling techniques and their core principles. By taking this course, you will form a solid foundation of predictive analytics, which refers to tools and techniques for building statistical or machine learning models to make predictions based on data. You will learn how to carry out exploratory data analysis to gain insights and prepare data for predictive modeling, an essential skill valued in the business. \n\nYou\u00e2\u0080\u0099ll also learn how to summarize and visualize datasets using plots so that you can present your results in a compelling and meaningful way. We will use a practical predictive modeling software, XLMiner, which is a popular Excel plug-in. This course is designed for anyone who is interested in using data to gain insights and make better business decisions. The techniques discussed are applied in all functional areas within business organizations including accounting, finance, human resource management, marketing, operations, and strategic planning. \n\nThe expected prerequisites for this course include a prior working knowledge of Excel, introductory level algebra, and basic statistics.  Exploratory Data Analysis and Visualizations Predicting a Continuous Variable Predicting a Binary Outcome Trees and Other Predictive Models At the end of this module students will be able to: 1. Carry out exploratory data analysis to gain insights and prepare data for predictive modeling 2. Summarize and visualize datasets using appropriate tools 3. Identify modeling techniques for prediction of continuous and discrete outcomes. 4. Explore datasets using Excel 5. Explain and perform several common data preprocessing steps         6. Choose appropriate graphs to explore and display datasets   This module introduces regression techniques to predict the value of continuous variables. Some fundamental concepts of predictive modeling are covered, including cross-validation, model selection, and overfitting. You will also learn how to build predictive models using the software tool XLMiner. This module introduces logistic regression models to predict the value of binary variables. Unlike continuous variables, a binary variable can only take two different values and predicting its value is commonly called classification. Several important concepts regarding classification are discussed, including cross validation and confusion matrix, cost sensitive classification, and ROC curves. You will also learn how to build classification models using the software tool XLMiner. This module introduces more advanced predictive models, including trees and neural networks. Both trees and neural networks can be used to predict continuous or binary variables. You will also learn how to build trees and neural networks using the software tool XLMiner.", "The capstone project class will allow students to create a usable/public data product that can be used to show your skills to potential employers. Projects will be drawn from real-world problems and will be conducted with industry, government, and academic partners. Overview, Understanding the Problem, and Getting the Data Exploratory Data Analysis and Modeling Prediction Model Creative Exploration Data Product  Slide Deck Final Project Submission and Evaluation  This week, we introduce the project so you can get a clear grip on the problem at hand and begin working with the dataset. This week, we move on to the next tasks, exploratory data analysis and modeling. You'll also submit your milestone report and review submissions from your classmates. This week, you'll build and evaluate your prediction model. The goal is to make your model efficient and accurate.  This week's goal is to improve the predictive accuracy while reducing computational runtime and model complexity. This week, you'll work on developing the first component of your final project, your data product.  This week, you'll work on developing the second component of your final project, a slide deck to accompany your data product.  This week, you'll submit your final project and review the work of your classmates.", "Regression Analysis is perhaps the single most important Business Statistics tool used in the industry. Regression is the engine behind a multitude of data analytics applications used for many forms of forecasting and prediction.  \nThis is the fourth course in the specialization, \"Business Statistics and Analysis\". The course  introduces you to the very important tool known as Linear Regression. You will learn to apply various procedures such as dummy variable regressions, transforming variables, and interaction effects. All these are introduced and explained using easy to understand examples in Microsoft Excel.\nThe focus of the course is on understanding and application, rather than detailed mathematical derivations.\nNote: This course uses the \u00e2\u0080\u0098Data Analysis\u00e2\u0080\u0099 tool box which is standard with the Windows version of Microsoft Excel. It is also standard with the 2016 or later Mac version of Excel. However, it is not standard with earlier versions of Excel for Mac. \n\n\nWEEK 1\nModule 1: Regression Analysis: An Introduction\nIn this module you will get introduced to the Linear Regression Model. We will build a regression model and estimate it using Excel. We will use the estimated model to infer relationships between various variables and use the model to make predictions. The module also introduces the notion of errors, residuals and R-square in a regression model.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tIntroducing the Linear Regression\n\u00e2\u0080\u00a2\tBuilding a Regression Model and estimating it using Excel\n\u00e2\u0080\u00a2\tMaking inferences using the estimated model\n\u00e2\u0080\u00a2\tUsing the Regression model to make predictions\n\u00e2\u0080\u00a2\tErrors, Residuals and R-square\n \n\nWEEK 2\nModule 2: Regression Analysis: Hypothesis Testing and Goodness of Fit\nThis module presents different hypothesis tests you could do using the Regression output. These tests are an important part of inference and the module introduces them using Excel based examples. The p-values are introduced along with goodness of fit measures R-square and the adjusted R-square. Towards the end of module we introduce the \u00e2\u0080\u0098Dummy variable regression\u00e2\u0080\u0099 which is used to incorporate categorical variables in a regression. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tHypothesis testing in a Linear Regression\n\u00e2\u0080\u00a2\t\u00e2\u0080\u0098Goodness of Fit\u00e2\u0080\u0099 measures (R-square, adjusted R-square)\n\u00e2\u0080\u00a2\tDummy variable Regression (using Categorical variables in a Regression)\n \n\nWEEK 3\nModule 3: Regression Analysis: Dummy Variables, Multicollinearity\nThis module continues with the application of Dummy variable Regression. You get to understand the interpretation of Regression output in the presence of categorical variables. Examples are worked out to re-inforce various concepts introduced. The module also explains what is Multicollinearity and how to deal with it. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tDummy variable Regression (using Categorical variables in a Regression)\n\u00e2\u0080\u00a2\tInterpretation of coefficients and p-values in the presence of Dummy variables\n\u00e2\u0080\u00a2\tMulticollinearity in Regression Models\n \n\nWEEK 4\nModule 4: Regression Analysis: Various Extensions\nThe module extends your understanding of the Linear Regression, introducing techniques such as mean-centering of variables and building confidence bounds for predictions using the Regression model. A powerful regression extension known as \u00e2\u0080\u0098Interaction variables\u00e2\u0080\u0099 is introduced and explained using examples. We also study the transformation of variables in a regression and in that context introduce the log-log and the semi-log regression models. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tMean centering of variables in a Regression model\n\u00e2\u0080\u00a2\tBuilding confidence bounds for predictions using a Regression model\n\u00e2\u0080\u00a2\tInteraction effects in a Regression\n\u00e2\u0080\u00a2\tTransformation of variables\n\u00e2\u0080\u00a2\tThe log-log and semi-log regression models Regression Analysis: An Introduction Regression Analysis: Hypothesis Testing and Goodness of Fit Regression Analysis: Dummy Variables, Multicollinearity Regression Analysis: Various Extensions    ", "This is the second course in the Data to Insights specialization. Here we will cover how to ingest new external datasets into BigQuery and  visualize them with Google Data Studio. We will also cover intermediate SQL concepts like multi-table JOINs and UNIONs which will allow you to analyze data across multiple data sources.\n\nNote: Even if you have a background in SQL, there are BigQuery specifics (like handling query cache and table wildcards) that may be new to you.\n\n>>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Storing and Exporting Data Ingesting New Datasets into Google BigQuery Joining and Merging Datasets Data Visualization End of Course Recap  Overview of what you will learn in this course Create new permanent and temporary tables from your query results Load and create new datasets inside BigQuery Understand the differences between SQL JOINs and UNIONs and when to use each Compare data visualizations and learn how to use Google Data Studio ", "An introduction to the statistics behind the most popular genomic data science projects. This is the sixth course in the Genomic Big Data Science Specialization from Johns Hopkins University. Module 1 Module 2 Module 3 Module 4 This course is structured to hit the key conceptual ideas of normalization, exploratory analysis, linear modeling, testing, and multiple testing that arise over and over in genomic studies.  This week we will cover preprocessing, linear modeling, and batch effects. This week we will cover modeling non-continuous outcomes (like binary or count data), hypothesis testing, and multiple hypothesis testing. In this week we will cover a lot of the general pipelines people use to analyze specific data types like RNA-seq, GWAS, ChIP-Seq, and DNA Methylation studies. ", "Epidemiological research is ubiquitous. Even if you don\u00e2\u0080\u0099t realise it, you come across epidemiological studies and the impact of their findings every single day. You have probably heard that obesity is increasing in high income countries or that malaria is killing millions of people in low income countries. It is common knowledge that smoking causes cancer and that physical activity is protective against heart disease. These facts may seem obvious today, but it took decades of epidemiological research to produce the necessary evidence. In this course, you will learn the fundamental tools of epidemiology which are essential to conduct such studies, starting with the measures used to describe the frequency of a disease or health-related condition. You will also learn how to quantify the strength of an association and discuss the distinction between association and causation. In the second half of the course, you will use this knowledge to describe different strategies for prevention, identify strengths and weaknesses of diagnostic tests and consider when a screening programme is appropriate. Measures of disease frequency Measures of association Attributable risk and strategies for prevention Disease detection and screening One of the main purposes of epidemiology is to describe the frequency of diseases or other conditions that are important for the health of populations. Depending on the circumstances, there are different types of measures that you can use to do this. In this module, you will learn to calculate measures such as the prevalence, odds, cumulative incidence and incidence rate. We will highlight what they have in common as well as their differences. By the end of the module, you will be able to select and calculate the appropriate measure of frequency in a variety of contexts. This module starts by introducing the distinction between association and causation, which is critical not only for epidemiology, but for research in general. Subsequently, you will learn all the main measures epidemiologists use to quantify association; mainly risk and rate differences and risk, rate and odds ratios. Over the course of this module, you will develop the skills to calculate and interpret measures of frequency. This is not enough by itself though, so you will also learn to select the most appropriate measure depending on the research question and the availability of data. Published studies often report the magnitude of the association they investigate, which is clearly important when trying to identify causal links. Sometimes though, what we are interested in is the impact of a factor or of a disease on the population as a whole. This is when the concepts of attributable risk and of population attributable risk come in handy. These measures quantify the population impact of a health-related factor and therefore are particularly useful for health policy. Equipped with this knowledge, you will then explore the two main approaches to disease prevention: the high-risk and the population approach. Diagnostic tests are used all the time to determine whether an individual is sick or not. However, these tests are far from perfect. Quantifying their imperfection allows us to understand their limitations and interpret their results. In this module, you will learn to calculate and interpret the metrics used to do this, including sensitivity, specificity, positive and negative predictive values. Using these metrics, you will subsequently learn to evaluate whether a screening programme can be effective or not considering its methodological and practical aspects.", "Whether being used to customize advertising to millions of website visitors or streamline inventory ordering at a small restaurant, data is becoming more integral to success. Too often, we\u00e2\u0080\u0099re not sure how use data to find answers to the questions that will make us more successful in what we do. In this course, you will discover what data is and think about what questions you have that can be answered by the data \u00e2\u0080\u0093 even if you\u00e2\u0080\u0099ve never thought about data before. Based on existing data, you will learn to develop a research question, describe the variables and their relationships, calculate basic statistics, and present your results clearly. By the end of the course, you will be able to use powerful data analysis tools \u00e2\u0080\u0093 either SAS or Python \u00e2\u0080\u0093 to manage and visualize your data, including how to deal with missing data, variable groups, and graphs. Throughout the course, you will share your progress with others to gain valuable feedback, while also learning how your peers use data to answer their own questions. Selecting a research question Writing your first program - SAS or Python Managing Data Visualizing Data Supplemental Materials (All Weeks) We would like to welcome you to Wesleyan University's Data Analysis and Interpretation Specialization. In this session, we will discuss the basics of data analysis. Your task will be to select a data set that you would like to work with and to review available code books that help you develop your own research question. You will also set up a Tumblr blog that will allow you to reflect on these experiences, submit assignments and share your work with others throughout the course. First, you may want to check out the welcome video In this session, we will discuss how to write a basic program that allows you to load a data set and examine frequency distributions. Your task will be to write a program that helps you to explore the variables you have selected for your own research question. You may choose either Python or SAS. Both are made freely available, and we have created a helpful guide to support you in making the decision. Once you have selected your platform, just follow the instructions in the appropriate \"GETTING STARTED....\" file, and then check out this week's video lessons aimed at helping you write and run your first program. You need only view the lessons for one of the statistical platforms (SAS or Python).\n In this session, we will help you to make and implement even more decisions with data. Statisticians often call this task 'data management', while computer scientists like the term 'data munging'. Whatever you call it, it is a vital and ongoing process when working with data. Your task will be to write a program that manages the variables you have selected for your own research question.  In this session we will discuss descriptive statistics and get you visualizing your newly data managed variables individually and as graphs showing the relationships between them.   ", "In most areas of health, data is being used to make important decisions.  As a health population manager, you will have the opportunity to use data to answer interesting questions. In this course, we will discuss data analysis from a responsible perspective, which will help you to extract useful information from data and enlarge your knowledge about specific aspects of interest of the population. \n\nFirst, you will learn how to obtain, safely gather, clean and explore data. Then, we will discuss that because data are usually obtained from a sample of a limited number of individuals, statistical methods are needed to make claims about the whole population of interest. You will discover how statistical inference, hypothesis testing and regression techniques will help you to make the connection between samples and populations.\n\nA final important aspect is interpreting and reporting. How can we transform information into knowledge? How can we separate trustworthy information from noise? In the last part of the course, we will cover the critical assessment of the results, and we will discuss challenges and dangers of data analysis in the era of big data and massive amounts of information.\u00c2\u00a0 \n\nIn this course, we will emphasize the concepts and we will also teach you how to effectively perform your analysis using R. You do not need to install R on your computer to follow the course, you will be able to access R and all the example data sets within the Coursera environment.  \n\nThis course will become part of the to-be-developed  Leiden University master program Population Health Management. If you wish to find out more about this program see the last reading of this Course! Welcome to Responsible Data Analysis From Individuals to Data From data to information I: statistical inference  From data to information II: regression techniques From information to knowledge Welcome to the course Responsible Data Analysis! You\u00e2\u0080\u0099re joining thousands of learners currently enrolled in the course. I'm excited to have you in class and look forward to your contributions to the learning community.To begin, I recommend taking a few minutes to explore the course site. Review the material we\u00e2\u0080\u0099ll cover each week, and preview the assignments you\u00e2\u0080\u0099ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class. If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center. Good luck as you get started, and I hope you enjoy the course! In this module, we will discuss how to obtain, store, clean and explore  the data necessary to answer your research question. First, we will see how to collect data of good quality. Second, we will see how to address privacy and security when dealing with personal data. Then, we will see how to first describe and summarize your data. Finally, we will discuss the principles of initial data analysis. In this module,  we will see how to  deal with  data obtained from a limited number of individuals. You will discover how statistical inference can make the connection between samples and populations. First, we will discuss  important concepts such as random variation, sampling distribution and standard error.  Second, we will discuss the principles of hypothesis testing. Then, we will review the moist commonly used statistical tests. Finally, we will discuss how to decide how large your study sample should be. In this module, we will discuss the basic principles of regression modeling, a collection of powerful tools to analyze complex data. We will start simple, and increase the complexity of the models step by step. We will start with linear regression, used with continuous outcomes. Then we will continue with logistic regression, which can be used to model binary variables, and finally we will consider regression with  time to event outcomes. \n In this module , we will cover the critical assessment of data analysis results, and we will discuss challenges and dangers of data analysis in the era of big data and massive amounts of information.  First,  we will see how bad data analysis practice can dramatically impact scientific progress. Second, we will address the hot topic of how to report uncertainty in scientific findings. This has been object of big controversy in the scientific literature. We invited two experts to present their different points of view.  Then, we will discuss different forms of bias.  Finally, we will give you tips and tricks to write  a perfect statistical plan. \n\nSpecial about this week is that we are working with a discussion group about some difficult social situations you might encounter when doing your own research. Most of us who have worked in research might have been through those, and if you feel comfortable, please do share your thoughts about what you think is appropriate, and follow the threads as the rest of us reply!  ", "In this project-based course, you will follow your own interests to create a portfolio worthy single-frame viz or multi-frame data story that will be shared on Tableau Public. You will use all the skills taught in this Specialization to complete this project step-by-step, with guidance from your instructors along the way. You will first create a project proposal to identify your goals for the project, including the question you wish to answer or explore with data. You will then find data that will provide the information you are seeking. You will then import that data into Tableau and  prepare it for analysis. Next you will create a dashboard that will allow you to explore the data in depth and identify meaningful insights. You will then give structure to your data story by writing the story arc in narrative form. Finally, you will consult your design checklist to craft the final viz or data story in Tableau. This is your opportunity to show the world what you\u00e2\u0080\u0099re capable of - so think big, and have confidence in your skills! Getting Started and Milestone 1: Develop a Project Proposal Milestone 2: Importing and Prepping the Data Milestone 3: Exploratory Analysis Milestone 3: Exploratory Analysis and Dashboard Submission Milestone 4: Storytelling and Storyboarding Milestone 5: Final Presentation In this first milestone, you will write a project proposal that will capture the \u00e2\u0080\u009cwho, what, why and how\u00e2\u0080\u009d of your project plus any challenges that you foresee along the way. Your proposal will include: a specific business case or personal objective, any intended outcomes, a description of the needs of the intended audience, a description of the dataset to be used, and any foreseeable challenges. In milestone two, you will acquire the dataset that supports your project proposal, import it into Tableau, and prepare the data for analysis. In this milestone, you will use the skills that you have learned in the specialization to perform exploratory analysis of your data. You will identify key metrics in the data and create KPIs, and you will use those KPIs to create dashboards that allow for comparative views and \u00e2\u0080\u009cbrushing and linking.\u00e2\u0080\u009d This will allow us to begin to think about the proper context of developing an explanatory analysis that will form the basis for the remaining milestones.  Be sure your visualizations demonstrate the visual and cognitive design principles learned throughout the Specialization, and make use of advanced features like hierarchies, actions, filters and parameters.  In this module, you will continue to work through Milestone 3, your exploratory analysis and dashboard creation as outlined in the third week. You will then submit your deliverables for peer review. In this milestone, you will take a short but essential break from the data visualization software and begin to give structure to your data stories. You will define the basic story arc of your data story, or draft a narrative description of what your data visualization communicates.  You have started that process in the previous milestones, but now we will start assembling our story using Story Points. In this final milestone, you will apply design elements from our design checklist. You have met the goals of the stakeholder\u00e2\u0080\u0099s and will complete your design to meet the needs of the audience. You will apply the cognitive and visual design concepts learned throughout this Specialization and create a visualization or data story that leaves a lasting impression with your audience.", "Have you ever had the perfect data science experience? The data pull went perfectly. There were no merging errors or missing data. Hypotheses were clearly defined prior to analyses. Randomization was performed for the treatment of interest. The analytic plan was outlined prior to analysis and followed exactly. The conclusions were clear and actionable decisions were obvious. Has that every happened to you? Of course not. Data analysis in real life is messy. How does one manage a team facing real data analyses? In this one-week course, we contrast the ideal with what happens in real life. By contrasting the ideal, you will learn key concepts that will help you manage real life analyses. \n\nThis is a focused course designed to rapidly get you up to speed on doing data science in real life. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the technical information aside so that you can focus on managing your team and moving it forward.\n\nAfter completing this course you will know how to:\n\n1, Describe the \u00e2\u0080\u009cperfect\u00e2\u0080\u009d data science experience\n2. Identify strengths and weaknesses in experimental designs\n3. Describe possible pitfalls when pulling / assembling data and learn solutions for managing data pulls.\n4. Challenge statistical modeling assumptions and drive feedback to data analysts\n5. Describe common pitfalls in communicating data analyses\n6. Get a glimpse into a day in the life of a data analysis manager.\n\nThe course will be taught at a conceptual level for active managers of data scientists and statisticians.  Some key concepts being discussed include:\n1. Experimental design, randomization, A/B testing\n2. Causal inference, counterfactuals, \n3. Strategies for managing data quality.\n4. Bias and confounding\n5. Contrasting machine learning versus classical statistical inference\n\nCourse promo:\nhttps://www.youtube.com/watch?v=9BIYmw5wnBI\n\nCourse cover image by Jonathan Gross. Creative Commons BY-ND https://flic.kr/p/q1vudb Introduction, the perfect data science experience This course is one module, intended to be taken in one week. Please do the course roughly in the order presented. Each lecture has reading and videos. Except for the introductory lecture, every lecture has a 5 question quiz; get 4 out of 5 or better on the quiz.", "Confidence intervals and Hypothesis tests are very important tools in the Business Statistics toolbox. A mastery over these topics will help enhance your business decision making and allow you to understand and measure the extent of \u00e2\u0080\u0098risk\u00e2\u0080\u0099 or \u00e2\u0080\u0098uncertainty\u00e2\u0080\u0099 in various business processes. \nThis is the third course in the specialization \"Business Statistics and Analysis\" and the course  advances your knowledge about Business Statistics by introducing you to Confidence Intervals and Hypothesis Testing. We first conceptually understand these tools and their business application. We then introduce various calculations to constructing confidence intervals and to conduct different kinds of Hypothesis Tests. These are done by easy to understand applications.\n\nTo successfully complete course assignments, students must have access to a Windows version of Microsoft Excel 2010 or later. Please note that earlier versions of Microsoft Excel (2007 and earlier) will not be compatible to some Excel functions covered in this course. \n\n\nWEEK 1\nModule 1: Confidence Interval - Introduction\nIn this module you will get to conceptually understand what a confidence interval is and how is its constructed. We will introduce the various building blocks for the confidence interval such as the t-distribution, the t-statistic, the z-statistic and their various excel formulas. We will then use these building blocks to construct confidence intervals.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tIntroducing the t-distribution, the T.DIST and T.INV excel functions\n\u00e2\u0080\u00a2\tConceptual understanding of a Confidence Interval\n\u00e2\u0080\u00a2\tThe z-statistic and the t-statistic\n\u00e2\u0080\u00a2\tConstructing a Confidence Interval using z-statistic and t-statistic \n\n\nWEEK 2\nModule 2: Confidence Interval - Applications\nThis module presents various business applications of the confidence interval including an application where we use the confidence interval to calculate an appropriate sample size. We also introduce with an application, the confidence interval for a population proportion. Towards the close of module we start introducing the concept of Hypothesis Testing.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tApplications of Confidence Interval\n\u00e2\u0080\u00a2\tConfidence Interval for a Population Proportion\n\u00e2\u0080\u00a2\tSample Size Calculation\n\u00e2\u0080\u00a2\tHypothesis Testing, An Introduction\n\n\nWEEK 3\nModule 3: Hypothesis Testing\nThis module introduces Hypothesis Testing. You get to understand the logic behind hypothesis tests. The four steps for conducting a hypothesis test are introduced and you get to apply them for hypothesis tests for a population mean as well as population proportion. You will understand the difference between single tail hypothesis tests and two tail hypothesis tests and also the Type I and Type II errors associated with hypothesis tests and ways to reduce such errors. \n\nTopics covered include:\n\u00e2\u0080\u00a2\tThe Logic of Hypothesis Testing\n\u00e2\u0080\u00a2\tThe Four Steps for conducting a Hypothesis Test\n\u00e2\u0080\u00a2\tSingle Tail and Two Tail Hypothesis Tests\n\u00e2\u0080\u00a2\tGuidelines, Formulas and an Application of Hypothesis Test\n\u00e2\u0080\u00a2\tHypothesis Test for a Population Proportion\n\u00e2\u0080\u00a2\tType I and Type II Errors in a Hypothesis \n\n\nWEEK 4\nModule 4: Hypothesis Test - Differences in Mean\nIn this module, you'll apply Hypothesis Tests to test the difference between two different data, such hypothesis tests are called difference in means tests. We will introduce the three kinds of difference in means test and apply them to various business applications. We will also introduce the Excel dialog box to conduct such hypothesis tests.\n\nTopics covered include:\n\u00e2\u0080\u00a2\tIntroducing the Difference-In-Means Hypothesis Test\n\u00e2\u0080\u00a2\tApplications of the Difference-In-Means Hypothesis Test\n\u00e2\u0080\u00a2\tThe Equal & Unequal Variance Assumption and the Paired t-test for difference in means.\n\u00e2\u0080\u00a2\tSome more applications Confidence Interval - Introduction Confidence Interval - Applications Hypothesis Testing Hypothesis Test - Differences in Mean    ", "This course provides an analytical framework to help you evaluate key problems in a structured fashion and will equip you with tools to better manage the uncertainties that pervade and complicate business processes. Specifically, you will be introduced to statistics and how to summarize data and learn concepts of frequency, normal distribution, statistical studies, sampling, and confidence intervals.\n\nWhile you will be introduced to some of the science of what is being taught, the focus will be on applying the methodologies. This will be accomplished through the use of Excel and data sets from many different disciplines, allowing you to see the use of statistics in very diverse settings. The course will focus not only on explaining these concepts, but also understanding the meaning of the results obtained.\n\nUpon successful completion of this course, you will be able to:\n\n\u00e2\u0080\u00a2\tSummarize large data sets in graphical, tabular, and numerical forms.\n\u00e2\u0080\u00a2\tUnderstand the significance of proper sampling and why you can rely on sample information.\n\u00e2\u0080\u00a2\tUnderstand why normal distribution can be used in so many settings.\n\u00e2\u0080\u00a2\tUse sample information to infer about the population with a certain level of confidence about the accuracy of the estimations.\n\u00e2\u0080\u00a2\tUse Excel for statistical analysis.\n\nThis course is part of the iMBA offered by the University of Illinois, a flexible, fully-accredited online MBA at an incredibly competitive price. For more information, please see the Resource page in this course and onlinemba.illinois.edu. Course Orientation Module 1: Introduction and Summarizing Data Module 2: Descriptive Statistics and Probability Distributions Module 3: Sampling and Central Limit Theorem Module 4: Inference You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. Data is all around you, but what is the data telling you? The first step in making better decisions and taking action is to get a good understanding of information you have gathered. In this module we will learn about some of the tools in statistics that help us achieve this. We all have heard the phrase that a \"picture is worth a thousand words,\" but you certainly don\u00e2\u0080\u0099t want one of those to be \"what exactly am I looking at?\" So, now that you know to use \"pictures\" to summarize your data, let\u00e2\u0080\u0099s make those pictures easier to understand. You are charged with analyzing a market segment for your company. You and your team have figured out what variables you need to understand; you also have an idea what factors might be influencing these variables of interest. Now you are ready to do your analysis. But, wait! Where is the data? How do you begin to get the data? In this module we will review the means by which you can begin to produce data \u00e2\u0080\u0093 the concepts of sampling and Central Limit Theorem \u00e2\u0080\u0093 and will help you understand how to produce \"good\" sample data and why sample data will work. You have sample data and have done the analysis \u00e2\u0080\u0093 you think you can say something about the population based on your sample study.  But, do you have a sense of what are the chances of you being right or wrong?  How can you be surer? What else should you have considered? In this module, you will learn how to find the answers to these questions.", "Data analysis has replaced data acquisition as the bottleneck to evidence-based decision making --- we are drowning in it.  Extracting knowledge from large, heterogeneous, and noisy datasets requires not only powerful computing resources, but the programming abstractions to use them effectively.  The abstractions that emerged in the last decade blend ideas from parallel databases, distributed systems, and programming languages to create a new class of scalable data analytics platforms that form the foundation for data science at realistic scales.\n\nIn this course, you will learn the landscape of relevant systems, the principles on which they rely, their tradeoffs, and how to evaluate their utility against your requirements. You will learn how practical systems were derived from the frontier of research in computer science and what systems are coming on the horizon.   Cloud computing, SQL and NoSQL databases, MapReduce and the ecosystem it spawned, Spark and its contemporaries, and specialized systems for graphs and arrays will be covered.\n\nYou will also learn the history and context of data science, the skills, challenges, and methodologies the term implies, and how to structure a data science project.  At the end of this course, you will be able to:\n\nLearning Goals: \n1. Describe common patterns, challenges, and approaches associated with data science projects, and what makes them different from projects in related fields.\n2. Identify and use the programming models associated with scalable data manipulation, including relational algebra, mapreduce, and other data flow models.\n3. Use database technology adapted for large-scale analytics, including the concepts driving parallel databases, parallel query processing, and in-database analytics\n4. Evaluate key-value stores and NoSQL systems, describe their tradeoffs with comparable systems, the details of important examples in the space, and future trends.\n5. \u00e2\u0080\u009cThink\u00e2\u0080\u009d in MapReduce to effectively write algorithms for systems including Hadoop and Spark.  You will understand their limitations, design details, their relationship to databases, and their associated ecosystem of algorithms, extensions, and languages.\nwrite programs in Spark\n6. Describe the landscape of specialized Big Data systems for graphs, arrays, and streams Data Science Context and Concepts Relational Databases and the Relational Algebra MapReduce and Parallel Dataflow Programming NoSQL: Systems and Concepts Graph Analytics Understand the terminology and recurring principles associated with data science, and understand the structure of data science projects and emerging methodologies to approach them.    Why does this emerging field exist?  How does it relate to other fields?  How does this course distinguish itself?  What do data science projects look like, and how should they be approached?  What are some examples of data science projects?   Relational Databases are the workhouse of large-scale data management.  Although originally motivated by problems in enterprise operations, they have proven remarkably capable for analytics as well.  But most importantly, the principles underlying relational databases are universal in managing, manipulating, and analyzing data at scale.  Even as the landscape of large-scale data systems has expanded dramatically in the last decade, relational models and languages have remained a unifying concept.  For working with large-scale data, there is no more important programming model to learn. The MapReduce programming model (as distinct from its implementations) was proposed as a simplifying abstraction for parallel manipulation of massive datasets, and remains an important concept to know when using and evaluating modern big data platforms.   NoSQL systems are purely about scale rather than analytics, and are arguably less relevant for the practicing data scientist.  However, they occupy an important place in many practical big data platform architectures, and data scientists need to understand their limitations and strengths to use them effectively. Graph-structured data are increasingly common in data science contexts due to their ubiquity in modeling the communication between entities: people (social networks), computers (Internet communication), cities and countries (transportation networks), or corporations (financial transactions).  Learn the common algorithms for extracting information from graph data and how to scale them up. ", "This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization\u00e2\u0080\u0099s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team. Welcome to Advanced R Programming Functions Functions: Lesson Choices Functional Programming Functional Programming: Lesson Choices Debugging and Profiling Object-Oriented Programming This course covers advanced topics in R programming that are necessary for developing powerful, robust, and reusable data science tools. Topics covered include functional programming in R, robust error handling, object oriented programming, profiling and benchmarking, debugging, and proper design of functions. Upon completing this course you will be able to identify and abstract common data analysis tasks and to encapsulate them in user-facing functions. Because every data science environment encounters unique data challenges, there is always a need to develop custom software specific to your organization\u00e2\u0080\u0099s mission. You will also be able to define new data types in R and to develop a universe of functionality specific to those data types to enable cleaner execution of data science tasks and stronger reusability within a team. This module begins with control structures in R for controlling the logical flow of an R program. We then move on to functions, their role in R programming, and some guidelines for writing good functions.  Functional programming is a key aspect of R and is one of R's differentiating factors as a data analysis language. Understanding the concepts of functional programming will help you to become a better data science software developer. In addition, we cover error and exception handling in R for writing robust code.  Debugging tools are useful for analyzing your code when it exhibits unexpected behavior. We go through the various debugging tools in R and how they can be used to identify problems in code. Profiling tools allow you to see where your code spends its time and to optimize your code for maximum efficiency. Object oriented programming allows you to define custom data types or classes and a set of functions for handling that data type in a way that you define. R has a three different methods for implementing object oriented programming and we will cover them in this section.", "Introduces to the commands that you need to manage and analyze directories, files, and large sets of genomic data. This is the fourth course in the Genomic Big Data Science Specialization from Johns Hopkins University. Basic Unix Commands Week Two Week Three Week Four In this module, you will be introduced to command Line Tools for Genomic Data Science In this module, we'll be taking a look at Sequences and Genomic Features in a sequence of 10 presentations.  In this module, we'll be going over Alignment and Sequence Variation in another sequence of 8 presentations. In this module, we'll be going over Tools for Transcriptomics in a sequence of 6 presentations.", "This Model-Based Systems Engineering (MBSE) course and the Digital Thread courses featured earlier in this specialization bring together the concepts from across digital manufacturing and design, forming a vision in which the geometry of a product is just one way of describing it. MBSE is where the model resulting from the evolution of system requirements, design, analysis, verification and validation activities is the focus of design and manufacturing. Students will gain an understanding of systems engineering, the model-based approach to design and manufacturing, the Digital Twin, and a roadmap toward a model-based enterprise.\n\nStudents will be able to explain the value and expectations of systems engineering and model-based systems engineering, and the underlying motivations and opportunities represented by a model-based enterprise. They will develop the knowledge necessary to perform a baseline assessment of an organization\u00e2\u0080\u0099s potential to leverage MBSE. \n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the eighth course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Systems Engineering Model-Based Systems Engineering  Applications of Model-Based Systems Engineering Model-Based Enterprise The purpose of this module is to establish a basic understanding of Systems Engineering and the role it plays in design and manufacturing. At the end of the module, learners will be able to explain a Systems Engineering process and discuss the advantages and disadvantages of the approach. The purpose of this module is to explain Model-Based Systems Engineering (MBSE) and its applications. Upon completion, learners will be able to assess a process for MBSE opportunities and develop an argument for implementation. The learners will also be able to develop a strategy on how to implement applications of MBSE. Learners will develop an appreciation of the Model-Based Enterprise, including the benefits and barriers to  successful implementation. Learners will be able to identify potential opportunities for the adoption of a Model-Based Enterprise. The purpose of this module is to develop an understanding of the MBE Self Assessment tools. At the end of the module, learners will be able to go through a tool to analyze MBE maturity.", "Organisations all around the world are using data to predict behaviours and extract valuable real-world insights to inform decisions. Managing and analysing big data has become an essential part of modern finance, retail, marketing, social science, development and research, medicine and government.\n\nThis MOOC, designed by an academic team from Goldsmiths, University of London, will quickly introduce you to the core concepts of Data Science to prepare you for intermediate and advanced Data Science courses. It focuses on the basic mathematics, statistics and programming skills that are necessary for typical data analysis tasks. \n\nYou will consider these fundamental concepts on an example data clustering task, and you will use this example to learn basic programming skills that are necessary for mastering Data Science techniques. During the course, you will be asked to do a series of mathematical and programming exercises and a small data clustering project for a given dataset. Week 1: Foundations of Data Science: K-Means Clustering in Python Week 2: Means and Deviations in Mathematics and Python Week 3: Moving from One to Two Dimensional Data Week 4: Introducing Pandas and Using K-Means to Analyse Data Week 5: A Data Clustering Project This week we will introduce you to the course and to the team who will be guiding you through the course over the next 5 weeks. The aim of this week's material is to gently introduce you to Data Science through some real-world examples of where Data Science is used, and also by highlighting some of the main concepts involved.    ", "This course offers a rigorous mathematical survey of causal inference at the Master\u00e2\u0080\u0099s level.\n\nInferences about causation are of great importance in science, medicine, policy, and business.  This course provides an introduction to the statistical literature on causal inference that has emerged in the last 35-40 years and that has revolutionized the way in which statisticians and applied researchers in many disciplines use data to make inferences about causal relationships.  \n\nWe will study methods for collecting data to estimate causal relationships. Students will learn how to distinguish between relationships that are causal and non-causal; this is not always obvious. We shall then study and evaluate the various methods students can use \u00e2\u0080\u0094 such as matching, sub-classification on the propensity score, inverse probability of treatment weighting, and machine learning \u00e2\u0080\u0094 to estimate a variety of effects \u00e2\u0080\u0094 such as the average treatment effect and the effect of treatment on the treated. At the end, we discuss methods for evaluating some of the assumptions we have made, and we offer a look forward to the extensions we take up in the sequel to this course. MODULE 1: Key Ideas Module 2: Randomization Inference  MODULE 3: Regression Module 4: Propensity Score Module 5: Matching Module 6: Special Topics      ", "Presentaremos TensorFlow de bajo nivel y abordaremos las API y los conceptos necesarios para poder escribir modelos de aprendizaje autom\u00c3\u00a1tico distribuido. Con un modelo de TensorFlow, explicaremos c\u00c3\u00b3mo escalar de manera horizontal el entrenamiento de ese modelo y ofreceremos predicciones de alto rendimiento mediante Cloud Machine Learning Engine.\n\nObjetivos del curso:\nCrear modelos de aprendizaje autom\u00c3\u00a1tico en TensorFlow\nUsar las bibliotecas de TensorFlow para resolver problemas num\u00c3\u00a9ricos\nSolucionar problemas y depurar errores de c\u00c3\u00b3digo comunes de TensorFlow\nUsar tf_estimator para crear, entrenar y evaluar un modelo de AA\nEntrenar, implementar y llevar a producci\u00c3\u00b3n modelos de AA a gran escala con Cloud ML Engine Introducci\u00c3\u00b3n Aspectos b\u00c3\u00a1sicos de TensorFlow API de Estimator Escalamiento de modelos de TensorFlow con CMLE Resumen Comenzaremos el curso con una introducci\u00c3\u00b3n de TensorFlow, la herramienta que usaremos para escribir programas de aprendizaje autom\u00c3\u00a1tico. En el primer curso, aprendi\u00c3\u00b3 a formular problemas de negocios como problemas de aprendizaje autom\u00c3\u00a1tico. En el segundo, aprendi\u00c3\u00b3 c\u00c3\u00b3mo funciona el aprendizaje autom\u00c3\u00a1tico en la pr\u00c3\u00a1ctica y c\u00c3\u00b3mo crear conjuntos de datos para ese uso espec\u00c3\u00adfico. Ahora que ya cuenta con los datos necesarios, es hora de prepararse para escribir programas de aprendizaje autom\u00c3\u00a1tico. Le presentaremos los componentes centrales de TensorFlow y obtendr\u00c3\u00a1 experiencia pr\u00c3\u00a1ctica en la compilaci\u00c3\u00b3n de programas de aprendizaje autom\u00c3\u00a1tico. Comparar\u00c3\u00a1 y escribir\u00c3\u00a1 programas imperativos y de evaluaci\u00c3\u00b3n perezosa; trabajar\u00c3\u00a1 con gr\u00c3\u00a1ficos, sesiones y variables y, por \u00c3\u00baltimo, depurar\u00c3\u00a1 programas de TensorFlow.\n En este m\u00c3\u00b3dulo, aprender\u00c3\u00a1 sobre la API de Estimator. En esta sesi\u00c3\u00b3n, hablaremos sobre c\u00c3\u00b3mo tomar un modelo de TensorFlow y entrenarlo en la infraestructura administrada de GCP para el entrenamiento y la implementaci\u00c3\u00b3n de modelos de aprendizaje autom\u00c3\u00a1tico. En esta sesi\u00c3\u00b3n, resumimos los temas de TensorFlow que se trataron durante este curso. Repasaremos el c\u00c3\u00b3digo b\u00c3\u00a1sico de TensorFlow y la API de Estimator, y terminaremos con el escalamiento de los modelos de aprendizaje autom\u00c3\u00a1tico con Cloud\u00c2\u00a0Machine\u00c2\u00a0Learning\u00c2\u00a0Engine.", "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. \n\nThis course is the second in a sequence of three. Following the first course, which focused on representation, this course addresses the question of probabilistic inference: how a PGM can be used to answer questions. Even though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem. Inference Overview  Variable Elimination Belief Propagation Algorithms  MAP Algorithms Sampling Methods Inference in Temporal Models Inference Summary This module provides a high-level overview of the main types of inference tasks typically encountered in graphical models: conditional probability queries, and finding the most likely assignment (MAP inference). This module presents the simplest algorithm for exact inference in graphical models: variable elimination. We describe the algorithm, and analyze its complexity in terms of properties of the graph structure. This module describes an alternative view of exact inference in graphical models: that of message passing between clusters each of which encodes a factor over a subset of variables. This framework provides a basis for a variety of exact and approximate inference algorithms. We focus here on the basic framework and on its instantiation in the exact case of clique tree propagation. An optional lesson describes the loopy belief propagation (LBP) algorithm and its properties. This module describes algorithms for finding the most likely assignment for a distribution encoded as a PGM (a task known as MAP inference). We describe message passing algorithms, which are very similar to the algorithms for computing conditional probabilities, except that we need to also consider how to decode the results to construct a single assignment. In an optional module, we describe a few other algorithms that are able to use very different techniques by exploiting the combinatorial optimization nature of the MAP task. In this module, we discuss a class of algorithms that uses random sampling to provide approximate answers to conditional probability queries. Most commonly used among these is the class of Markov Chain Monte Carlo (MCMC) algorithms, which includes the simple Gibbs sampling algorithm, as well as a family of methods known as Metropolis-Hastings. In this brief lesson, we discuss some of the complexities of applying some of the exact or approximate inference algorithms that we learned earlier in this course to dynamic Bayesian networks. This module summarizes some of the topics that we covered in this course and discusses tradeoffs between different algorithms. It also includes the course final exam.", "\u00e3\u0081\u0093\u00e3\u0081\u00ae 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e9\u0080\u009f\u00e7\u00bf\u0092\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u0087\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0083\u0089 \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google Cloud Platform\u00ef\u00bc\u0088GCP\u00ef\u00bc\u0089\u00e3\u0081\u00ae\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u00a8\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0082\u0092\u00e7\u00b4\u00b9\u00e4\u00bb\u008b\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082Google Cloud Platform \u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0082\u0092\u00e7\u00b0\u00a1\u00e5\u008d\u0098\u00e3\u0081\u00ab\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e5\u00be\u008c\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e4\u00bf\u00ae\u00e4\u00ba\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e5\u008f\u0097\u00e8\u00ac\u009b\u00e8\u0080\u0085\u00e3\u0081\u00af\u00e6\u00ac\u00a1\u00e3\u0081\u00ae\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\u00e2\u0080\u00a2 Google Cloud Platform \u00e3\u0081\u00ae\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00a8\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e4\u00bf\u0082\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e4\u00b8\u00bb\u00e8\u00a6\u0081\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0083\u0080\u00e3\u0082\u00af\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e7\u009b\u00ae\u00e7\u009a\u0084\u00e3\u0081\u00a8\u00e4\u00be\u00a1\u00e5\u0080\u00a4\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 Cloud SQL \u00e3\u0081\u00a8 Cloud Dataproc \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e6\u0097\u00a2\u00e5\u00ad\u0098\u00e3\u0081\u00ae MySQL \u00e3\u0081\u00a8 Hadoop\u00e3\u0080\u0081Pig\u00e3\u0080\u0081Spark\u00e3\u0080\u0081Hive \u00e3\u0081\u00ae\u00e3\u0083\u00af\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0083\u00ad\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0082\u0092 Google Cloud Platform \u00e3\u0081\u00ab\u00e7\u00a7\u00bb\u00e8\u00a1\u008c\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 BigQuery \u00e3\u0081\u00a8 Cloud Datalab \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u00bf\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u0096\u00e3\u0081\u00aa\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e8\u00a7\u00a3\u00e6\u009e\u0090\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 Cloud SQL\u00e3\u0080\u0081Bigtable\u00e3\u0080\u0081Datastore \u00e3\u0081\u00ae\u00e3\u0081\u0084\u00e3\u0081\u009a\u00e3\u0082\u008c\u00e3\u0081\u008b\u00e3\u0082\u0092\u00e9\u0081\u00b8\u00e6\u008a\u009e\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 TensorFlow \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u008b\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u00ab \u00e3\u0083\u008d\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u00af\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0082\u0092\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 Google Cloud Platform \u00e3\u0081\u00ae\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0083\u0080\u00e3\u0082\u00af\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e9\u0081\u00b8\u00e6\u008a\u009e\u00e3\u0081\u0099\u00e3\u0082\u008b\n\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00ac\u00a1\u00e3\u0081\u00ae 1 \u00e3\u0081\u00a4\u00e4\u00bb\u00a5\u00e4\u00b8\u008a\u00e3\u0081\u00ae\u00e5\u0088\u0086\u00e9\u0087\u008e\u00e3\u0081\u00a7 1 \u00e5\u00b9\u00b4\u00e7\u00a8\u008b\u00e5\u00ba\u00a6\u00e3\u0081\u00ae\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e3\u0082\u0092\u00e5\u00af\u00be\u00e8\u00b1\u00a1\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\u00e2\u0080\u00a2 SQL \u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e8\u00a8\u0080\u00e8\u00aa\u009e\n\u00e2\u0080\u00a2 \u00e6\u008a\u00bd\u00e5\u0087\u00ba\u00e3\u0080\u0081\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0080\u0081\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0081\u00ae\u00e6\u0093\u008d\u00e4\u00bd\u009c\n\u00e2\u0080\u00a2 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\n\u00e2\u0080\u00a2 \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0081\u00af\u00e7\u00b5\u00b1\u00e8\u00a8\u0088\n\u00e2\u0080\u00a2 Python \u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\n\nGoogle \u00e3\u0082\u00a2\u00e3\u0082\u00ab\u00e3\u0082\u00a6\u00e3\u0083\u00b3\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u00b3\u00a8\u00e6\u0084\u008f\u00e7\u0082\u00b9:\n\u00e2\u0080\u00a2 \u00e7\u008f\u00be\u00e5\u009c\u00a8 Google \u00e3\u0082\u00b5\u00e3\u0083\u00bc\u00e3\u0083\u0093\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e4\u00b8\u00ad\u00e5\u009b\u00bd\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u009b\u00e3\u0082\u0093\u00e3\u0080\u0082  Google Cloud Platform\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00a8\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e7\u00b4\u00b9\u00e4\u00bb\u008b Google Cloud Platform \u00e3\u0081\u00a8\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0083\u0080\u00e3\u0082\u00af\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081 GCP \u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0094\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0081\u00ae\u00e5\u009f\u00ba\u00e7\u00a4\u008e \u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00a6\u00e3\u0083\u0089\u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0: GCP \u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0094\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3: \u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0081\u00aa\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0080\u0081\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0080\u0081\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0081\u00bf Google Cloud Platform\u00e3\u0080\u0081\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0080\u0081ML \u00e3\u0081\u00ae\u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081  \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00a8\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u00a0\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0091\u00e3\u0082\u008b\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00ae\u00e5\u009f\u00ba\u00e7\u00a4\u008e\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0094\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0080\u0081\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0080\u0081\u00e3\u0083\u0095\u00e3\u0082\u00a7\u00e3\u0083\u0087\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0083\u0083\u00e3\u0083\u0089\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0082\u0092\u00e6\u008f\u0090\u00e4\u00be\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e4\u00bb\u0095\u00e7\u00b5\u0084\u00e3\u0081\u00bf\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google \u00e3\u0081\u00a7\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0083\u00a6\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e7\u00b4\u00b9\u00e4\u00bb\u008b\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082Google \u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a5\u00ad\u00e7\u0095\u008c\u00e5\u0086\u0085\u00e3\u0081\u00a7\u00e5\u00b9\u0085\u00e5\u00ba\u0083\u00e3\u0081\u008f\u00e6\u0089\u00b1\u00e3\u0082\u008f\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e5\u00a4\u00a7\u00e9\u0087\u008f\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e7\u00b0\u00a1\u00e5\u008d\u0098\u00e3\u0081\u00ab\u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00a6\u00e3\u0083\u0089\u00e3\u0081\u00ab\u00e7\u00a7\u00bb\u00e8\u00a1\u008c\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e5\u008f\u0097\u00e8\u00ac\u009b\u00e8\u0080\u0085\u00e3\u0081\u008c\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u008e\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0083\u00bc\u00e3\u0081\u00a8\u00e7\u009b\u00b4\u00e6\u008e\u00a5\u00e9\u0096\u00a2\u00e4\u00bf\u0082\u00e3\u0081\u0097\u00e3\u0081\u00aa\u00e3\u0081\u0084\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e6\u0080\u00a7\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0082\u0082\u00e3\u0081\u00ae\u00e3\u0081\u00ae\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00ae\u00e3\u0081\u009d\u00e3\u0081\u00ae\u00e4\u00bb\u0096\u00e3\u0081\u00ae\u00e9\u009d\u00a9\u00e6\u0096\u00b0\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u008e\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0083\u00bc\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00ef\u00bc\u0088\u00e3\u0080\u008c\u00e6\u00ac\u00a1\u00e3\u0081\u00ae\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u008e\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0083\u00bc\u00e3\u0080\u008d\u00ef\u00bc\u0089\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081TaskQueues \u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e9\u009d\u009e\u00e5\u0090\u008c\u00e6\u009c\u009f\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0080\u0081Pub/Sub \u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u00a1\u00e3\u0083\u0083\u00e3\u0082\u00bb\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e6\u008c\u0087\u00e5\u0090\u0091\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3\u00e3\u0080\u0081Dataflow \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u00a8\u00e3\u0081\u0084\u00e3\u0081\u00a3\u00e3\u0081\u009f\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 ", "This course introduces students to the science of business analytics while casting a keen eye toward the artful use of numbers found in the digital space. The goal is to provide businesses and managers with the foundation needed to apply data analytics to real-world challenges they confront daily in their professional lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize data; and utilize data in decision making for their agencies, organizations or clients. Module 0:  Get Ready & Module 1: Drowning in Data, Starving for Knowledge Module 2: Decision Trees  Module 3: Rules, Rules, and More Rules Module 4: Model Performance and Recommendation Systems This module will introduce you to the most common and important unsupervised learning technique \u00e2\u0080\u0093 Clustering. You will have an understanding of different applications of clustering analysis after this module. You will also learn when we need clustering and why it is important. Then, you will be introduced to a variety of clustering methods. In this module, we will discuss how to use decision trees to represent knowledge. The module concludes with a presentation of the Random Forest method that overcomes some of the limitations\u00c2\u00a0(such as high variance or low precision)\u00c2\u00a0of a single decision tree constructed from data.\n This module will\u00c2\u00a0focus\u00c2\u00a0on three key topics, namely rules, nearest neighbor methods, and Bayesian methods. Over the course of this module, you will be exposed to how rules factor into the world of data and how they play a role in the analysis of data. The second and third topics focus on the classification of data. In this module, you will\u00c2\u00a0study tools for recognizing what to recommend, and identify cross-sell or upsell opportunities. As the last module of the course, we will wrap up the content so far and you will get an opportunity to practice on your own and learn how to adapt these models to drive business impact in your own organizations.", "This course will continue the introduction to Python programming that started with Python Programming Essentials.  We'll learn about different data representations, including strings, lists, and tuples, that form the core of all Python programs.  We will also teach you how to access files, which will allow you to store and retrieve data within your programs. These concepts and skills will help you to manipulate data and write more complex Python programs.\n\nBy the end of the course, you will be able to write Python programs that can manipulate data stored in files.  This will extend your Python programming expertise, enabling you to write a wide range of scripts using Python\n\nThis course uses Python 3.  While most Python programs continue to use Python 2, Python 3 is the future of the Python programming language. This course introduces basic desktop Python development environments, allowing you to run Python programs directly on your computer. This choice enables a smooth transition from online development environments. Strings Basics of Lists List Manipulation File Access This module will teach you about Python's string data type and its capabilities. Strings are used to represent text within programs. This module will teach you the basics of Python's list data type. Lists are used to hold a sequence of data within programs. This module will dive further into the use of lists. You will learn how about mutating the contents of a list and the implications of doing so. This module will teach you how to access files in Python.", "There is a significant number of tasks when we need not just to process an enormous volume of data but to process it as quickly as possible. Delays in tsunami prediction can cost people\u00e2\u0080\u0099s lives. Delays in traffic jam prediction cost extra time. Advertisements based on the recent users\u00e2\u0080\u0099 activity are ten times more popular.\n\nHowever, stream processing techniques alone are not enough to create a complete real-time system. For example to create a recommendation system we need to have a storage that allows to store and fetch data for a user with minimal latency. These databases should be able to store hundreds of terabytes of data, handle billions of requests per day and have a 100% uptime. NoSQL databases are commonly used to solve this challenging problem.\n\nAfter you finish this course, you will master stream processing systems and NoSQL databases. You will also learn how to use such popular and powerful systems as  Kafka, Cassandra and Redis.\n\nTo get the most out of this course, you need to know Hadoop and SQL. You should also have a working knowledge of bash, Python and Spark.\n\nDo you want to learn how to build Big Data applications that can withstand modern challenges? Jump right in! Welcome to the course \"Big Data Applications: Real-Time Streaming\" Basics of real-time data processing Spark Streaming NoSQL. Cassandra NoSQL. Redis     ", "In this course you will learn how to use D3.js to create powerful visualizations for web. Learning D3.js will enable you to create many different types of visualization and to visualize many different data types. It will give you the freedom to create something as simple as a bar chart as well your own new revolutionary technique. \n\nIn this course we will cover the basics of creating visualizations with D3 as well as how to deal with tabular data, geography and networks. By the end of this course you will be able to:\n\n- Create bar and line charts\n- Create choropleth and symbol maps\n- Create node-link diagrams and tree maps\n- Implement zooming and brushing\n- Link two or more views through interaction\n\nThe course mixes theoretical and practical lectures. We will show you step by step how to use the library to build actual visualizations and what theoretical concepts lie behind them. Throughout the course you will learn skills that will lead you to building a whole application by the end of the lectures (a fully working visualization system to visualize airlines routes).\n\nThis course is the third one of the \u00e2\u0080\u009cSpecialization in Information Visualization\". The course expects you to have some basic knowledge of programming as well as some basic visualization skills. Introduction to web and d3 Dealing  & drawing with data Lines, Arcs, and maps Layouts and interaction In this module we will focus on the basics of web development and d3.js In this week we will learn how can we load and manipulate data using d3.js  ", "Welcome to Survival Analysis in R for Public Health!\n\nThe three earlier courses in this series covered statistical thinking, correlation, linear regression and logistic regression. This one will show you how to run survival \u00e2\u0080\u0093 or \u00e2\u0080\u009ctime to event\u00e2\u0080\u009d \u00e2\u0080\u0093 analysis, explaining what\u00e2\u0080\u0099s meant by familiar-sounding but deceptive terms like hazard and censoring, which have specific meanings in this context. Using the popular and completely free software R, you\u00e2\u0080\u0099ll learn how to take a data set from scratch, import it into R, run essential descriptive analyses to get to know the data\u00e2\u0080\u0099s features and quirks, and progress from Kaplan-Meier plots through to multiple Cox regression. You\u00e2\u0080\u0099ll use data simulated from real, messy patient-level data for patients admitted to hospital with heart failure and learn how to explore which factors predict their subsequent mortality. You\u00e2\u0080\u0099ll learn how to test model assumptions and fit to the data and some simple tricks to get round common problems that real public health data have. There will be mini-quizzes on the videos and the R exercises with feedback along the way to check your understanding.\n\nPrerequisites\n\nSome formulae are given to aid understanding, but this is not one of those courses where you need a mathematics degree to follow it. You will need basic numeracy (for example, we will not use calculus) and familiarity with graphical and tabular ways of presenting results. The three previous courses in the series explained concepts such as hypothesis testing, p values, confidence intervals, correlation and regression and showed how to install R and run basic commands. In this course, we will recap all these core ideas in brief, but if you are unfamiliar with them, then you may prefer to take the first course in particular, Statistical Thinking in Public Health, and perhaps also the second, on linear regression, before embarking on this one. The Kaplan-Meier Plot  The Cox Model The Multiple Cox Model The Proportionality Assumption What is survival analysis? You\u00e2\u0080\u0099ll see what it is, when to use it and how to run and interpret the most common descriptive survival analysis method, the Kaplan-Meier plot and its associated log-rank test for comparing the survival of two or more patient groups, e.g. those on different treatments. You\u00e2\u0080\u0099ll learn about the key concept of censoring. This week you\u00e2\u0080\u0099ll get to know the most commonly used survival analysis method for incorporating not just one but multiple predictors of survival: Cox proportional hazards regression modelling. You\u00e2\u0080\u0099ll learn about the key concepts of hazards and the risk set. From now and until the end of this course, there\u00e2\u0080\u0099ll be plenty of chance to run Cox models on data simulated from real patient-level records for people admitted to hospital with heart failure. You\u00e2\u0080\u0099ll see why missing data and categorical variables can cause problems in regression models such as Cox. You\u00e2\u0080\u0099ll extend the simple Cox model to the multiple Cox model. As preparation, you\u00e2\u0080\u0099ll run the essential descriptive statistics on your main variables. Then you\u00e2\u0080\u0099ll see what can happen with real-life public health data and learn some simple tricks to fix the problem. In this final part of the course, you\u00e2\u0080\u0099ll learn how to assess the fit of the model and test the validity of the main assumptions involved in Cox regression such as proportional hazards. This will cover three types of residuals. Lastly, you\u00e2\u0080\u0099ll get to practise fitting a multiple Cox regression model and will have to decide which predictors to include and which to drop, a ubiquitous challenge for people fitting any type of regression model.", "Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for data-driven phrase mining and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns. Course Orientation Module 1 Module 2 Module 3 Week 4 The course orientation will get you familiar with the course, your instructor, your classmates, and our learning environment. Module 1 consists of two lessons. Lesson 1 covers the general concepts of pattern discovery. This includes the basic concepts of frequent patterns, closed patterns, max-patterns, and association rules. Lesson 2 covers three major approaches for mining frequent patterns. We will learn the downward closure (or Apriori) property of frequent patterns and three major categories of methods for mining frequent patterns: the Apriori algorithm, the method that explores vertical data format, and the pattern-growth approach.  We will also discuss how to directly mine the set of closed patterns. Module 2 covers two lessons: Lessons 3 and 4.  In Lesson 3, we discuss pattern evaluation and learn what kind of interesting measures should be used in pattern analysis. We show that the support-confidence framework is inadequate for pattern evaluation, and even the popularly used lift and chi-square measures may not be good under certain situations. We introduce the concept of null-invariance and introduce a new null-invariant measure for pattern evaluation. In Lesson 4, we examine the issues on mining a diverse spectrum of patterns. We learn the concepts of and mining methods for multiple-level associations, multi-dimensional associations, quantitative associations, negative correlations, compressed patterns, and redundancy-aware patterns. Module 3 consists of two lessons: Lessons 5 and 6.   In Lesson 5, we discuss mining sequential patterns.   We will learn several popular and efficient sequential pattern mining methods, including an Apriori-based sequential pattern mining method, GSP; a vertical data format-based sequential pattern method, SPADE; and a pattern-growth-based sequential pattern mining method, PrefixSpan. We will also learn how to directly mine closed sequential patterns. In Lesson 6, we will study concepts and methods for mining spatiotemporal and trajectory patterns as one kind of pattern mining applications. We will introduce a few popular kinds of patterns and their mining methods, including mining spatial associations, mining spatial colocation patterns, mining and aggregating patterns over multiple trajectories, mining semantics-rich movement patterns, and mining periodic movement patterns. Module 4 consists of two lessons: Lessons 7 and 8.   In Lesson 7, we study mining quality phrases from text data as the second kind of pattern mining application. We will mainly introduce two newer methods for phrase mining: ToPMine and SegPhrase, and show frequent pattern mining may be an important role for mining quality phrases in massive text data. In Lesson 8, we will learn several advanced topics on pattern discovery, including mining frequent patterns in data streams, pattern discovery for software bug mining, pattern discovery for image analysis, and pattern discovery and society: privacy-preserving pattern mining.  Finally, we look forward to the future of pattern mining research and application exploration.", "The lectures of this course are based on the first 11 chapters of Prof. Raymond Yeung\u00e2\u0080\u0099s textbook entitled Information Theory and Network Coding (Springer 2008).  This book and its predecessor, A First Course in Information Theory (Kluwer 2002, essentially the first edition of the 2008 book), have been adopted by over 60 universities around the world as either a textbook or reference text.\n\nAt the completion of this course, the student should be able to:\n1) Demonstrate knowledge and understanding of the fundamentals of information theory.\n2) Appreciate the notion of fundamental limits in communication systems and more generally all systems.\n3) Develop deeper understanding of communication systems.\n4) Apply the concepts of information theory to various disciplines in information science. Course Preliminaries Chapter 1 Information Measures Chapter 2 Information Measures - Part 1 Chapter 2 Information Measures - Part 2 Chapter 3 The I-Measure - Part 1 Chapter 3 The I-Measure - Part 2 Chapter 4 Zero-Error Data Compression - Part 1 Chapter 4 Zero-Error Data Compression - Part 2 Chapter 5 Weak Typicality Chapter 6 Strong Typicality Chapter 7 Discrete Memoryless Channels - Part 1 Chapter 7 Discrete Memoryless Channels - Part 2 Chapter 8 Rate-Distortion Theory - Part 1 Chapter 8 Rate-Distortion Theory - Part 2 Chapter 9 The Blahut-Arimoto Algorithms - Part 1 Chapter 9 The Blahut-Arimoto Algorithms - Part 2 Chapter 10 Differential Entropy - Part 1 Chapter 10 Differential Entropy - Part 2 Chapter 11 Continuous-Valued Channels - Part 1 Chapter 11 Continuous-Valued Channels - Part 2 Chapter 11 Continuous-Valued Channels - Part 3                      ", "\u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084\u00d9\u0089 \u00d9\u0081\u00d9\u008a \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d9\u0084\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a8\u00d8\u00a7\u00d8\u00af\u00d8\u00a6 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Microsoft Excel. \u00d9\u0081\u00d9\u008a \u00d8\u00ba\u00d8\u00b6\u00d9\u0088\u00d9\u0086 \u00d8\u00b3\u00d8\u00aa\u00d8\u00a9 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d9\u008a\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d9\u0082\u00d9\u0084 \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d9\u0085\u00d8\u00ad\u00d8\u00aa\u00d8\u00b1\u00d9\u0081 \u00d9\u0081\u00d9\u008a \u00d9\u0088\u00d8\u00a7\u00d8\u00ac\u00d9\u0087\u00d8\u00a9 \u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085 \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel\u00d8\u008c \u00d9\u0088\u00d8\u00a5\u00d8\u00ac\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u008a\u00d8\u00ba \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00aa\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00ac\u00d8\u00af\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d8\u00a8\u00d8\u00a7\u00d8\u00ad\u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d9\u0081\u00d9\u008a\u00d8\u008c \u00d9\u0088\u00d8\u00a5\u00d9\u0086\u00d8\u00b4\u00d8\u00a7\u00d8\u00a1 \u00d8\u00aa\u00d8\u00b5\u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d9\u0086 \u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00b7\u00d8\u00b7\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00b3\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d9\u008a\u00d8\u00a9.\u00e2\u0080\u008f\n\n\u00d9\u0088\u00d8\u00b3\u00d9\u0088\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a3\u00d9\u0083\u00d9\u0086\u00d8\u00aa \u00d9\u0082\u00d8\u00af \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00aa \u00d8\u00b0\u00d8\u00a7\u00d8\u00aa\u00d9\u008a\u00d9\u008b\u00d8\u00a7 \u00d9\u0088\u00d8\u00aa\u00d8\u00b1\u00d9\u008a\u00d8\u00af \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d8\u00b3\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00ac\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0081\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a5\u00d9\u0086\u00d8\u00aa\u00d8\u00a7\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d8\u00a3\u00d9\u0081\u00d8\u00b6\u00d9\u0084\u00d8\u008c \u00d8\u00a3\u00d9\u0088 \u00d8\u00a5\u00d8\u00b0\u00d8\u00a7 \u00d9\u0084\u00d9\u0085 \u00d8\u00aa\u00d9\u0083\u00d9\u0086 \u00d9\u0082\u00d8\u00af \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d9\u0085\u00d9\u0086 \u00d9\u0082\u00d8\u00a8\u00d9\u0084\u00d8\u008c \u00d9\u0081\u00d8\u00a5\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00b3\u00d8\u00aa\u00d8\u00b2\u00d9\u0088\u00d8\u00af\u00d9\u0083 \u00d8\u00a8\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3 \u00d9\u0085\u00d8\u00aa\u00d9\u008a\u00d9\u0086 \u00d9\u0084\u00d9\u0083\u00d9\u008a \u00d8\u00aa\u00d8\u00b5\u00d8\u00a8\u00d8\u00ad \u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d9\u008b\u00d8\u00a7 \u00d9\u0088\u00d8\u00a7\u00d8\u00ab\u00d9\u0082\u00d9\u008b\u00d8\u00a7 \u00d9\u0088\u00d8\u00aa\u00d8\u00b7\u00d9\u0088\u00d8\u00b1 \u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d9\u008b\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0084\u00d8\u00a7\u00d8\u00ad\u00d9\u0082\u00d8\u00a9.\n\n\u00d9\u0084\u00d9\u0082\u00d8\u00af \u00d8\u00b4\u00d9\u0083\u00d9\u0091\u00d9\u0084\u00d9\u0086\u00d8\u00a7 \u00d9\u0081\u00d8\u00b1\u00d9\u008a\u00d9\u0082\u00d9\u008b\u00d8\u00a7 \u00d8\u00b1\u00d8\u00a7\u00d8\u00a6\u00d8\u00b9\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00b3 \u00d8\u00b3\u00d9\u008a\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0085\u00d8\u00b9\u00d9\u0083 \u00d9\u0081\u00d9\u008a \u00d9\u0083\u00d9\u0084 \u00d8\u00ae\u00d8\u00b7\u00d9\u0088\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d9\u0085\u00d9\u0086\u00d8\u00ad\u00d9\u0083 \u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d9\u0083\u00d8\u00a8\u00d9\u008a\u00d8\u00b1\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00ae\u00d8\u00aa\u00d8\u00a8\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0081\u00d8\u00b1\u00d8\u00b5\u00d9\u008b\u00d8\u00a7 \u00d8\u00b1\u00d8\u00a7\u00d8\u00a6\u00d8\u00b9\u00d8\u00a9 \u00d9\u0084\u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1 \u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00af\u00d9\u008a\u00d9\u0083. \u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0088\u00d9\u0086 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d9\u0084 \u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008d \u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d8\u00b9 \u00d9\u0081\u00d8\u00b1\u00d9\u008a\u00d9\u0082\u00d9\u0086\u00d8\u00a7\u00d8\u008c \u00d9\u0088\u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d9\u0081\u00d8\u00a7\u00d8\u00ac\u00d8\u00a6 \u00d9\u0086\u00d9\u0081\u00d8\u00b3\u00d9\u0083 \u00d8\u00a8\u00d9\u0085\u00d8\u00af\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d8\u00a3\u00d8\u00ad\u00d8\u00b1\u00d8\u00b2\u00d8\u00aa\u00d9\u0087 \u00d9\u0081\u00d9\u008a \u00d9\u0088\u00d9\u0082\u00d8\u00aa \u00d9\u0082\u00d8\u00b5\u00d9\u008a\u00d8\u00b1.\n\n\u00d9\u008f\u00d9\u008a\u00d8\u00b9\u00d8\u00af \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d8\u00ac\u00d8\u00af\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a3\u00d8\u00ad\u00d8\u00af \u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0086\u00d8\u00aa\u00d8\u00b4\u00d8\u00a7\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081 \u00d8\u00a3\u00d9\u0085\u00d8\u00a7\u00d9\u0083\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0081\u00d9\u008a \u00d8\u00ac\u00d9\u0085\u00d9\u008a\u00d8\u00b9 \u00d8\u00a3\u00d9\u0086\u00d8\u00ad\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0085. \u00d9\u0088\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0091\u00d9\u008f\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d8\u00a8\u00d8\u00ab\u00d9\u0082\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d8\u00ad\u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d9\u008a\u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d8\u00a5\u00d8\u00b6\u00d8\u00a7\u00d9\u0081\u00d8\u00a9\u00d9\u008b \u00d8\u00ab\u00d9\u0085\u00d9\u008a\u00d9\u0086\u00d8\u00a9 \u00d9\u0088\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b3\u00d9\u008a\u00d8\u00b1\u00d8\u00aa\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d9\u0081\u00d8\u00b1\u00d8\u00b5 \u00d8\u00aa\u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d9\u0083. \u00d9\u0088\u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d9\u0082\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d8\u00aa\u00d9\u0086\u00d9\u0085\u00d9\u0088 \u00d9\u0081\u00d9\u008a\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00b9\u00d8\u00aa\u00d9\u0085\u00d8\u00af \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d8\u00a3\u00d8\u00b3\u00d8\u00b1\u00d8\u00b9 \u00d8\u00a8\u00d9\u0083\u00d8\u00ab\u00d9\u008a\u00d8\u00b1 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081 \u00d8\u00ba\u00d9\u008a\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00a7\u00d8\u00ad\u00d8\u00b1\u00d8\u00b5 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0085\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d8\u00b2\u00d9\u008b\u00d8\u00a7 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a2\u00d8\u00ae\u00d8\u00b1\u00d9\u008a\u00d9\u0086 \u00d8\u00b9\u00d9\u0086 \u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082 \u00d8\u00a5\u00d8\u00b6\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa Excel \u00d9\u0084\u00d9\u0085\u00d8\u00ad\u00d9\u0081\u00d8\u00b8\u00d8\u00aa\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9. \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d8\u00a5\u00d8\u00ac\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d8\u00a8\u00d8\u00a7\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00b7\u00d8\u00b7\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d8\u00a6\u00d9\u008a \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u008a\u00d8\u00b2\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel: \u00d9\u0088\u00d8\u00a7\u00d8\u00ac\u00d9\u0087\u00d8\u00a9 \u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085 \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel\u00d8\u008c \u00d9\u0085\u00d8\u00b5\u00d8\u00b7\u00d9\u0084\u00d8\u00ad\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00b4\u00d8\u00ba\u00d9\u008a\u00d9\u0084 \u00d9\u0085\u00d9\u0081\u00d8\u00a7\u00d8\u00aa\u00d9\u008a\u00d8\u00ad \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d9\u0083\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d9\u0082\u00d9\u0084 \u00d9\u0081\u00d9\u008a \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d9\u0086\u00d9\u0081\u00d9\u008a\u00d8\u00b0 \u00d8\u00a5\u00d8\u00af\u00d8\u00ae\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00ac\u00d8\u00af\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel. \u00d9\u008a\u00d8\u00b9\u00d8\u00aa\u00d8\u00a8\u00d8\u00b1 \u00d8\u00a5\u00d9\u0086\u00d8\u00b4\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u008a\u00d8\u00ba \u00d9\u0081\u00d9\u008a \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel \u00d8\u00a3\u00d9\u0085\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d9\u0084\u00d8\u00ba\u00d8\u00a7\u00d9\u008a\u00d8\u00a9. \u00d9\u0088\u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u008a\u00d8\u00ba \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00aa- \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d9\u0083\u00d8\u00aa\u00d8\u00a7\u00d8\u00a8\u00d8\u00aa\u00d9\u0087\u00d8\u00a7\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d8\u00a5\u00d8\u00ac\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d9\u0081\u00d9\u0087\u00d9\u0085 \u00d9\u0085\u00d8\u00b1\u00d8\u00a7\u00d8\u00ac\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9. \u00d9\u008a\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a5\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d8\u00b2 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00ac\u00d8\u00b9\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a3\u00d9\u0086\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d9\u0088\u00d8\u00b5\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6. \u00d8\u00aa\u00d8\u00ba\u00d8\u00b7\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0086 \u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00b7 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00af\u00d9\u0088\u00d8\u00af \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ad\u00d8\u00a7\u00d8\u00b0\u00d8\u00a7\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b1\u00d9\u0082\u00d8\u00a7\u00d9\u0085\u00d8\u008c \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00b6\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u0086\u00d9\u0085\u00d8\u00a7\u00d8\u00b7 \u00d9\u0088\u00d8\u00b3\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel. \u00d8\u00aa\u00d8\u00af\u00d9\u0088\u00d8\u00b1 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9 \u00d8\u00ad\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa - \u00d9\u0088\u00d8\u00aa\u00d8\u00b3\u00d9\u0087\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9\u00d9\u0087\u00d8\u00a7. \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a5\u00d8\u00af\u00d8\u00a7\u00d8\u00b1\u00d8\u00a9 \u00d8\u00ac\u00d8\u00af\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa - \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00ab\u00d9\u0088\u00d8\u00b1 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b5\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00b1\u00d8\u00b2\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00b1\u00d8\u00af\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00aa\u00d8\u00ba\u00d9\u008a\u00d9\u008a\u00d8\u00b1\u00d9\u0087\u00d8\u00a7 \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00ad\u00d8\u00ab \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00a8\u00d8\u00af\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00b3\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d8\u00b7\u00d9\u008a \u00d9\u0084\u00d8\u00a5\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d8\u00b2 \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d8\u00ad\u00d8\u00af\u00d8\u00af\u00d8\u00a9. \u00d9\u0081\u00d9\u008a \u00d8\u00a8\u00d8\u00b9\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ad\u00d9\u008a\u00d8\u00a7\u00d9\u0086 \u00d8\u00aa\u00d8\u00ad\u00d8\u00aa\u00d8\u00a7\u00d8\u00ac \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00b7\u00d8\u00a8\u00d8\u00a7\u00d8\u00b9\u00d8\u00a9 \u00d8\u00ac\u00d8\u00af\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0083\u00d8\u008c \u00d9\u0088\u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af\u00d9\u0083 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00b0\u00d9\u0084\u00d9\u0083. \u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00ad\u00d8\u00b3\u00d9\u008a\u00d9\u0086 \u00d8\u00ac\u00d8\u00af\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0084\u00d8\u00b7\u00d8\u00a8\u00d8\u00a7\u00d8\u00b9\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082 \u00d8\u00a5\u00d8\u00af\u00d8\u00a7\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d9\u0088\u00d8\u00a7\u00d8\u00b4\u00d9\u008a \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d9\u0087 \u00d9\u0088\u00d8\u00b1\u00d8\u00a4\u00d9\u0088\u00d8\u00b3 \u00d9\u0088\u00d8\u00aa\u00d8\u00b0\u00d9\u008a\u00d9\u008a\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u0081\u00d8\u00ad\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d9\u0085\u00d9\u0086 \u00d8\u00b0\u00d9\u0084\u00d9\u0083. \u00d8\u00aa\u00d8\u00b9\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00b7\u00d8\u00b7\u00d8\u00a7\u00d8\u00aa \u00d8\u00a5\u00d8\u00ad\u00d8\u00af\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d8\u00b1\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00b4\u00d9\u008a\u00d9\u0088\u00d8\u00b9\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d9\u0085\u00d8\u00b1\u00d8\u00a6\u00d9\u008a. \u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d9\u0083 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a5\u00d9\u0086\u00d8\u00b4\u00d8\u00a7\u00d8\u00a1 \u00d9\u0088\u00d8\u00aa\u00d8\u00b9\u00d8\u00af\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00b7\u00d8\u00b7\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Excel. ", "The third course in this specialization is Achieving Advanced Insights with BigQuery. Here we will build on your growing knowledge of SQL as we dive into advanced functions and how to break apart a complex query into manageable steps. \n\nWe will cover the internal architecture of BigQuery (column-based sharded storage) and advanced SQL topics like nested and repeated fields through the use of Arrays and Structs. Lastly we will dive into optimizing your queries for performance and how you can secure your data through authorized views.\n\n>>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Advanced Functions and Clauses Schema Design and Nested Data Structures More Visualization with Google Data Studio Optimizing for Performance Advanced Insights with Cloud Datalab Data Access Summary Welcome to the third course in the Data Insights specialization. Let's review what material we will cover with advanced BigQuery functions and architecture. Deepen your knowledge of SQL on BigQuery by learning about more advanced functions like statistical approximations, analytical window queries, user-defined functions, and WITH clauses. Walkthrough the evolution of how traditional databases handle dataset scale and compare how BigQuery was developed to address scaling limitations. Deep dive into nested and repeated fields which are a key part of denormalized BigQuery data structures. Dive deeper into advanced visualization topics like dashboard calculated fields, filters, multi-page reports, and dashboard cache. Learn the fundamental pieces of work that impact BigQuery performance and how to optimize your queries for speed. Introducing Cloud Datalab -- a key tool in the Data Scientist toolkit -- which enables analysts to collaborate through the use of scalable cloud notebooks. Securing and sharing your BigQuery datasets is critical for any organization. Learn what Google Cloud Platform and BigQuery tools are available to you to permission control and share your data.  Congratulations! You have made it to the end. Let's recap what we have covered so far.", "This course provides you with an understanding of what Enterprise Systems (also commonly termed as Enterprise Resource Planning Systems, ERPs) are. After learning about what these systems are, we would touch upon why these systems are useful to companies, through which you would get to see the various jobs and positions that are associated with the use and deployment of ERPs. \n\nIn this course, you would also develop an appreciation of the managerial aspects related to the selection and implementation of ERPs. Specifically, we would touch on the important points to consider when shortlisting and purchasing an ERP, the approaches taken in ERP implementation, and change management techniques to utilize when an organization is undergoing ERP implementation. At the end of this class, you will be endowed with practical knowledge that would help you to address real world business problems associated with ERP usage and implementation. Enterprise Resource Planning (ERP) Fundamentals Business Processes in ERP Software Selection & Considerations Change Management Course Project     ", "The practice of investment management has been transformed in recent years by computational methods. This course provides an introduction to the underlying science, with the aim of giving you a thorough understanding of that scientific basis. However, instead of merely explaining the science, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the Python programming language. \n\nThis course is the first in a four course specialization in Data Science and Machine Learning in Asset Management but can be taken independently. In this course, we cover the basics of Investment Science, and we'll build practical implementations of each of the concepts along the way. We'll start with the very basics of risk and return and quickly progress to cover a range of topics including several Nobel Prize winning concepts. We'll cover some of the most popular practical techniques in modern, state of the art investment management and portfolio construction. \n\nAs we cover the theory and math in lecture videos, we'll also implement the concepts in Python, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern computational methods in investment management, you'll have practical mastery in the implementation of those methods. Analysing returns An Introduction to Portfolio Optimization Beyond Diversification Introduction to Asset-Liability Management    ", "This course will provide learners with an introduction to research data management and sharing. After completing this course, learners will understand the diversity of data and their management needs across the research data lifecycle, be able to identify the components of good data management plans, and be familiar with best practices for working with data including the organization, documentation, and storage and security of data. Learners will also understand the impetus and importance of archiving and sharing data as well as how to assess the trustworthiness of repositories. \n\nToday, an increasing number of funding agencies, journals, and other stakeholders are requiring data producers to share, archive, and plan for the management of their data. In order to respond to these requirements, researchers and information professionals will need the data management and curation knowledge and skills that support the long-term preservation, access, and reuse of data. Effectively managing data can also help optimize research outputs, increase the impact of research, and support open scientific inquiry. After completing this course, learners will be better equipped to manage data throughout the entire research data lifecycle from project planning to the end of the project when data ideally are shared and made available within a trustworthy repository.\n\nThis course was developed by the Curating Research Assets and Data Using Lifecycle Education (CRADLE) Project in collaboration with EDINA at the University of Edinburgh. \n\nThis course was made possible in part by the Institute of Museum and Library Services under award #RE-06-13-0052-13. The views, findings, conclusions or recommendations expressed in this Research Data Management and Sharing MOOC do not necessarily represent those of the Institute of Museum and Library Services.\n\nHashtag: #RDMSmooc Understanding Research Data Data Management Planning Working with Data Sharing Data Archiving Data This week introduces multiple types of research data in an array of contexts as well as important data management concepts including metadata and the research data lifecycle. We will also define the concept of data management, identify the roles and responsibilities of key stakeholders, and examine various data management tasks throughout the research data lifecycle.  This week provides an overview of Data Management Plans (DMPs) including the components of good DMPs, the DMP policies of several funding agencies, and information on data management planning tools.   This week is brought to you by EDINA and the Data Library at the University of Edinburgh and is presented by Sarah Jones from the Digital Curation Centre. Sarah will introduce strategies for organizing research data including versioning and file naming conventions as well as data file formatting and transformations. She will also discuss why documenting data and data citation are important. Finally, she will present issues involved in storing, securing, and backing up research data.    This week examines the benefits and challenges of sharing research data. We will also discuss how to protect confidentiality and how data ownership can affect data sharing. Finally, we will examine different types of access restrictions that may be placed on data as well as how to enable data sharing through the application of a standard license. During the final week of the course, we will examine the preservation needs of research data, introduce the concepts of authenticity and integrity, and identify the different types of metadata and their role in data discovery and reuse. We will also discuss the role of trustworthy repositories as well as how repositories demonstrate their trustworthiness through audit and certification. Finally, we will present key archival standards and best practices for ensuring data remains accessible and understandable for the long-term.", "This project completer has proven a deep understanding on massive parallel data processing, data exploration and visualization, advanced machine learning and deep learning and how to apply his knowledge in a real-world practical use case where he justifies architectural decisions, proves understanding the characteristics of different algorithms, frameworks and technologies and how they impact model performance and scalability.\u00c2\u00a0\n\nPlease note: You are requested to create a short video presentation at the end of the course. This is mandatory to pass. You don't need to share the video in public. Week 1 - Identify DataSet and UseCase Week 2 - ETL and Feature Creation Week 3 - Model Definition and Training Model Evaluation, Tuning, Deployment and Documentation In this module, the basic process model used for this capstone project is introduced. Furthermore, the learner is required to identify a practical use case and data set This module emphasizes on the importance of ETL, data cleansing and feature creation as a preliminary step in ever data science project  This module emphasizes on model selection based on use case and data set. It is important to understand how those two factors impact choice of a useful model algorithm.  One a model is trained it is important to assess its performance using an appropriate metric. In addition, once the model is finished, it has to be made consumable by business stakeholders in an appropriate way  ", "You may never be sure whether you have an effective user experience until you have tested it with users. In this course, you\u00e2\u0080\u0099ll learn how to design user-centered experiments, how to run such experiments, and how to analyze data from these experiments in order to evaluate and validate user experiences. You will work through real-world examples of experiments from the fields of UX, IxD, and HCI, understanding issues in experiment design and analysis. You will analyze multiple data sets using recipes given to you in the R statistical programming language -- no prior programming experience is assumed or required, but you will be required to read, understand, and modify code snippets provided to you. By the end of the course, you will be able to knowledgeably design, run, and analyze your own experiments that give statistical weight to your designs. Basic Experiment Design Concepts Tests of Proportions The T-Test Validity in Design and Analysis One-Factor Between-Subjects Experiments One-Factor Within-Subjects Experiments Factorial Experiment Designs Generalizing the Response The Power of Mixed Effects Models In this module, you will learn basic concepts relevant to the design and analysis of experiments, including mean comparisons, variance, statistical significance, practical significance, sampling, inclusion and exclusion criteria, and informed consent. You\u00e2\u0080\u0099ll also learn to think of an experiment in terms of usability, its participants, apparatus, procedure, and design & analysis. This module covers lecture videos 1-2. In this module, you will learn how to analyze user preferences (or other tallies) using tests of proportions. You will also get up and running with R and RStudio. Topics covered include independent and dependent variables, variable types, exploratory data analysis, p-values, asymptotic tests, exact tests, one-sample tests, two-sample tests, Chi-Square test, G-test, Fisher\u00e2\u0080\u0099s exact test, binomial test, multinomial test, post hoc tests, and pairwise comparisons. This module covers lecture videos 3-9. In this module, you will learn how to design and analyze a simple website A/B test. Topics include measurement error, independent variables as factors, factor levels, between-subjects factors, within-subjects factors, dependent variables as responses, response types, balanced designs, and how to report a t-test. You will perform your first analysis of variance in the form of an independent-samples t-test. This module covers lecture videos 10-11. In this module, you will learn about how to ensure that your data is valid through the design of experiments, and that your analyses are valid by understanding and testing for their assumptions. Topics include how to achieve experimental control, confounds, ecological validity, the three assumptions of ANOVA, data distributions, residuals, normality, homoscedasticity, parametric versus nonparametric tests, the Shapiro-Wilk test, the Kolmogorov-Smirnov test, Levene\u00e2\u0080\u0099s test, the Brown-Forsythe test, and the Mann-Whitney U test. This module covers lecture videos 12-15. In this module, you will learn about one-factor between-subjects experiments. The experiment examined will be a between-subjects study of task completion time with various programming tools. You will understand and analyze data from two-level factors and three-level factors using the independent-samples t-test, Mann-Whitney U test, one-way ANOVA, and Kruskal-Wallis test. You will learn how to report an F-test. You will also understand omnibus tests and how they relate to post hoc pairwise comparisons with adjustments for multiple comparisons. This module covers lecture videos 16-18. In this module, you will learn about one-factor within-subjects experiments, also known as repeated measures designs. The experiment examined will be a within-subjects study of subjects searching for contacts in a smartphone contacts manager, including the analysis of times, errors, and effort Likert-type scale ratings. You will learn counterbalancing strategies to avoid carryover effects, including full counterbalancing, Latin Squares, and balanced Latin Squares. You will understand and analyze data from two-level factors and three-level factors using the paired-samples t-test, Wilcoxon signed-rank test, oneway repeated measures ANOVA, and Friedman test. This module covers lecture videos 19-23. In this module, you will learn about experiments with multiple factors and factorial ANOVAs. The experiment examined will be text entry performance on different smartphone keyboards while sitting, standing, and walking. Topics include mixed factorial designs, interaction effects, factorial ANOVAs, and the Aligned Rank Transform as a nonparametric factorial ANOVA. This module covers lecture videos 24-27. In this module, you will learn about analyses for non-normal or non-numeric responses for between-subjects experiments using Generalized Linear Models (GLM). We will revisit three previous experiments and analyze them using generalized models. Topics include a review of response distributions, nominal logistic regression, ordinal logistic regression, and Poisson regression. This module covers lecture videos 28-29. In this module, you will learn about mixed effects models, specifically Linear Mixed Models (LMM) and Generalized Linear Mixed Models (GLMM). We will revisit our prior experiment on text entry performance on smartphones but this time, keeping every single measurement trial as part of the analysis. The full set of analyses covered in this course will also be reviewed. This module covers lecture videos 30-33.", "In this course you have the opportunity to use the skills you acquired in the two SAS programming courses to solve realistic problems. This course is also designed to give you a thorough review of SAS programming concepts so you are prepared to take the SAS Certified Specialist: Base Programming Using SAS 9.4 Exam. Course Overview and Data Setup Review of Getting Started with SAS Programming, Part 1 Review of Getting Started with SAS Programming, Part 2 Case Study/Programming Assignment: Analyze TSA Claims Data Review of Doing More with SAS Programming, Part 1 Review of Doing More with SAS Programming, Part 2 Case Study: Preparing World Tourism Data In this module you get an overview of this course and set up the data you need for practices and activities. This module is a review of the first three modules of the Getting Started with SAS Programming course. Lectures demonstrate the concepts you learned, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material. This module reviews the preparing, analyzing and exporting modules of the Getting Started with SAS Programming course. Lectures demonstrate the concepts you learned, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material. This module enables you to apply what you learned in Getting Started with SAS Programming to a real programming problem. This module is a review of the first four modules of the Doing More with SAS Programming course. Lectures demonstrate the concepts you learned about for preparing data, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material. This module is a review of the last three modules of the Doing More with SAS Programming course. Lectures demonstrate the concepts you learned about for preparing data, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material. ", "This course is an introduction to sequence models and their applications, including an overview of sequence model architectures and how to handle inputs of variable length.\n\n\u00e2\u0080\u00a2 Predict future values of a time-series\n\u00e2\u0080\u00a2 Classify free form text\n\u00e2\u0080\u00a2 Address time-series and text problems with recurrent neural networks\n\u00e2\u0080\u00a2 Choose between RNNs/LSTMs and simpler models\n\u00e2\u0080\u00a2 Train and reuse word embeddings in text problems\n\nYou will get hands-on practice building and optimizing your own text classification and sequence models on a variety of public datasets in the labs we\u00e2\u0080\u0099ll work on together.  \n\nPrerequisites: Basic SQL, familiarity with Python and TensorFlow\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Working with Sequences Recurrent Neural Networks Dealing with Longer Sequences Text Classification Reusable Embeddings Encoder-Decoder Models Summary In this module, you\u00e2\u0080\u0099ll learn what a sequence is, see how you can prepare sequence data for modeling, and be introduced to some classical approaches to sequence modeling and practice applying them. In this module, we introduce recurrent neural nets, explain how they address the variable-length sequence problem, explain how our traditional optimization procedure applies to RNNs, and review the limits of what RNNs can and can\u00e2\u0080\u0099t represent. In this module we dive deeper into RNNs. We\u00e2\u0080\u0099ll talk about LSTMs, Deep RNNs, working with real world data, and more. In this module we look at different ways of working with text and how to create your own text classification models.  Labeled data for our classification models is expensive and precious. Here we will address how we can reuse pre-trained embeddings to make our models with TensorFlow Hub. In this module, we focus on a sequence-to-sequence model called the encoder-decoder network to solve tasks, such as Machine Translation, Text Summarization and Question Answering. In this final module, we review what you have learned so far about sequence modeling for time-series and natural language data. ", "Accounting has always been about analytical thinking. From the earliest days of the profession, Luca Pacioli emphasized the importance of math and order for analyzing business transactions. The skillset that accountants have needed to perform math and to keep order has evolved from pencil and paper, to typewriters and calculators, then to spreadsheets and accounting software. A new skillset that is becoming more important for nearly every aspect of business is that of big data analytics: analyzing large amounts of data to find actionable insights. This course is designed to help accounting students develop an analytical mindset and prepare them to use data analytic programming languages like Python and R.\n \nWe\u00e2\u0080\u0099ve divided the course into three main sections. In the first section, we bridge accountancy to analytics. We identify how tasks in the five major subdomains of accounting (i.e., financial, managerial, audit, tax, and systems) have historically required an analytical mindset, and we then explore how those tasks can be  completed more effectively and efficiently by using big data analytics. We then present a FACT framework for guiding big data analytics: Frame a question, Assemble data, Calculate the data, and Tell others about the results.\n \nIn the second section of the course, we emphasize the importance of assembling data. Using financial statement data, we explain desirable characteristics of both data and datasets that will lead to effective calculations and visualizations.\n \nIn the third, and largest section of the course, we demonstrate and explore how Excel and Tableau can be used to analyze big data. We describe visual perception principles and then apply those principles to create effective visualizations. We then examine fundamental data analytic tools, such as regression, linear programming (using Excel Solver), and clustering in the context of point of sale data and loan data. We conclude by demonstrating the power of data analytic programming languages to assemble, visualize, and analyze data. We introduce Visual Basic for Applications  as an example of a programming language, and the Visual Basic Editor as an example of an integrated development environment (IDE). INTRODUCTION TO THE COURSE MODULE 1: INTRODUCTION TO ACCOUNTANCY ANALYTICS MODULE 2: ACCOUNTING ANALYSIS AND AN ANALYTICS MINDSET MODULE 3: DATA AND ITS PROPERTIES MODULE 4: DATA VISUALIZATION 1 MODULE 5: DATA VISUALIZATION 2 MODULE 6: ANALYTIC TOOLS IN EXCEL 1 MODULE 7: ANALYTIC TOOLS IN EXCEL 2 MODULE 8: AUTOMATION IN EXCEL In this module, you will become familiar with the course, your instructor and your classmates, and our learning environment. This orientation module will also help you obtain the technical skills required to navigate and be successful in this course. In this module, you will learn how the accounting profession has evolved. You will recognize how data analytics has influenced the accounting profession and how accountants have the ability to impact how data analytics is used in the profession, as well as in an organization. Finally, you will learn how data analytics is influencing the different subdomains within accounting. In this module, you will learn to recognize the importance of making room for empirical enquiry in decision making. You will explore characteristics of an analytical mindset in business and accounting contexts, and link those to your core courses. You will then evaluate a framework for making data-driven decisions using big data. This module looks at specific characteristics of data that make it useful for decision making. In this module, you will learn fundamental principles that underlie data visualizations. Using those principles, you will identify use cases for different charts and learn how to build those charts in Excel. You will then use your knowledge of different charts to identify alternative charts that are better suited for directing attention. In this module, you\u00e2\u0080\u0099ll learn how to use Tableau to do with data what spies do when observing their surroundings: get an overview of the data, narrow in on certain aspects of the data that seem abnormal, and then analyze the data. Tableau is a great tool for facilitating the overview, zoom, then filter details-on-demand approach. Tableau is a lot like a more powerful version of Excel's pivot table and pivot chart functionality. In this module, you'll be guided through a mini-case study that will illustrate the first three parts of the FACT model, with a focus on the C, or calculations part of the FACT model. First, you will perform a correlation analysis to identify two-way relationships, and analyze correlations using a correlation matrix and scatter plots. You will then build on your knowledge of correlations and learn how to perform regression analysis in Excel. Finally, you will learn how to interpret and evaluate the diagnostic metrics and plots of a regression analysis. In this module, you\u00e2\u0080\u0099ll learn how the regression algorithm can be applied to fit a wide variety of relationships among data. Specifically, you\u00e2\u0080\u0099ll learn how to set up the data and run a regression to estimate the parameters of nonlinear relationships, categorical independent variables. You\u00e2\u0080\u0099ll also investigate if the effect of an independent variable depends on the level of another independent variable by including interaction terms in the multiple regression model. Another aspect of this module is learning how to evaluate models, regression or otherwise, to find the most favorable levels of the independent variables. For models that explain revenue, the most favorable levels of the independent variables will maximize revenue. In contrast, if you have a model that describes costs, like a budget, then the most favorable levels of the independent variables will minimize costs. Optimizing models can be difficult because there are so many inputs and constraints that need to be managed. In this module, you\u00e2\u0080\u0099ll learn how to use the Solver Add-In to find the optimal level of inputs. For some models, the dependent variable is a binary variable that has only two values, such as true/false, win/lose, or invest/not invest. In these situations, a special type of regression, called logistic regression, is used to predict how each observation should be classified. You\u00e2\u0080\u0099ll learn about the logit transformation that\u00e2\u0080\u0099s used to convert a binary outcome to a linear relationship with the independent variables. Excel doesn\u00e2\u0080\u0099t have a built-in logistic regression tool, so you\u00e2\u0080\u0099ll learn how to manually design a logistic regression model, and then optimize the parameters using the Solver Add-In tool. The lessons in this module are organized around several useful tasks, including stacking multiple dataframes together into one dataframe, creating multiple histograms to accompany the descriptive statistics, and learning how to perform k-means clustering. After going through this module, you\u00e2\u0080\u0099ll not only gain a foundation to help you understand coding, but you\u00e2\u0080\u0099ll also learn more about analyzing financial data. Along the way, I hope that you\u00e2\u0080\u0099ll also pick up on a few other useful Excel functions.", "In the second course of this specialization, we will dive into the components and best practices of a high-performing ML system in production environments. \n\nPrerequisites: Basic SQL, familiarity with Python and TensorFlow\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Welcome to the course Architecting Production ML Systems Ingesting data for Cloud-based analytics and ML Designing Adaptable ML systems Designing High-performance ML systems Hybrid ML systems Course Summary In this module we will preview the topics covered in the course and how to use Qwiklabs to complete each of your labs using Google Cloud Platform. In this module, we\u00e2\u0080\u0099ll talk about what else a production ML system needs to do and how you can meet those needs. We\u00e2\u0080\u0099ll then review some important, high-level, design decisions around training and model serving that you\u00e2\u0080\u0099ll need to make in order to get the right performance profile for your model. In this module, we\u00e2\u0080\u0099ll talk about how to bring your data to the cloud. There are many ways to bring your data into cloud to power your machine learning models. We\u00e2\u0080\u0099ll first review why your data needs to be on the cloud to get the advantages of scale and using fully-managed services and what options you have to bring your data over.  \n In this module, we\u00e2\u0080\u0099ll learn how to recognize the ways that our model is dependent on our data, make cost-conscious engineering decisions, know when to roll back our models to earlier versions, debug the causes of observed model behavior and implement a pipeline that is immune to one type of dependency. In this module, you will learn how to identify performance considerations for machine learning models. \n\nMachine learning models are not all identical. For some models, you will be focused on improving I/O performance, and on others, you will be focused on squeezing out more computational speed.\n\n\n Understand the tools and systems available and when to leverage hybrid machine learning models. Review the content covered in the modules on Production ML systems", "Welcome to Linear Regression in R for Public Health!\n\nPublic Health has been defined as \u00e2\u0080\u009cthe art and science of preventing disease, prolonging life and promoting health through the organized efforts of society\u00e2\u0080\u009d. Knowing what causes disease and what makes it worse are clearly vital parts of this. This requires the development of statistical models that describe how patient and environmental factors affect our chances of getting ill. This course will show you how to create such models from scratch, beginning with introducing you to the concept of correlation and  linear regression before walking you through importing and examining your data, and then showing you how to fit models. Using the example of respiratory disease, these models will describe how patient and other factors affect outcomes such as lung function. \n\nLinear regression is one of a family of regression models, and the other courses in this series will cover two further members. Regression models have many things in common with each other, though the mathematical details differ. \nThis course will show you how to prepare the data, assess how well the model fits the data, and test its underlying assumptions \u00e2\u0080\u0093 vital tasks with any type of regression. \nYou will use the free and versatile software package R, used by statisticians and data scientists in academia, governments and industry worldwide. INTRODUCTION TO LINEAR REGRESSION Linear Regression in R Multiple Regression and Interaction MODEL BUILDING Before jumping ahead to run a regression model, you need to understand a related concept: correlation. This week you\u00e2\u0080\u0099ll learn what it means and how to generate Pearson\u00e2\u0080\u0099s and Spearman\u00e2\u0080\u0099s correlation coefficients in R to assess the strength of the association between a risk factor or predictor and the patient outcome. Then you\u00e2\u0080\u0099ll be introduced to linear regression and the concept of model assumptions, a key idea underpinning so much of statistical analysis. You\u00e2\u0080\u0099ll be introduced to the COPD data set that you\u00e2\u0080\u0099ll use throughout the course and will run basic descriptive analyses. You\u00e2\u0080\u0099ll also practise running correlations in R. Next, you\u00e2\u0080\u0099ll see how to run a linear regression model, firstly with one and then with several predictors, and examine whether model assumptions hold. Now you\u00e2\u0080\u0099ll see how to extend the linear regression model to include binary and categorical variables as predictors and learn how to check the correlation between predictors. Then you\u00e2\u0080\u0099ll see how predictors can interact with each other and how to incorporate the necessary interaction terms into the model and interpret them. Different kinds of interactions exist and can be challenging to interpret, so we will take it slowly with worked examples and opportunities to practise. The last part of the course looks at how to build a regression model when you have a choice of what predictors to include in it. It describes commonly used automated procedures for model building and shows you why they are so problematic. Lastly, you\u00e2\u0080\u0099ll have the chance to fit some models using a more defensible and robust approach.", "This course provides an introduction to Deep Learning, a field that aims to harness the enormous amounts of data that we are surrounded by with artificial neural networks, allowing for the development of self-driving cars, speech interfaces, genomic sequence analysis and algorithmic trading. \n\nYou will explore important concepts in Deep Learning, train deep networks using Intel Nervana Neon, apply Deep Learning to various applications and explore new and emerging Deep Learning topics. Introduction to Deep Learning and Deep Learning Basics Convolutional Neural Networks (CNN), Fine-Tuning and Detection Recurrent Neural Networks (RNN) Training Tips and Multinode Distributed Training Hot Research and Intel's Roadmap Final Quiz      ", "Biostatistics is an essential skill for every public health researcher because it provides a set of precise methods for extracting meaningful conclusions from data. In this second course of the Biostatistics in Public Health Specialization, you'll learn to evaluate sample variability and apply statistical hypothesis testing methods. Along the way, you'll perform calculations and interpret real-world data from the published scientific literature. Topics include sample statistics, the central limit theorem, confidence intervals, hypothesis testing, and p values. Sampling Distributions and Standard Errors Confidence Intervals for Single Population Parameters Confidence Intervals for Population Comparison Measures Two-Group Hypothesis Testing: The General Concept and Comparing Means Hypothesis Testing (Comparing Proportions and Incidence Rates Between Two Populations) & Extended Hypothesis Testing Project Within module one, you will learn about sample statistics, sampling distribution, and the central limit theorem. You will have the opportunity to test your knowledge with a practice quiz and, then, apply what you learned to the graded quiz.  Module two builds upon previous materials to discuss confidence intervals, the need for ample sizes of data, and ways to get around the need for ample sizes of data. The practice quiz helps you prepare for the graded quiz.   Within module three, confidence intervals are discussed at length and ratios are discussed again. Aside from the lectures, you will also be completing a practice quiz and graded quiz.   Within module four, you will look at statistical hypothesis tests, confidence intervals, and p-value. There is a practice quiz to prepare you for the graded quiz.   During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about asthma medication and the other about self-administration of injectable contraception. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken the Summarization and Measurement course, then this scenario will be familiar. ", "This course covers the theoretical foundation for different techniques associated with supervised machine learning models. In addition, a business case study is defined to guide participants through all steps of the analytical life cycle, from problem understanding to model deployment, through data preparation, feature selection, model training and validation, and model assessment. A series of demonstrations and exercises is used to reinforce the concepts and the analytical approach to solving business problems. \n\nThis course uses Model Studio, the pipeline flow interface in SAS Viya that enables you to prepare, develop, compare, and deploy advanced analytics models. You learn to train supervised machine learning models to make better decisions on big data. The SAS applications used in this course make machine learning possible without programming or coding. Course Overview Getting Started with Machine Learning using SAS\u00c2\u00ae  Viya\u00c2\u00ae Data Preparation and Algorithm Selection Decision Trees and Ensembles of Trees Neural Networks Support Vector Machines Model Deployment In this module, you meet the instructor and learn about course logistics, such as how to access the software for this course. In this module, you learn how you can meet today's business challenges with machine learning using SAS\u00c2\u00ae Viya\u00c2\u00ae. You start working on the project that runs throughout the course. In this module, you learn to explore the data and finish preparing the data for analysis. You also learn some general considerations for selecting an algorithm. In this module, you learn to build decision tree models as well as models based on ensembles, or combinations, of decision trees. In this module, you learn to build neural network models. In this module, you learn to build support vector machine models. In this module, you learn how to select the model that best meets the requirements of your business challenge and put the model into production. You also learn about managing the model over time.", "In this course, you'll apply your knowledge of classification models and embeddings to build a ML pipeline that functions as a recommendation engine.\n\n\u00e2\u0080\u00a2 Devise a content-based recommendation engine\n\u00e2\u0080\u00a2 Implement a collaborative filtering recommendation engine\n\u00e2\u0080\u00a2 Build a hybrid recommendation engine with user and content embeddings\n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Recommendation Systems Overview Content-Based Recommendation Systems COLLABORATIVE FILTERING RECOMMENDATION SYSTEMS Neural Networks for Recommendation Systems Building an End-to-End Recommendation System Summary In this module, we review the scope and plan for the course, define what recommendation systems are, review the different types of recommendation systems and discuss common problems that arise when developing recommendation systems. In this module, we demonstrate how to build a recommendation system using characteristics of the users and items. In this module, we show how the data of the interactions between users and items from many different users can be combined to improve the quality of predictions.\n In this module we show how various recommendation systems can be combined as part of a hybrid approach. In this module we put all the pieces together to build a smart end-to-end workflow for your newly built WALS recommendation model for news articles. In this final module, we review what you have learnt so far about recommendation systems and the specialization more broadly.", "Discover the basic concepts of cluster analysis, and then study a set of typical clustering methodologies, algorithms, and applications. This includes partitioning methods such as k-means, hierarchical methods such as BIRCH, and density-based methods such as DBSCAN/OPTICS. Moreover, learn methods for clustering validation and evaluation of clustering quality. Finally, see examples of cluster analysis in applications. Course Orientation Module 1 Week 2 Week 3 Week 4 Course Conclusion You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course.     In the course conclusion, feel free to share any thoughts you have on this course experience.", "In this course, you will learn how to find GIS data for your own projects, and how to create a well-designed map that effectively communicates your message. The first section focuses on the basic building blocks of GIS data, so that you know what types of GIS files exist, and the implications of choosing one type over another. Next, we'll discuss metadata (which is information about a data set) so you know how to evaluate a data set before you decide to use it, as well as preparing data by merging and clipping files as needed. We'll then talk about how to take non-GIS data, such as a list of addresses, and convert it into \"mappable\" data using geocoding. Finally, you'll learn about how to take data that you have found and design a map using cartographic principles. In the course project, you will find your own data and create your own quantitative map. GIS File Types, Data Models, and Topology Finding data and preparing it for your project Geocoding addresses and postal codes Map Design Principles Mapping Quantitative Data Quantitative Map Types Project: Getting Data and Making Your Own Map       ", "Big Data analytics tools are increasingly critical for providing meaningful information for making better business decisions.\nBig data technologies bring significant cost advantages when it comes to storing and managing large amounts of data. Understanding how to query a database to extract data will empower better analysis of large, complex datasets. Knowledge of Indexing mechanisms makes possible high-speed, selective retrieval of large amounts of information. Big Data and Data Processing Entity Relationship Model to Relational Model Relational Model and Relational Algebra Data Storage and Indexing Database management systems are critical to businesses and organizations. They provide an efficient method for handling different types of data in the era of big data. A university database, for example, stores millions of student and course records. The relational database management system ensures that the university can define entity relationships and logically maintain its data over time.  We live in an era where data is the critical factor in decision-making. To provide that critical factor, we must be able to extract data from database management systems. This week, you will learn how Entity Relationship (ER) models visually show the various entities (tables) and the relationships between them. You are also provided with an introduction to SQL, the standard language for relational database management systems. Databases use relational algebra operators to execute SQL queries; this week, you will learn about relational algebra as the mathematical query language for relations.  In the first part of this week, you will learn how indexes are important for looking up information (Imagine searching for your order in Amazon without an index!) and optimizing query performance. \n\nAs the week continues, we discuss transaction locks and database recovery. Transaction locks are critical to data consistency. All organizations think about recovery mechanisms for databases, and have some in place in the event of a crash. We close out this week with a discussion on database recovery.", "In this course, we will learn all the core techniques needed to make effective use of H2O. Even if you have no prior experience of machine learning, even if your math is weak, by the end of this course you will be able to make machine learning models using a variety of algorithms. We will be using linear models, random forest, GBMs and of course deep learning, as well as some unsupervised learning algorithms. You will also be able to evaluate your models and choose the best model to suit not just your data but the other business restraints you may be under. H2O AND THE FUNDAMENTALS Trees And Overfitting LINEAR MODELS AND MORE Deep Learning UNSUPERVISED LEARNING Everything Else!      ", "In this module, we define what Machine Learning is and how it can benefit your business. You'll see a few demos of ML in action and learn key ML terms like instances, features, and labels. In the interactive labs, you will practice invoking the pretrained ML APIs available as well as build your own Machine Learning models using just SQL with BigQuery ML.\n\nPREREQUISITES\nTo get the most out of this course, participants must complete the prior courses in this specialization:\n\u00e2\u0080\u00a2 Exploring and Preparing your Data\n\u00e2\u0080\u00a2 Storing and Visualizing your Data\n\u00e2\u0080\u00a2 Architecture and Performance\n\n>>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<\n\nCOMPLETION CHALLENGE\nComplete any GCP specialization from November 5 - November 30, 2019 for an opportunity to receive a GCP t-shirt (while supplies last). Check Discussion Forums for details. Introduction Introduction to Machine Learning Pre-trained ML APIs Creating \u00e2\u0080\u008bML Datasets in BigQuery Creating ML Models in BigQuery End of Course Recap Welcome to the last course in the Data Insights specialization. In this module we will overview the course topics and the labs platform you will be using.  In this module, we define what Machine Learning is and how it can benefit your business. You'll see a few demos of ML in action and learn key ML terms like instances, features, and labels. In this module we will dive into pre-built and pre-trained ML models that we can access (like image recognition and sentiment analysis) within Cloud Datalab.   In this module, you will learn how to create machine learning models directly inside of BigQuery. You will learn the new syntax and work through the phases of building, evaluating, and testing an ML model. You've made it to the end! Let's review the lessons learned in the course and what resources are available for continued learning.", "This course will provide you with an overview over existing data products and a good understanding of the data collection landscape. With the help of various examples you will learn how to identify which data sources likely matches your research question, how to turn your research question into measurable pieces, and how to think about an analysis plan. Furthermore this course will provide you with a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also help you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source. Finally we will introduce different large scale data collection efforts done by private industry and government agencies, and review the learned concepts through these examples. This course is suitable for beginners as well as those that know about one particular data source, but not others, and are looking for a general framework to evaluate data products. Research Designs and Data Sources Measurements and Analysis Plan Quality Framework Application of TSE Framework to Existing Surveys The first course in the specialization provides an overview of the topics to come. This module walks you through the process of data collection and analysis. Starting with a research question and a review of existing data sources, we cover survey data collection techniques, highlight the importance of data curation, and discuss some basic features that can affect your data analysis when dealing with sample data. Issues of data access and resources for access are introduced in this module.  In this module we will emphasize the importance of having a well-specified research question and analysis plan. We will provide an overview over the various data collection strategies, a variety of available modes for data collection and some thinking on how to choose the right mode.  In this module you will be introduced to a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also helps you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source. In this module we introduce a few surveys across a variety of topics. For each we highlight data collection features. The surveys span a variety of topics. We challenge you to think about alternative data sources that can be used to gather the same information or insights.", "What are the ethical considerations regarding the privacy and control of consumer information and big data, especially in the aftermath of recent large-scale data breaches?\n\nThis course provides a framework to analyze these concerns as you examine the ethical and privacy implications of collecting and managing big data. Explore the broader impact of the data science field on modern society and the principles of fairness, accountability and transparency as you gain a deeper understanding of the importance of a shared set of ethical values. You will examine the need for voluntary disclosure when leveraging metadata to inform basic algorithms and/or complex artificial intelligence systems while also learning best practices for responsible data management, understanding the significance of the Fair Information Practices Principles Act and the laws concerning the \"right to be forgotten.\"\n\nThis course will help you answer questions such as who owns data, how do we value privacy, how to receive informed consent and what it means to be fair.\n\nData scientists and anyone beginning to use or expand their use of data will benefit from this course. No particular previous knowledge needed. What are Ethics? History, Concept of Informed Consent Data Ownership Privacy Anonymity Data Validity Algorithmic Fairness Societal Consequences Code of Ethics Attributions Module 1 of this course establishes a basic foundation in the notion of simple utilitarian ethics we use for this course. The lecture material and the quiz questions are designed to get most people to come to an agreement about right and wrong, using the utilitarian framework taught here. If you bring your own moral sense to bear, or think hard about possible counter-arguments, it is likely that you can arrive at a different conclusion. But that discussion is not what this course is about. So resist that temptation, so that we can jointly lay a common foundation for the rest of this course. Early experiments on human subjects were by scientists intent on advancing medicine, to the benefit of all humanity, disregard for welfare of individual human subjects. Often these were performed by white scientists, on black subject. In this module we will talk about the laws that govern the Principle of Informed Consent. We will also discuss why informed consent doesn\u00e2\u0080\u0099t work well for retrospective studies, or for the customers of electronic businesses. Who owns data about you? We'll explore that question in this module. A few examples of personal data include copyrights for biographies; ownership of photos posted online, Yelp, Trip Advisor, public data capture, and data sale. We'll also explore the limits on recording and use of data.  Privacy is a basic human need. Privacy means the ability to control information about yourself, not necessarily the ability to hide things. We have seen the rise different value systems with regards to privacy. Kids today are more likely to share personal information on social media, for example. So while values are changing, this doesn\u00e2\u0080\u0099t remove the fundamental need to be able to control personal information. In this module we'll examine the relationship between the services we are provided and the data we provide in exchange: for example, the location for a cell phone. We'll also compare and contrast \"data\" against \"metadata\". Certain transactions can be performed anonymously. But many cannot, including where there is physical delivery of product. Two examples related to anonymous transactions we'll look at are \"block chains\" and \"bitcoin\". We'll also look at some of the drawbacks that come with anonymity. Data validity is not a new concern. All too often, we see the inappropriate use of Data Science methods leading to erroneous conclusions. This module points out common errors, in language suited for a student with limited exposure to statistics. We'll focus on the notion of representative sample: opinionated customers, for example, are not necessarily representative of all customers. What could be fairer than a data-driven analysis? Surely the dumb computer cannot harbor prejudice or stereotypes. While indeed the analysis technique may be completely neutral, given the assumptions, the model, the training data, and so forth, all of these boundary conditions are set by humans, who may reflect their biases in the analysis result, possibly without even intending to do so. Only recently have people begun to think about how algorithmic decisions can be unfair. Consider this article, published in the New York Times. This module discusses this cutting edge issue. In Module 8, we consider societal consequences of Data Science that we should be concerned about even if there are no issues with fairness, validity, anonymity, privacy, ownership or human subjects research. These \u00e2\u0080\u009csystemic\u00e2\u0080\u009d concerns are often the hardest to address, yet just as important as other issues discussed before. For example, we consider ossification, or the tendency of algorithmic methods to learn and codify the current state of the world and thereby make it harder to change. Information asymmetry has long been exploited for the advantage of some, to the disadvantage of others. Information technology makes spread of information easier, and hence generally decreases asymmetry. However, Big Data sets and sophisticated analyses increase asymmetry in favor of those with ability to acquire/access.  Finally, in Module 9, we tie all the issues we have considered together into a simple, two-point code of ethics for the practitioner. This module contains lists of attributions for the external audio-visual resources used throughout the course.", "Welcome to Data Analytics Foundations for Accountancy I! You\u00e2\u0080\u0099re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.\n\nTo begin, I recommend taking a few minutes to explore the course site. Review the material we\u00e2\u0080\u0099ll cover each week, and preview the assignments you\u00e2\u0080\u0099ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.\n\nIf you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.\n\nGood luck as you get started, and I hope you enjoy the course! Course Orientation Module 1: Foundations Module 2: Introduction to Python Module 3: Introduction to Data Analysis Module 4: Statistical Data Analysis Module 5: Introduction to Visualization Module 6: Introduction to Probability Module 7: Exploring Two-Dimensional Data Module 8: Introduction to Density Estimation You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. This module serves as the introduction to the course content and the course Jupyter server, where you will run your analytics scripts. First, you will read about specific examples of how analytics is being employed by Accounting firms. Next, you will learn about the capabilities of the course Jupyter server, and how to create, edit, and run notebooks on the course server. After this, you will learn how to write Markdown formatted documents, which is an easy way to quickly write formatted text, including descriptive text inside a course notebook. Finally, you will begin learning about Python, the programming language used in this course for data analytics. This module focuses on the basic features in the Python programming language that underlie most data analytics scripts. First, you will read about why accounting students should learn to write computer programs. Second, you will learn about basic data structures commonly used in Python programs. Third, you will learn how to write functions, which can be repeatedly called, in Python, and how to use them effectively in your own programs. Finally, you will learn how to control the execution process of your Python program by using conditional statements and looping constructs. At the conclusion of this module, you will be able to write Python scripts to perform basic data analytic tasks. This module introduces fundamental concepts in data analysis. First, you will read a report from the Association of Accountants and Financial Professionals in Business that explores Big Data in Accountancy. Next, you will learn about the Unix file system, which is the operating system used for most big data processing (as well as Linux and Mac OSX desktops and many mobile phones). Second, you will learn how to read and write data to a file from within a Python program. Finally, you will learn about the Pandas Python module that can simplify many challenging data analysis tasks, and includes the DataFrame, which programmatically mimics many of the features of a traditional spreadsheet. This module introduces fundamental concepts in data analysis. First, you will read about how to perform many basic tasks in Excel by using the Pandas module in Python. Second, you will learn about the Numpy module, which provides support for fast numerical operations within Python. This module will focus on using Numpy with one-dimensional data (i.e., vectors or 1-D arrays), but a later module will explore using Numpy for higher-dimensional data. Third, you will learn about descriptive statistics, which can be used to characterize a data set by using a few specific measurements. Finally, you will learn about advanced functionality within the Pandas module including masking, grouping, stacking, and pivot tables. This module introduces visualization as an important tool for exploring and understanding data. First, the basic components of visualizations are introduced with an emphasis on how they can be used to convey information. Also, you will learn how to identify and avoid ways that a visualization can mislead or confuse a viewer. Next, you will learn more about conveying information to a user visually, including the use of form, color, and location. Third, you will learn how to actually create a simple visualization (basic line plot) in Python, which will introduce creating and displaying a visualization within a notebook, how to annotate a plot, and how to improve the visual aesthetics of a plot by using the Seaborn module. Finally, you will learn how to explore a one-dimensional data set by using rug plots, box plots, and histograms. In this Module, you will learn the basics of probability, and how it relates to statistical data analysis. First, you will learn about the basic concepts of probability, including random variables, the calculation of simple probabilities, and several theoretical distributions that commonly occur in discussions of probability. Next, you will learn about conditional probability and Bayes theorem. Third, you will learn to calculate probabilities and to apply Bayes theorem directly by using Python. Finally, you will learn to work with both empirical and theoretical distributions in Python, and how to model an empirical data set by using a theoretical distribution. This modules extends what you have learned in previous modules to the visual and analytic exploration of two-dimensional data. First, you will learn how to make two-dimensional scatter plots in Python and how they can be used to graphically identify a correlation and outlier points. Second, you will learn how to work with two-dimensional data by using the Numpy module, including a discussion on analytically quantifying correlations in data. Third, you will read about statistical issues that can impact understanding multi-dimensional data, which will allow you to avoid them in the future. Finally, you will learn about ordinary linear regression and how this technique can be used to model the relationship between two variables. Often, as part of exploratory data analysis, a histogram is used to understand how data are distributed, and in fact this technique can be used to compute a probability mass function (or PMF) from a data set as was shown in an earlier module. However, the binning approach has issues, including a dependance on the number and width of the bins used to compute the histogram. One approach to overcome these issues is to fit a function to the binned data, which is known as parametric estimation. Alternatively, we can construct an approximation to the data by employing a non-parametric density estimation. The most commonly used non-parametric technique is kernel density estimation (or KDE). In this module, you will learn about density estimation and specifically how to employ KDE. One often overlooked aspect of density estimation is the model representation that is generated for the data, which can be used to emulate new data. This concept is demonstrated by applying density estimation to images of handwritten digits, and sampling from the resulting model.", "This course (The English copy of \"\u00e7\u0094\u00a8Python\u00e7\u008e\u00a9\u00e8\u00bd\u00ac\u00e6\u0095\u00b0\u00e6\u008d\u00ae\" <https://www.coursera.org/learn/hipython/home/welcome>)  is mainly for non-computer majors. It starts with the basic syntax of Python, to how to acquire data in Python locally and from network, to how to present data, then to how to conduct basic and advanced statistic analysis and visualization of data, and finally to how to design a simple GUI to present and process data, advancing level by level. \n\nThis course, as a whole, based on Finance data and through the establishment of popular cases one after another, enables learners to more vividly feel the simplicity, elegance, and robustness of Python. Also, it discusses the fast, convenient and efficient data processing capacity of Python in humanities and social sciences fields like literature, sociology and journalism and science and engineering fields like mathematics and biology, in addition to business fields. Similarly, it may also be flexibly applied into other fields.\n\nThe course has been updated. Updates in the new version are : \n\n1) the whole course has moved from Python 2.x to Python 3.x \n2) Added manual webpage fetching and parsing. Web API is also added. \n3) Improve the content order and enrich details of some content especially for some practice projects.\n\nNote: videos are in Chinese (Simplified) with English subtitles. All other materials are in English. Welcome to learn Data Processing Using Python! Basics of Python Data Acquisition and Presentation Powerful Data Structures and Python Extension Libraries Python Data Statistics and Visualization Object Orientation and Graphical User Interface Hi, guys, welcome to learn \u00e2\u0080\u009cData Processing Using Python\u00e2\u0080\u009d(The English version of \"\u00e7\u0094\u00a8Python\u00e7\u008e\u00a9\u00e8\u00bd\u00ac\u00e6\u0095\u00b0\u00e6\u008d\u00ae\", url is https://www.coursera.org/learn/hipython/home/welcome)!In this course, I tell in a manner that enables non-computer majors to understand how to utilize this simple and easy programming language \u00e2\u0080\u0093 Python to rapidly acquire, express, analyze and present data based on SciPy, Requests, Beautiful Soup libraries etc. Many cases are provided to enable you to easily and happily learn how to use Python to process data in many fields. \u00e3\u0080\u0090Nov 18, 2019 @ @ @ @ @ @ @ Hi, all! The content is planned to be updated in the last two months. This update is relatively large, including practical operation and explanation of Python based cases, vector operation and broadcast ideas of numpy package and common applications, multiple links of data exploration and preprocessing (including in module 4), data analysis and data mining cases based on pandas, some of which are directly modified on the original video Some of them are presented in the form of expanded videos, especially the newly recorded videos, which have a lot of content to say, take a long time, and will be a little hard to learn. Come on!  Note: The updated content will not affect the learners who are studying in this round(by Dec 2). Module 4 with some completely new concepts will be updated after Dec 2.\u00e3\u0080\u0091 Hi, guys, welcome to learn Module 01 \u00e2\u0080\u009cBasics of Python\u00e2\u0080\u009d! I\u00e2\u0080\u0099ll first guide you to have a glimpse of its simplicity for learning as well as elegance and robustness. Less is more: the author of Python must know this idea well. After learning this module, you can master the basic language structures, data types, basic operations, conditions, loops, functions and modules in Python. With them, we can write some useful programs!  Welcome to learn Module 02 \u00e2\u0080\u009cData Acquisition and Presentation\u00e2\u0080\u009d! After learning this module, you can master the modes of acquiring local data and network data in Python and use the basic and yet very powerful data structure sequence, string, list and tuple in Python to fast and effectively present data and simply process data.  Welcome to learn Module 03 \u00e2\u0080\u009cPowerful Data Structures and Python Extension Libraries\u00e2\u0080\u009d! Have you felt you are closer to using Python to process data? After learning this module, you can master the intermediate-level and advanced uses of Python: data structure dictionaries and sets. In some applications, they can be very convenient. What\u00e2\u0080\u0099s special here is that, you can also feel the charm of such concise and efficient data structures: ndarray, Series and DataFrame in the most famous and widely applied scientific computing package SciPy in Python.  Welcome to learn Module 04 \u00e2\u0080\u009cPython Data Statistics and Visualization\u00e2\u0080\u009d! In this module, I will show you, over the entire process of data processing, the unique advantages of Python in data processing and analysis, and use many cases familiar to and loved by us to learn about and master methods and characteristics. After learning this module, you can fast and effectively mine your desired or expected or unknown results from a large amount of data, and can also present those data in various images. In addition, the data statistics modes of all third party packages in Python are extraordinarily and surprisingly strong, but we, as average persons, can still understand and possess them.  Welcome to Module 05 \u00e2\u0080\u009cObject Orientation and Graphical User Interface\u00e2\u0080\u009d! In this module, I will guide you to understand what object orientation is and the relationship between graphical user interface and object orientation. Learners are only required to understand the concepts so that you can more freely and easily pick up various new functions in future. No program writing is required here. Besides, you also need to master the basic framework of GUI, common components and layout management. After learning them, you will find development with GUI is actually not remote. It has an Easter egg, too ~~~ ", "Learn how probability, math, and statistics can be used to help baseball, football and basketball teams improve, player and lineup selection as well as in game strategy. Before you start... Module 1 Module 2 Module 3 Module 4 Module 5 Module 6 Module 7 Module 8 Module 9 Module 10 Final Exam  You will learn how to predict a team\u00e2\u0080\u0099s won loss record from the number of runs, points, or goals scored by a team and its opponents. Then we will introduce you to multiple regression and show how multiple regression is used to evaluate baseball hitters. Excel data tables, VLOOKUP, MATCH, and INDEX functions will be discussed.   You will concentrate on learning important Excel tools including Range Names, Tables, Conditional Formatting, PivotTables, and the family of COUNTIFS, SUMIFS, and AVERAGEIFS functions.  You will concentrate on learning important Excel tools including Range Names, Tables, Conditional Formatting, PivotTables, and the family of COUNTIFS, SUMIFS, and AVERAGEIFS functions.   You will learn how Monte Carlo simulation works and how it can be used to evaluate a baseball team\u00e2\u0080\u0099s offense and the famous DEFLATEGATE controversy. You will learn how to evaluate baseball fielding, baseball pitchers, and evaluate in game baseball decision-making. The math behind WAR (Wins above Replacement) and Park Factors will also be discussed. Modern developments such as infield shifts and pitch framing will also be discussed. You will learn basic concepts involving random variables (specifically the normal random variable, expected value, variance and standard deviation.) You will learn how regression can be used to analyze what makes NFL teams win and decode the NFL QB rating system. You will also learn that momentum and the \u00e2\u0080\u009chot hand\u00e2\u0080\u009d is mostly a myth. Finally, you will use Excel text functions and the concept of Expected Points per play to analyze the effectiveness of a football team\u00e2\u0080\u0099s play calling. You will learn how two-person zero sum game theory sheds light on football play selection and soccer penalty kick strategies. Our discussion of basketball begins with an analysis of NBA shooting, box score based player metrics, and the Four Factor concept which explains what makes basketball teams win. You will learn about advanced basketball concepts such as Adjusted plus minus, ESPN\u00e2\u0080\u0099s RPM, SportVu data, and NBA in game decision-making. You will learn how to use game results to rate sports teams and set point spreads. Simulation of the NCAA basketball tournament will aid you in filling out your 2016 bracket. Final 4 is in Houston! You will learn how to rate NASCAR drivers and get an introduction to sports betting concepts such as the Money line, Props Bets, and evaluation of gambling betting systems. You will learn how Kelly Growth can optimize your sports betting, how regression to the mean explains the SI cover jinx and how to optimize a daily fantasy sports lineup. We close with a discussion of golf analytics. Final exam has 10 questions.  Please download and open Excel files before taking the exam.  You will be referred to Excel files during the exam.  Each question is wort 1 point.  You need to answer 6 questions or more correctly to pass the exam.", "This course is organized into two parts presenting the theoretical and practical foundations of geographic information systems (GIS).\n- Together theses courses constitute an introduction to GIS and require no prior knowledge.\n- By following this introduction to GIS you will quickly acquire the basic knowledge required to create spatial databases and produce high-quality maps and cartographic representations.\n- This is a practical course and is based on free, open-source software, including QGIS.\nIf you study or work in the fields of land management or the analysis of geographically distributed objects such as land use planning, biology, public health, ecology, or energy, then this course is for you!\n\nIn this first part of the course, we will focus on the digitization and the storage of geodata. In particular, you will learn:\n- To characterize spatial objects and/or phenomena (territory modeling) with respect to their position in space (through coordinate systems, projections, and spatial relationships) and according to their intrinsic nature (object/vector mode vs. Image/raster mode); \n- About the different means used to acquire spatial data; including direct measurement, georeferencing images, digitization, existing data source, etc.);\n- About the different ways in which geodata can be stored - notably, files and relational databases;\n- How to use data modeling tools to describe and create a spatial database;\n- To query and analyze data using SQL, a common data manipulation language.\n\nThe second part of this course will focus on methods of spatial analysis and geodata representation. In this section, you will learn:\n- How to describe and quantify the spatial properties of discrete variables, for example through spatial autocorrelation;\n- To work with continuous variables. In particular, we will look at sampling strategies, how to construct contour lines and isovalue curves, and we will explore different interpolation methods;\n- To use digital elevation models and create their derivative products (i.e. slope, orientation);\n- How to evaluate the interaction between different types of geodata through overlay and interaction techniques;\n- How to create effective maps based around the rules of graphic semiology;\n- Finally, we will also explore other, increasingly common, forms of spatial representation such as interactive web-mapping and 3D representations.\n\nYou can find an interactive forum for course participants on our Facebook page: https://www.facebook.com/moocsig Digitization \u00e2\u0080\u0093 Territorial Modeling: Spatial elements and the characteristics Digitization - Geodata Capture and Documentation Digitization - Automated Capture and Use of Existing Geodata Storage - Geodata Structure and Organization  Storage - Data Management with SQL Storage - Spatial SQL and NoSQL Databases This first week deals with the first step in digitizing terrain, namely territorial modeling. In this week, we will consider factors such as the scale and theme of interest in order to determine which objects or spatial phenomena should included in the model, and we will also see how the geographic positioning and intrinsic nature (e.g. raster or vector) of these elements factors into how they are characterized in a terrain model.  Digital data acquisition involves various techniques including the direct measurement of primary data, the semi-automated vectorization and digitization of spatial objects, or the georeferencing of digital images. In this week\u00e2\u0080\u0099s module we will begin with a lesson on metadata in which we will discuss the processes and rules for documenting a dataset, which are essential for data sustainability, and we will also introduce a case study on participatory GIS in Senegal and Seychelles. In this week we will continue to build on the topic introduced last week with automatic vectorization, and we will also review a non-exhaustive list of some important pre-existent data sources that are available for you to access. We will finish with a case study of a Senegal-Mauritania biodiversity project before you will test your knowledge in the first quiz of the module.   In this 4th week, which marks the beginning of the second module of the course devoted to data storage, we begin by reviewing the fundamental aspects of geodata storage and the most common data formats, before tackling the theme of relational databases and data modeling. The week concludes with a lesson on creating databases in the QGIS environment and a case study on the role of GIS in a transport and urban planning project in Senegal. The SQL language is the preferred vector for access to relational databases, and can be used to search for data meeting certain criteria (conditional queries), to aggregate and calculate statistics on subsets of data (aggregation queries), to combine the results of several queries (nesting and merging), to edit and modify data, or even to manipulate data structure (DDL, DML). While the previous week's lessons dealt with various general aspects of the SQL language, in this week we will learn about SQL queries specifically related to the spatial dimensions and the relationships that characterize geodata (geometric and topological spatial queries). We will also present a brief introduction into the rapidly expanding field of noSQL databases and finish the lesson portion of this module with a case study devoted to bushfire management in sub-Saharan Africa. Finally, this first MOOC on an introduction to geographic information systems will conclude with a second quiz to test your knowledge.", "The Code Free Data Science class is designed for learners seeking to gain or expand their knowledge in the area of Data Science.  Participants will receive the basic training in effective predictive analytic approaches accompanying the growing discipline of Data Science without any programming requirements.  Machine Learning methods will be presented by utilizing the KNIME Analytics Platform to discover patterns and relationships in data. Predicting future trends and behaviors allows for proactive, data-driven decisions.  During the class learners will acquire new skills to apply predictive algorithms to real data, evaluate, validate and interpret the results without any pre requisites for any kind of programming.  Participants will gain the essential skills to design, build, verify and test predictive models.  \nYou Will Learn\n\u00e2\u0080\u00a2\tHow to design Data Science workflows without any programming involved\n\u00e2\u0080\u00a2\tEssential Data Science skills to design, build, test and evaluate predictive models\n\u00e2\u0080\u00a2\tData Manipulation, preparation and Classification and clustering methods\n\u00e2\u0080\u00a2\tWays to apply Data Science algorithms to real data and evaluate and interpret the results Welcome to the world of Big Data Introduction to KNIME Analytics Platform Data Manipulation and Visualization Machine Learning Welcome to the first module of the Code Free Data Science course. This first module will provide insight into Big Data Hype, its technologies opportunities and challenges.  We will take a deeper look into the Big Data Analytics  and methodology associated with Data Science approaches. This module will introduce the KNIME analytics platform.  Learners will be guided to download, install and setup KNIME.  We will explore and become familiar with the KNIME workflow editor and its components.  In this module we will create the very first basic workflow, and explore the kinds of analysis KNIME empowers users to perform.  ", "Welcome! In this course learners will develop expertise in basic magnetic resonance imaging (MRI) physics and principles and gain knowledge of many different data acquisition strategies in MRI. In particular, learners will get to know what is magnetic resonance phenomenon, how magnetic resonance signals are generated, how an image can be formulated using MRI, how soft tissue contrast can change with imaging parameters. Also introduced will be MR imaging sequences of spin echo, gradient echo, fast spin echo, echo planar imaging, inversion recovery, etc. 1 Overview of MRI  2 Mag Reson Phenomenon 3 Signal Processing Theory 4 MR IMAGE FORMATION 5 IMAGE CONTRAST, FIELD OF VIEW, AND RESOLUTION 6 MRI DATA ACQUISITION STRATEGIES Welcome! This first week you will be introduced to the course and key principles of mri. You will learn what is mri and the major components of mri. This week, you will explore what is magnetic resonance phenomeon. We will also guide you to what is relaxation and how RF apply to cause magnetic resonance. In this module, you will learn signal processing theory. You will be exposed to the most important concepts in mri which contain fourier transform and nyquist sampling therom. In lecture 4, we will expose you to how to sequence gradient pulses to get an mr image. You will also learn about sequence gradient pulses which include slice selection, frequency encoding and phase encoding. In this week, learners will get to know how soft tissue contrast can change with imaging parameters. We will also guide you what is field of view and how to measure resolution. In this module, you will learn about mri data acquisition strategies. Also, you will be exposed to MR imaging sequences of spin echo, gradient echo, fast spin echo, echo planar imaging, and inversion recovery.", "Learn how advances in geospatial technology and analytical methods have changed how we do everything, and discover how to make maps and analyze geographic patterns using the latest tools.\n\nThe past decade has seen an explosion of new mechanisms for understanding and using location information in widely-accessible technologies. This Geospatial Revolution has resulted in the development of consumer GPS tools, interactive web maps, and location-aware mobile devices. These radical advances are making it possible for people from all walks of life to use, collect, and understand spatial information like never before.\n \nThis course brings together core concepts in cartography, geographic information systems, and spatial thinking with real-world examples to provide the fundamentals necessary to engage with Geography beyond the surface-level. We will explore what makes spatial information special, how spatial data is created, how spatial analysis is conducted, and how to design maps so that they\u00e2\u0080\u0099re effective at telling the stories we wish to share. To gain experience using this knowledge, we will work with the latest mapping and analysis software to explore geographic problems. Getting Started The Changing Nature of Place Spatial is Special Understanding Spatial Data Doing Spatial Analysis Making Great Maps Watch a short video to learn how this class works (it's a bit different than other MOOCs) and help make a map with your classmates. Discover the Geospatial Revolution and its impact on the rapidly evolving science of Geography. Explore what it means to think spatially and consider the impacts of scale and time. Examine the key elements of spatial datasets. Learn how Geographers solve problems using spatial analysis techniques. Explore the key elements of effective cartographic design.", "In this course we will learn about Recommender Systems (which we will study for the Capstone project), and also look at deployment issues for data products. By the end of this course, you should be able to implement a working recommender system (e.g. to predict ratings, or generate lists of related products), and you should understand the tools and techniques required to deploy such a working system on real-world, large-scale datasets.\n\nThis course is the final course in the Python Data Products for Predictive Analytics Specialization, building on the previous three courses (Basic Data Processing and Visualization, Design Thinking and Predictive Analytics for Data Products, and Meaningful Predictive Modeling). At each step in the specialization, you will gain hands-on experience in data manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization. Introduction Implementing Recommender Systems Deploying Recommender Systems Project 4: Recommender System Capstone Welcome to the first week of Deploying Machine Learning Models! We will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of recommender systems and differentiate it from other types of machine learning This week, we will learn how to implement a similarity-based recommender, returning predictions similar to an user's given item. We will cover how to optimize these models based on gradient descent and Jaccard similarity. This week, we will learn about Python web server frameworks and the overall structure of interactive Python data applications. We will also cover some tips for best practices on deploying and monitoring your applications. For this final project, you will build a recommender system of your own. Find a dataset, clean it, and create a predictive system from the dataset. This will help prepare you for the upcoming capstone, where you will harness your skills from all courses of this specialization into one single project! Time to put all your hard work to the test! This capstone project consists of four components, each drawing from a separate course in this specialization. It's time to show off everything you've learned from this specialization.", "The explosion in digital media - web, social and now mobile - represents a departure from how things were like in the last century. This proliferation of digital media is both a threat and an opportunity for many businesses. Business Analytics can be leveraged to process data, sentiment, buzz, contacts, context and other aspects of business interest in real time, for business performance and impact. The course picks and uses use-cases from a variety of industries and geographies, to showcase the potential and impact that business analytics done properly (or not) can have on business performance. Introduction to Business Analytics Toolscape Customer Analytics  Digital Media  An overview of the what and the why\nLearning outcomes by the end of week 1 should normatively be that (a) Students grasp what is business analytics from the perspective of a business manager, (b) identify areas of interest, overlap, co-ordination and conflict with other business functions and processes in the firm, (c) and, develop an appreciation for the value of data, of analyses and of the components of analytics.\n An overview of the broad tools available for business analytics and the leveraging of digital media. \nLearning outcomes by the end of week 2 should normatively be that (a) Students have an understanding of the broad classes of analytics tools and platforms that currently dominate the market, (b) and, develop an appreciation for the pros and cons of the major groups of tools. \n Introduction to and the application of some important analytical processes in Marketing Analytics\nLearning outcomes by the end of week 3 should normatively be that  (a) Students have an understanding of the major processes and procedures typically used in a customer analytics setting, in particular factor and cluster analyses (b) and, develop an appreciation for the possibilities that emerge from recombining procedures, data, algorithms and problem formulation perspectives in open source environments.\n An overview of the big questions, possibilities and challenges.\nLearning outcomes by the end of week 4 should normatively be that \n(a) Students have an understanding of the major types of digital media in use currently by people and firms,  (b) and, develop an appreciation for the types of problem solving, data collection, prediction and optimization that can be enabled using digital media tools. \n", "Learn to use tools from the Bioconductor project to perform analysis of genomic data. This is the fifth course in the Genomic Big Data Specialization from Johns Hopkins University. Week One Week Two Week Three Week Four The class will cover how to install and use Bioconductor software. We will discuss common data structures, including ExpressionSets, SummarizedExperiment and GRanges used across several types of analyses. In this week we will learn how to represent and compute on biological sequences, both at the whole-genome level and at the level of millions of short reads.  In this week we will cover Basic Data Types, ExpressionSet, biomaRt, and R S4. In this week, we will cover Getting data in Bioconductor, Rsamtools, oligo, limma, and minfi", "The course will begin with what is familiar to many business managers and those who have taken the first two courses in this specialization. The first set of tools will explore data description, statistical inference, and regression. We will\u00c2\u00a0extend these concepts to other statistical methods used for prediction when the response variable is categorical such as win-don\u00e2\u0080\u0099t win an auction. In the next segment, students will learn about tools used for identifying important features in the dataset that can either reduce the complexity\u00c2\u00a0or\u00c2\u00a0help identify important features of the data or further help explain behavior.\u00c2\u00a0 Module 0:  Get Ready & Module 1: Introduction to Analytics and Evolution of Statistical Inference Module 2: Dating with Data Module 3: Model Development and Testing with Holdout Data Module 4: Curse of Dimensionality This session is an overview of the business data analytics\u00c2\u00a0process\u00c2\u00a0and its components.\u00c2\u00a0We introduce you to different modeling paradigms\u00c2\u00a0and invite you to match problems to modeling paradigms.\u00c2\u00a0The module concludes with an overview of Rattle (an interface for the statistical package R) and its use for univariate analysis.\u00c2\u00a0 This session\u00c2\u00a0focuses on identifying\u00c2\u00a0relationships between dependent and independent variables\u00c2\u00a0using a regression model. The goal is to find\u00c2\u00a0the best\u00c2\u00a0fitted\u00c2\u00a0model to the data to learn about the underlying relationship of variables in\u00c2\u00a0the\u00c2\u00a0population.\u00c2\u00a0\u00c2\u00a0 This session introduces the student to use of a holdout data set for evaluating model performance.\u00c2\u00a0Methods of improving the model are discussed with emphasis on variable selection. Nuances of modeling discrete predictor\u00c2\u00a0variables\u00c2\u00a0and response variables\u00c2\u00a0are discussed.\u00c2\u00a0 There has been a tremendous increase in the way data generation via sensors, digital platforms, user-generated content, etc. are being used in the industry. For example, sensors continuously record data and store it for analysis at a later point. In the way data gets captured, there can be a lot of redundancy. With more variables, comes more trouble! There may be very little (or no) incremental information gained from these sources. This is the problem of a high number of unwanted dimensions. To avoid this pitfall, data\u00c2\u00a0transformation\u00c2\u00a0and dimension reduction comes to the rescue by examining and extracting\u00c2\u00a0fewer\u00c2\u00a0dimensions\u00c2\u00a0while\u00c2\u00a0ensuring that it conveys\u00c2\u00a0the full\u00c2\u00a0information concisely.\u00c2\u00a0", "Welcome to Logistic Regression in R for Public Health! \n\nWhy logistic regression for public health rather than just logistic regression? Well, there are some particular considerations for every data set, and public health data sets have particular features that need special attention. In a word, they're messy. Like the others in the series, this is a hands-on course, giving you plenty of practice with R on real-life, messy data, with predicting who has diabetes from a set of patient characteristics as the worked example for this course. Additionally, the interpretation of the outputs from the regression model can differ depending on the perspective that you take, and public health doesn\u00e2\u0080\u0099t just take the perspective of an individual patient but must also consider the population angle. That said, much of what is covered in this course is true for logistic regression when applied to any data set, so you will be able to apply the principles of this course to logistic regression more broadly too. \n\nBy the end of this course, you will be able to: \nExplain when it is valid to use logistic regression \nDefine odds and odds ratios \nRun simple and multiple logistic regression analysis in R and interpret the output \nEvaluate the model assumptions for multiple logistic regression in R \nDescribe and compare some common ways to choose a multiple regression model \n\nThis course builds on skills such as hypothesis testing, p values, and how to use R, which are covered in the first two courses of the Statistics for Public Health specialisation. If you are unfamiliar with these skills, we suggest you review Statistical Thinking for Public Health and Linear Regression for Public Health before beginning this course. If you are already familiar with these skills, we are confident that you will enjoy furthering your knowledge and skills in Statistics for Public Health: Logistic Regression for Public Health. \n\nWe hope you enjoy the course! Introduction to Logistic Regression Logistic Regression in R Running Multiple Logistic Regression in R Assessing Model Fit Welcome to Statistics for Public Health: Logistic Regression for Public Health! In this week, you will be introduced to logistic regression and its uses in public health. We will focus on why linear regression does not work with binary outcomes and on odds and odds ratios, and you will finish the week by practising your new skills. By the end of this week, you will be able to explain when it is valid to use logistic regression, and define odds and odds ratios. Good luck! In this week, you will learn how to prepare data for logistic regression, how to describe data in R, how to run a simple logistic regression model in R, and how to interpret the output. You will also have the opportunity to practise your new skills. By the end of this week, you will be able to run simple logistic regression analysis in R and interpret the output. Good luck!  Now that you're happy with including one predictor in the model, this week you'll learn how to run multiple logistic regression, including describing and preparing your data and running new logistic regression models. You will have the opportunity to practise your new skills. By the end of the week, you will be able to run multiple logistic regression analysis in R and interpret the output. Good luck! Welcome to the final week of the course! In this week, you will learn how to assess model fit and model performance, how to avoid the problem of overfitting, and how to choose what variables from your data set should go into your multiple regression model. You will put all the skills you have learned throughout the course into practice. By the end of this week, you will be able to evaluate the model assumptions for multiple logistic regression in R, and describe and compare some common ways to choose a multiple regression model. Good luck! ", "This course will help lay the foundation of your healthcare data journey and provide you with knowledge and skills necessary to work in the healthcare industry as a data scientist. Healthcare is unique because it is associated with continually evolving and complex processes associated with health management and medical care. We'll learn about the many facets to consider in healthcare and determine the value and growing need for data analysts in healthcare. We'll learn about the Triple Aim and other data-enabled healthcare drivers. We'll cover different concepts and categories of healthcare data and describe how ontologies and related terms such as taxonomy and terminology organize concepts and facilitate computation. We'll discuss the common clinical representations of data in healthcare systems, including ICD-10, SNOMED, LOINC, drug vocabularies (e.g., RxNorm), and clinical data standards. We\u00e2\u0080\u0099ll discuss the various types of healthcare data and assess the complexity that occurs as you work with pulling in all the different types of data to aid in decisions. We will analyze various types and sources of healthcare data, including clinical, operational claims, and patient generated data as well as differentiate unstructured, semi-structured and structured data within health data contexts. We'll examine the inner workings of data and conceptual harmony  offer some solutions to the data integration problem by defining some important concepts, methods, and applications that are important to this domain. Healthcare 101 Concepts and Categories Healthcare Data Data and Conceptual Harmony In this module, you will be able to identify how biological and social systems are features of human well-being and health. You'll be able to describe important organizations in the US healthcare system and be able to discuss specific examples that document high cost and possible waste in the US healthcare system. You'll be able to identify and discuss the knowing-doing gap and be able to describe  evidence-based efforts to transform fragmented care processes into coordinated patient-centered activities. In this module, you will be able to compare forms of communication and describe why people us ontologies to describe the world. You'll be able to describe the evolution of standardized railroads in the US and recognize why the evolution of railroad tracks also applies to medical terminologies. You'll be able to analyze a dataset with disease codes and also be able to select which codes refer to specific diseases. You'll be able to match different terminologies with different descriptive domains as well as be able to contrast the different ways of organizing information into hierarchies or other categories. In this module, you will be able to identify different types of medical processes and be able to explain why specific data formats emerged from these varied processes. You'll be able to list numerous data types that are found within EHRs and link specific clinical processes that created these outputs. You'll be able to trace why various types of administrative data are collected and describe the value of this data for analytics. You'll be able to identify the common ways that gene sequences are stored in computer readable files and be able to describe how big data formats are different than common relational database technologies that require a lot of data modeling and planning. In this module, you will be able to tell leaders and coworkers why they should invest time in creating data dictionaries and other meta-data. You'll be able to describe why one burn registry had data fragmentation issues, and how a variety of standardization and centralization processes helped to achieve data harmony. You'll be able to answer why it is necessary to integrate data, even though the data is coming from disparate sources. You'll be able to perform data mapping as well as communicate the technical terms used to describe and perform record linkages.", "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. \n\nThis course is the third in a sequence of three. Following the first course, which focused on representation, and the second, which focused on inference, this course addresses the question of learning: how a PGM can be learned from a data set of examples. The course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem. Learning: Overview Review of Machine Learning Concepts from Prof. Andrew Ng's Machine Learning Class (Optional) Parameter Estimation in Bayesian Networks Learning Undirected Models Learning BN Structure Learning BNs with Incomplete Data Learning Summary and Final PGM Wrapup This module presents some of the learning tasks for probabilistic graphical models that we will tackle in this course. This module contains some basic concepts from the general framework of machine learning, taken from Professor Andrew Ng's Stanford class offered on Coursera. Many of these concepts are highly relevant to the problems we'll tackle in this course. This module discusses the simples and most basic of the learning problems in probabilistic graphical models: that of parameter estimation in a Bayesian network. We discuss maximum likelihood estimation, and the issues with it. We then discuss Bayesian estimation and how it can ameliorate these problems. In this module, we discuss the parameter estimation problem for Markov networks - undirected graphical models. This task is considerably more complex, both conceptually and computationally, than parameter estimation for Bayesian networks, due to the issues presented by the global partition function. This module discusses the problem of learning the structure of Bayesian networks. We first discuss how this problem can be formulated as an optimization problem over a space of graph structures, and what are good ways to score different structures so as to trade off fit to data and model complexity. We then talk about how the optimization problem can be solved: exactly in a few cases, approximately in most others. In this module, we discuss the problem of learning models in cases where some of the variables in some of the data cases are not fully observed. We discuss why this situation is considerably more complex than the fully observable case. We then present the Expectation Maximization (EM) algorithm, which is used in a wide variety of problems. This module summarizes some of the issues that arise when learning probabilistic graphical models from data. It also contains the course final. This module contains an overview of PGM methods as a whole, discussing some of the real-world tradeoffs when using this framework in practice. It refers to topics from all three of the PGM courses.", "In this course, you will learn how to analyze map data using different data types and methods to answer geographic questions. First, you will learn how to filter a data set using different types of queries to find just the data you need to answer a particular question. Then, we will discuss simple yet powerful analysis methods that use vector data to find spatial relationships within and between data sets. In this section, you will also learn about how to use ModelBuilder, a simple but powerful tool for building analysis flowcharts that can then also be run as models. You will then learn how to find, understand, and use remotely sensed data such as satellite imagery, as a rich source of GIS data. You will then learn how to analyze raster data. Finally, you will complete your own project where you get to try out the new skills and tools you have learned about in this course. Filtering Data Using Queries Vector analysis Remote sensing as a GIS data source Raster analysis Project: Spatial Analysis     ", "Welcome to the specialization course Relational Database Systems. This course will be completed on six weeks, it will be supported with videos and various documents that will allow you to learn in a very simple way how several types of information systems and databases are available to solve different problems and needs of the companies. \n\nObjective:\n\nA learner will be able to design, test, and implement analytical, transactional or NoSQL database systems according to business requirements by programming reliable, scalable and maintainable applications and resources using SQL and Hadoop ecosystem.\n\nProgramming languages:\n\nFor course 1 you will use the MYSQL language.\n\nSoftware to download:\nMySQL\nWorkbench \n\nIn case you have a Mac / IOS operating system you will need to use a virtual Machine (VirtualBox, Vmware). Information Systems Entity Relationship Theory and Conceptual Design Relational Database Theory and Logical Design Structured Query Language Data Manipulation Language Structured Query Language and Advanced SQL Programming Transactions and query optimization In the first module named information systems, we will learn how people, hardware, software, networks, techniques and procedures work together to automate transactional processes that companies need for their daily operations. Let's start! The present module is focused on Conceptual Design. The learner will be able to create an Entity Relationship Diagram through the Conceptual Design from business requirements. The present module is focused on Logical Design. The learner will be able to create an Relational Model from the entity-relationship diagram The present module is focused on Physical Design. The learner will be able to create database objects with data definition language from the Structured Query Language. The present module is focused on Data Manipulation Language on SQL programming to feed and query relational database objects. The present module is focused on query optimization according to type of information systems.", "Neurohacking describes how to use the R programming language (https://cran.r-project.org/) and its associated package to perform manipulation, processing, and analysis of neuroimaging data. We focus on publicly-available structural magnetic resonance imaging (MRI). We discuss concepts such as inhomogeneity correction, image registration, and image visualization.\n\nBy the end of this course, you will be able to:\n\nRead/write images of the brain in the NIfTI (Neuroimaging Informatics Technology Initiative) format\nVisualize and explore these images\nPerform inhomogeneity correction, brain extraction, and image registration (within a subject and to a template). Introduction Neuroimaging: Formats and Visualization Image Processing Extended Image Processing  In this section, we will discuss different formats that brain images come in, as well as some of the commonly done magnetic resonance imaging (MRI) scans. In this section, we will discuss the steps done to process brain MRI data.  We will discuss inhomogeneity correction, brain extraction or skull stripping, and various image registration techniques. In this section, we will discuss the different types of registration and how one would go through processing a multi-sequence MRI scan, as well as wrapper functions that make the process much easier.  We also cover interactive exploration of brain image data and tissue-level (white/gray matter and cerebrospinal fluid (CSF)) segmentation from a T1-weighted image.", "This course aims to teach the concepts of clinical data models and common data models. Upon completion of this course, learners will be able to interpret and evaluate data model designs using Entity-Relationship Diagrams (ERDs), differentiate between data models and articulate how each are used to support clinical care and data science, and create SQL statements in Google BigQuery to query the MIMIC3 clinical data model and the OMOP common data model. Introduction: Clinical Data Models and Common Data Models Tools: Querying Clinical Data Models Techniques: Extract-Transform-Load and Terminology Mapping Techniques: Data Quality Assessments Practical Application: Create an ETL Process to Transform a MIMIC-III Table to OMOP This week describes clinical data models and explains the need for and use of common data models in national and international data networks.  We will also cover the features of Entity-Relationship Diagrams (ERDs) to describe the key technical features of data models.  We take a deep dive into the technical features of clinical data models using MIMIC3 as our example and research common data models using OMOP as our example. This module teaches learners about the processes and challenges with extracting, transforming and loading (ETL) data with real-world examples in data and terminology mapping. \n We explore the dimensions of data quality by reviewing its challenges, data quality measurements used to measure it, and data quality rules to assess its acceptability for use. In this module, you gather everything you\u00e2\u0080\u0099ve learned to complete a real-world hands-on exercise using ETL methods to convert MIMIC3 data into the OMOP common data model.", "With marketers are poised to be the largest users of data within the organization, there is a need to make sense of the variety of consumer data that the organization collects. Surveys, transaction histories and billing records can all provide insight into consumers\u00e2\u0080\u0099 future behavior, provided that they are interpreted correctly. In Introduction to Marketing Analytics, we introduce the tools that learners will need to convert raw data into marketing insights. The included exercises are conducted using Microsoft Excel, ensuring that learners will have the tools they need to extract information from the data available to them. The course provides learners with exposure to essential tools including exploratory data analysis, as well as regression methods that can be used to investigate the impact of marketing activity on aggregate data (e.g., sales) and on individual-level choice data (e.g., brand choices). \n\nTo successfully complete the assignments in this course, you will require Microsoft Excel. If you do not have Excel, you can download a free 30-day trial here: https://products.office.com/en-us/try Meet Dr. Schweidel & Course Overview Exploring your Data with Visualization and Descriptive Statistics, Part 1 Exploring your Data with Visualization and Descriptive Statistics, Part 2 Regression Analysis for Marketing Data From Analysis to Action In this module, students will be introduced to the instructor, Dr. David Schweidel and get and overview of the course.  Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions. Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions. In this module, you will be asked to determine the appropriate type of regression for different types of marketing data and will perform regression analysis to assess the impact of marketing actions on outcomes of interest, such as sales, traffic, and brand choices. You will also be asked to interpret regression output to understand overall model performance and importance of different predictors, as well as make predictions using the appropriate regression model. This final module will connect the results of regression analysis to marketing decisions. You will learn to build tools that allow users to evaluate outcomes based on different marketing decisions, as well as characterize the extent of uncertainty in outcomes based on the selected marketing decisions.", "The Business Analytics Capstone Project gives you the opportunity to apply what you've learned about how to make data-driven decisions to a real business challenge faced by global technology companies like Yahoo, Google, and Facebook. At the end of this Capstone, you'll be able to ask the right questions of the data, and know how to use data effectively to address business challenges of your own. You\u00e2\u0080\u0099ll understand how cutting-edge businesses use data to optimize marketing, maximize revenue, make operations efficient, and make hiring and management decisions so that you can apply these strategies to your own company or business. Designed with Yahoo to give you invaluable experience in evaluating and creating data-driven decisions, the Business Analytics Capstone Project provides the chance for you to devise a plan of action for optimizing data itself to provide key insights and analysis, and to describe the interaction between key financial and non-financial indicators. Once you complete your analysis, you'll be better prepared to make better data-driven business decisions of your own.  Module 1: Capstone Project Topic - The Problem of Adblocking Module 2:  Defining the Problem Module 3:  Your Strategy Module 4:  Effects of Your Strategy/Measuring these Effects Module 5:  Final Project Submission The Business Analytics Specialization was designed to help you learn how to think about using data in making big (and small) business decisions. In this Capstone project, you'll be asked to create a strategy for a fictional digital search engine and content provider, GoYaFace, Inc. (often abbreviated as \u00e2\u0080\u009cGYF\u00e2\u0080\u009d). The strategy will be used in responding to the increasing popularity and availability of \u00e2\u0080\u009cadblocking\u00e2\u0080\u009d software, which could have significant negative repercussions for GYF\u00e2\u0080\u0099s business.  You are to assume the role of the leader of the Digital Advertising Tactics and Action (\u00e2\u0080\u009cDATA\u00e2\u0080\u009d) Team at GYF, which has been assigned the job of formulating GYF\u00e2\u0080\u0099s strategy in responding to the threat of adblocking.  Your task is to develop a strategy that will be recommended to GYF\u00e2\u0080\u0099s senior leadership.  Using what you've learned about business analytics, you'll (i) create a detailed problem statement focusing on GYF\u00e2\u0080\u0099s ad-buying customers (Module 2), (ii) develop a strategy (Module 3), (iii) describe the anticipated effects of the strategy (Module 4), and (iv) form a plan for measuring the effects of your strategy (Module 4).  You'll then put these four pieces together into a final project (Module 5).  First, please read the full description of the project in the \u00e2\u0080\u009cProject Description\u00e2\u0080\u009d link below, and then look at the background information about adblockers and the \u00e2\u0080\u009cGYF Company Profile\u00e2\u0080\u009d link in the content for Module 1. When you are ready to begin the first assignment, please move on to Module 2: Defining the Problem. In Module 2, you'll define the problem adblockers poses for GYF. GYF is intended to be a composite of leading internet platform and content providers who derive substantial revenues from mobile advertising like Google, Yahoo, and Facebook, so you should frame your research around the real-world problems these companies have faced and are facing. Defining the problem thoroughly will have a direct impact on how successful your strategy will be received by your peers. The more deeply you consider the effects of adblockers on the companies that buy advertising space from GYF, the more appropriate your overall strategy is likely to be. Please use the resources below to find out more about the problem, and then create your Problem Statement and submit it for peer review below. You can and should draw from all of the Business Analytics Specialization courses, but your Problem Statement should focus on how adblockers might adversely affect GYF\u00e2\u0080\u0099s relationship with the companies that pay GYF to place advertisements on GYF\u00e2\u0080\u0099s mobile applications and content. You should consider the issue of causality in your Problem Statement - we've included some lectures from the underlying courses to refresh you on that topci.  And you are strongly encouraged to complete and include a response to Application Exercise 1 (see link below) as part of your Problem Statement. In Module 3, you will focus on creating your recommended strategy for GYF to address adblockers. Your strategy does not have to be lengthy, but it must be clear, and it must address the problem. (Hint: if you have a clearly defined problem, your strategy is much more likely to be clearly defined as well). You'll be submitting your strategy for peer review, and then also reviewing the work of at least 3 of your peers. It's OK if reviewing the strategies of other learners in this course gives you further ideas for revising your own strategy. One of the primary benefits of peer review is to expand the range of feedback you can get, and we designed this Module around peer review so that you can get as much feedback as possible before moving on to the next phase of the project.  You may find the resources and lectures below helpful in formulating your strategy and considering how data can be leveraged and appropriately understood.  You are strongly encouraged to complete and include your response to Application Exercise 2 as part of your Strategy.   Module 4 was designed to give you the opportunity to focus on the effects of your strategy. Effects and Measurement can be often overlooked in strategy development; creating a thoughtful and thorough plan for measuring the effects will improve your final project tremendously. In this part of the project, you will describe two events: what you think will happen and how you will measure it. Look to the courses in the Business Analytics Specialization to see what kind of data companies use to measure effects to create a measurement plan of your own. You are strongly encouraged to complete and include your responses to Application Exercises 3 and 4 as part of your Effects and Measurement components.  You may create a scenario (Operations Analytics) to predict some of the intended effects of your strategy, either following the outline of Application Exercise 3, or of your own design.  Once you submit your own plan for effects and measurement, please review the work of at least three of your peers. You may find new ideas, or new ways of looking at data and measurement from this exercise.  We encourage you to incorporate what you've learned into your final submission! In this final Module, you will combine the four revised elements of your presentation (Problem Statement, Strategy, Effects, and Measurement, including any responses to the Application Exercises you've completed) into one presentation and submit it for peer review. You'll then be asked to review the work of at least three of your peers. Once you have gotten feedback on your plan, you may use it as an example of strategic thinking at your current job, or as a work sample when you are applying for a new one. A successful strategic analysis which describes the use of data-driven decision making will make you much more marketable in almost any field. Good luck!", "In this course, you will learn how to solve problems with large, high-dimensional, and potentially infinite state spaces. You will see that estimating value functions can be cast as a supervised learning problem---function approximation---allowing you to build agents that carefully balance generalization and discrimination in order to maximize reward. We will begin this journey by investigating how our policy evaluation or prediction methods like Monte Carlo and TD can be extended to the function approximation setting. You will learn about feature construction techniques for RL, and representation learning via neural networks and backprop. We conclude this course with a deep-dive into policy gradient methods; a way to learn policies directly without learning a value function. In this course you will solve two continuous-state control tasks and investigate the benefits of policy gradient methods in a continuous-action environment. \n\nPrerequisites: This course strongly builds on the fundamentals of Courses 1 and 2, and learners should have completed these before starting this course.  Learners should also be comfortable with probabilities & expectations, basic linear algebra, basic calculus, Python 3.0 (at least 1 year), and  implementing algorithms from pseudocode.\n\nBy the end of this course, you will be able to: \n\n-Understand how to use supervised learning approaches to approximate value functions\n-Understand objectives for prediction (value estimation) under function approximation\n-Implement TD with function approximation (state aggregation), on an environment with an infinite state space (continuous state space)\n-Understand fixed basis and neural network approaches to feature construction \n-Implement TD with neural network function approximation in a continuous state environment\n-Understand new difficulties in exploration when moving to function approximation\n-Contrast discounted problem formulations for control versus an average reward problem formulation\n-Implement expected Sarsa and Q-learning with function approximation on a continuous state control task\n-Understand objectives for directly estimating policies (policy gradient objectives)\n-Implement a policy gradient method (called Actor-Critic) on a discrete state environment Welcome to the Course! On-policy Prediction with Approximation Constructing Features for Prediction Control with Approximation  Policy Gradient Welcome to the third course in the Reinforcement Learning Specialization: Prediction and Control with Function Approximation, brought to you by the University of Alberta, Onlea, and Coursera. In this pre-course module, you'll be introduced to your instructors, and get a flavour of what the course has in store for you. Make sure to introduce yourself to your classmates in the \"Meet and Greet\" section!  This week you will learn how to estimate a value function for a given policy, when the number of states is much larger than the memory available to the agent. You will learn how to specify a parametric form of the value function, how to specify an objective function, and how estimating gradient descent can be used to estimate values from interaction with the world.  The features used to construct the agent\u00e2\u0080\u0099s value estimates are perhaps the most crucial part of a successful learning system. In this module we discuss two basic strategies for constructing features: (1) fixed basis that form an exhaustive partition of the input, and (2) adapting the features while the agent interacts with the world via Neural Networks and Backpropagation. In this week\u00e2\u0080\u0099s graded assessment you will solve a simple but infinite state prediction task with a Neural Network and TD learning.   This week, you will see that the concepts and tools introduced in modules two and three allow straightforward extension of classic TD control methods to the function approximation setting. In particular, you will learn how to find the optimal policy in infinite-state MDPs by simply combining semi-gradient TD methods with generalized policy iteration, yielding classic control methods like Q-learning, and Sarsa. We conclude with a discussion of a new problem formulation for RL---average reward---which will undoubtedly be used in many applications of RL in the future.  Every algorithm you have learned about so far estimates a value function as an intermediate step towards the goal of finding an optimal policy. An alternative strategy is to directly learn the parameters of the policy. This week you will learn about these policy gradient methods, and their advantages over value-function based methods. You will also learn how policy gradient methods can be used to find the optimal policy in tasks with both continuous state and action spaces.", "Biostatistics is the application of statistical reasoning to the life sciences, and it's the key to unlocking the data gathered by researchers and the evidence presented in the scientific public health literature. In this course, we'll focus on the use of simple regression methods to determine the relationship between an outcome of interest and a single predictor via a linear equation. Along the way, you'll be introduced to a variety of methods, and you'll practice interpreting data and performing calculations on real data from published studies.  Topics include logistic regression, confidence intervals, p-values, Cox regression, confounding, adjustment, and effect modification. Simple Regression Methods Simple Logistic Regression Simple Cox Proportional Hazards Regression Confounding, Adjustment, and Effect Modification Course Project Module one covers simple regression, the four different types of regression, commonalities between them, and simple linear aggression. Before completing the graded quiz, you can test your knowledge with the practice quiz.   Within module two, we will look at logistic regression, create confidence intervals, and estimate p-values. You will have the opportunity to test your knowledge in both a practice quiz and a graded quiz.  Module three focuses on Cox regression with different predictors. You will have the opportunity to test your knowledge first with the practice quiz and, then, with the graded quiz.   Within module four, you will look at confounding and adjustment, and unadjusted and adjusted association estimates. Additionally, you will learn about effect modification. Similar to previous modules, you will first take a practice quiz before completing the graded quiz.   During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about self-administration of injectable contraception and one about medical appointment scheduling in Brazil. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken other courses in this specialization, then this scenario will be familiar. ", "Businesses run on data, and data offers little value without analytics. The ability to process data to make predictions about the behavior of individuals or markets, to diagnose systems or situations, or to prescribe actions for people or processes drives business today. Increasingly many businesses are striving to become \u00e2\u0080\u009cdata-driven\u00e2\u0080\u009d, proactively relying more on cold hard information and sophisticated algorithms than upon the gut instinct or slow reactions of humans. \n\nThis course will focus on understanding key analytics concepts and the breadth of analytic possibilities. Together, the class will explore dozens of real-world analytics problems and solutions across most major industries and business functions. The course will also touch on analytic technologies, architectures, and roles from business intelligence to data science, and from data warehouses to data lakes. And the course will wrap up with a discussion of analytics trends and futures. Course Overview & Module 1 Analytics Beyond the Spreadsheet Module 2 Industry and Business Function Analytics  Module 3 Staffing and Organizing for Analytics  Module 4 Analytics Success Today and Tomorrow This first module exposes and explains key data and analytics concepts from Big Data to data warehousing to natural language query, and everything in-between. Next we will explore various analytic techniques, types of visualizations, and types of analytics solutions. The course will continue with identifying and learning about key data and analytics roles and organization structures, including chief data and analytics officers, data scientists, and analytics centers of excellence. Alternatives to direct hiring, such as outsourcing and crowdsourcing, will also be covered. Finally, the course will scrutinize analytic trends and futures.  Over the course of the module, you will also see how data and analytics in each of these organizations can be used in similar ways, in similar business functions. Accordingly, you will appreciate that to be truly data-driven, you need not only look to examples in your own industry, but, also learn and apply analytics concepts from organizations in other fields. In this module you will learn a bunch of crucial analytical roles and the emergence of new roles in organizations from the C-suite down to various analyst roles. You will take a brief look at the job descriptions and the responsibilities. You will also put yourself in either a job seeker\u00e2\u0080\u0099s or a recruiter\u00e2\u0080\u0099s shoes to see what kind of skill sets are the most important and which position fits you the best. For example, it will introduce you to the three core skills of the data scientist and the crucial soft skills required to be a successful data scientist.  This module explores telling stories, through data, that connect emotionally with your audience. It will also review examples and figures that make the concept easy to understand. You will learn the major do\u00e2\u0080\u0099s and don\u00e2\u0080\u0099ts of creating dataviz and rules that lead to the clear depiction of your findings. This unit specifically focuses on Dona Wong\u00e2\u0080\u0099s guidelines for good data visualization and charts. The last leg of Module 4 teaches the three tests that help you improve your visualization. In the final step of dataviz execution, you will learn the McCandless Method for presenting visualizations. This five-step process produces the most effective communication of the graphics to your audience.", "Are you trying to understand data from your research? Learn how and when to conduct mediation, moderation, and conditional indirect effects analyses? Or, perhaps, how to theorize and test your theoretical models? If so, this is the course for you! We will walk you through the steps of conducting multilevel analyses using a real dataset and provide articles and templates designed to facilitate your learning. You'll leave with the tools you need to analyze and interpret the results of the datasets you collect as a researcher. \n\nBy the end of this course, you will understand the differences between mediation and moderation and between moderated mediation and mediated moderation models (conditional indirect effects), and the importance of multilevel analysis. Most important, you will be able to run mediation, moderation, conditional indirect effect and multilevel models and interpret the results.\n\nThis course is supported by the BRAD Lab at the Darden School of Business, which studies organizational behavior, marketing, business ethics, judgment and decision-making, behavioral operations, and entrepreneurship, among other areas. More: http://www.darden.virginia.edu/brad-lab/ Mediation and Moderation Conditional Indirect Effects Multilevel Analysis Welcome to the first week of our research methods course! We'll start with mediation analysis, following by parallel mediation, serial mediation, and moderation. Mediation is all about the mechanisms connecting the independent variable and dependent variable. Moderation refers to the circumstances under which the independent variable influences the dependent variable. By the end of this week, you will know how, when, and where the independent variable influences the dependent variable and how to theorize and conduct analysis using SPSS. Now that you know more about mediation and moderation, let's take a look at conditional indirect effects models, which are a combination of mediation and moderation models. First we will get to the heart of the differences between moderated mediation and mediated moderation models.This will allow you to fully understand the relationships between independent variables and dependent variables. By the end of the module, you will be able to theorize about conditional indirect effect models on SPSS and to test which path of the mediation model is affected by the moderator. Then you'll dive into SPSS, run different models, and learn how to interpret the results. \n Now that you know how to run mediation, moderation, and conditional indirect effect analyses, we can turn our attention to multilevel models. Multilevel models are statistical models of parameters that vary at more than one level. Think about employees nested in departments, or departments nested in firms. You will learn the importance of multilevel analysis to your research and get familiar with multilevel analysis language. By the end of this module, you will be able to use HLM software to run multilevel models and interpret the results.", "Welcome to Demand Analytics - one of the most sought-after skills in supply chain management and marketing!\n\nThrough the real-life story and data of a leading cookware manufacturer in North America, you will learn the data analytics skills for demand planning and forecasting. Upon the completion of this course, you will be able to  \n\n1. Improve the forecasting accuracy by building and validating demand prediction models. \n2. Better stimulate and influence demand by identifying the drivers (e.g., time, seasonality, price, and other environmental factors) for demand and quantifying their impact.\n\nAK is a leading cookware manufacturer in North America. Its newly launched top-line product was gaining momentum in the marketplace. However, a price adjustment at the peak season stimulated a significant demand surge which took AK completely by surprise and resulted in huge backorders. AK faced the risk of losing the market momentum due to the upset customers and the high cost associated with over-time production and expedited shipping. Accurate demand forecast is essential for increasing revenue and reducing cost. Identifying the drivers for demand and assessing their impact on demand can help companies better influence and stimulate demand.\n\nI hope you enjoy the course! Welcome! Predicting Trend Predicting the Impact of Price and Other Environmental Factors Predicting Seasonality Welcome to the exciting world of Demand Analytics! In Week 1, you will learn the crisis that AK MetalCrafters, a leading cookware manufacturer in North America, faced in launching new products, and how AK successfully resolved the crisis using Demand Analytics. You will also learn the general principles of demand planning and forecasting, and how it fits into a firm's integrated business planning. Welcome to Week 2 of Demand Analytics! In Week 1, you learned the general principles, now in Week 2, you will put them to action by building and interpreting a linear model for predicting the trend (as in new product introduction). You will also learn data collection, pre-processing and visualization techniques, which are critical to model building. Welcome to Week 3 of Demand Analytics! In Week 2, you built a linear model to predict the trend. In this week, you will validate and improve the model by first analyzing its errors to identify missing variables and then building a multiple regression model to capture not only the trend but also the impact of price and other environmental factors.  In this last week of Demand Analytics, you will further improve your demand forecasting model built in Week 3 by including seasonality to capture the periodic patterns in the errors; you will learn how to model and format categorical variables, and how to create and test your forecast.", "In this course, you'll learn how to manage big datasets, how to load them into clusters and cloud storage, and how to apply structure to the data so that you can run queries on it using distributed SQL engines like Apache Hive and Apache Impala. You\u00e2\u0080\u0099ll learn how to choose the right data types, storage systems, and file formats based on which tools you\u00e2\u0080\u0099ll use and what performance you need.\n\nBy the end of the course, you will be able to\n\u00e2\u0080\u00a2 use different tools to browse existing databases and tables in big data systems;\n\u00e2\u0080\u00a2 use different tools to explore files in distributed big data filesystems and cloud storage;\n\u00e2\u0080\u00a2\u00c2\u00a0create and manage big data databases and tables using Apache Hive and Apache Impala; and\n\u00e2\u0080\u00a2 describe and choose among different data types and file formats for big data systems.\n\nTo use the hands-on environment for this course, you need to download and install a virtual machine and the software on which to run it. Before continuing, be sure that you have access to a computer that meets the following hardware and software requirements:\n\u00e2\u0080\u00a2\u00c2\u00a0Windows, macOS, or Linux operating system (iPads and Android tablets will not work)\n\u00e2\u0080\u00a2 64-bit operating system (32-bit operating systems will not work)\n\u00e2\u0080\u00a2 8 GB RAM or more\n\u00e2\u0080\u00a2\u00c2\u00a025GB free disk space or more\n\u00e2\u0080\u00a2 Intel VT-x or AMD-V virtualization support enabled (on Mac computers with Intel processors, this is always enabled;\non Windows and Linux computers, you might need to enable it in the BIOS)\n\u00e2\u0080\u00a2 For Windows XP computers only: You must have an unzip utility such as 7-Zip or WinZip installed (Windows XP\u00e2\u0080\u0099s built-in unzip utility will not work) Orientation to Data in Clusters and Cloud Storage Defining Databases, Tables, and Columns Data Types and File Types Managing Datasets in Clusters and Cloud Storage Optimizing Hive and Impala (Honors)     Honors (Optional)", "In this Capstone you will recommend a business strategy based on a data model you\u00e2\u0080\u0099ve constructed. Using a data set designed by Wharton Research Data Services (WRDS), you will implement quantitative models in spreadsheets to identify the best opportunities for success and minimizing risk. Using your newly acquired decision-making skills, you will structure a decision and present this course of action in a professional quality PowerPoint presentation which includes both data and data analysis from your quantitative models.\n\nWharton Research Data Services (WRDS) is the leading data research platform and business intelligence tool for over 30,000 corporate, academic, government and nonprofit clients in 33 countries. WRDS provides the user with one location to access over 200 terabytes of data across multiple disciplines including Accounting, Banking, Economics, ESG, Finance, Insurance, Marketing, and Statistics. Getting Started Steps 1 and 2: Yahoo Finance Step 3: Creating an optimal risky portfolio on the efficient frontier Step 4: Optional exercise using CAPM tables Step 5: Creating Your Asset Allocation & Final Presentation Welcome!  This opening module was designed to give you an overview of the Business and Financial Modeling Capstone, in which you will be working with historical financial data to calculate individual returns and summary statistics on those returns. The project has multiple steps, which are outlined below in the \"Project Prompt\", and culminates in a recommendation for portfolio allocation that you will prepare a presentation on. You will draw on elements from all courses to complete this project, and you can use your final presentation as a work sample to improve your current job or even find a new one.  Before moving on, complete the \"Project Scope Quiz.\" The work you do this week enables you to understand the steps needed to successfully complete your final project. In this module, which correlates to Steps 1 and 2 in the Project Prompt, you'll be working with a historical data set to calculate performance data and to provide summary statistics on that data. These calculations will allow you to practice using Spreadsheets for financial calculations, and provides the foundational skills and numbers for the next steps of the project. First, you'll use the set to calculate daily returns on a set of securities. You'll then use your Spreadsheet skills to calculate summary statistics. You'll be given the opportunity to test your knowledge with a sample return to see if your calculations are correct.  And you may want to refresh your recollection of the content from the Specialization with the lectures included here.  The work you complete this week allows you to form the basis for comparing stock performance, which you will use in creating the investment portfolio for your final project as well as the comparison to the performance of a single stock. In this module, you'll go beyond calculating simple returns to tackle the more advanced task of finding the minimum variance and \"optimal risk portfolio\" weights for a portfolio of selected securities (note, the \"optimal risky portfolio\" is also known as an \"optimal portfolio\" or \"tangent portfolio\").  You'll follow the tasks in Step 3 in the Project Prompt and use the resources below to calculate the portfolio weights for two securities that results in the portfolio with the minimum variance; then, you'll calculate the \"optimal risky portfolio\" on the efficient frontier for these same two securities, then for all 10 stocks in the pool.  You'll be quizzed on your calculations and other insights that emerge from this exercise. The work you complete this week gives you practice in creating an optimal risky portfolio, which is a key component of your final project. Note:  There are a number of resources available on the internet providing step-by-step instructions on how to use Excel to create an \"optimal risky portfolio\" on the efficient frontier given a certain set of available assets.  We encourage you to attempt to use the skills you gained during the Specialization to work through these steps independently; you are, however, permitted to utilize third-party resources if you find it necessary.  We've included some lectures from the underlying Specialization courses concerning Solver, optimization, and other relevant topics.   The Capital Asset Pricing Model, or CAPM, is another tool used by investors to weigh the risks and rewards of potential investments.  In this optional module covering Step 4 in the Project Prompt, you can use CAPM as a vehicle to further strengthen your financial modeling skills, including using regression concepts.  You may revisit the Specialization lectures below touching on regression.  To test whether you've grasped the concepts in the CAPM model, this module includes a short quiz.  This assessment is formative, meaning your score will not count towards your final grade. The work you do this week may inform how you build the mixed asset portfolio of your final project, but it is not necessary to complete the final project. In this final module you are asked to move beyond a stock-only portfolio to one utilizing more diversified assets and to prepare a short presentation summarizing your findings.  As explained in Step 5 of the Project Prompt, you have $5 million to invest in the Vanguard Total Bond Market Index Fund (ticker: VBTLX) and Vanguard 500 Index (ticker: VFIAX) investment vehicles.  There are two assessments in this module. First, you'll complete a short quiz on the characteristics of your optimal risky portfolio.  Then, in the peer review component of this Capstone, you are tasked with preparing a short presentation that (i) explores how your portfolio of mixed asset class of funds compares to a single security (AAPL) and (ii) uses that comparison to discuss the importance of portfolio diversification.", "Manufacturers are increasingly utilizing machine tools that are self-aware \u00e2\u0080\u0093 they perceive their own states and the state of the surrounding environment \u00e2\u0080\u0093 and are able to make decisions related to machine activity processes. This is called intelligent machining, and through this course students will receive a primer on its background, tools and related terminology. \n\nLearn how the integration of smart sensors and controls are helping to improve productivity. You\u00e2\u0080\u0099ll be exposed to various sensors and sensing techniques, process control strategies, and open architecture systems that can be leveraged to enable intelligent machining. This course will prepare you to contribute to the implementation of intelligent machining projects. \n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the fifth course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Introduction to Intelligent Machining  Sensors and Sensing Techniques Process Control Strategies Future Directions in Advanced Machining The purpose of this module is to introduce the concepts related to intelligent machining paradigm. The key focus will be discussing two key components of intelligent machining, i.e., sensing and control.  The purpose of this module is to introduce spectrum of sensors used to implement intelligent machining. The module will also discuss the basics of signal processing and analysis techniques that has brought intelligent machining paradigm closer to industrial realization. Following issues pertaining to sensors and sensing techniques will be elaborated up: (1) Which sensors are to be used in each application? (2) How to acquire and process sensor signals? \n The purpose of this module is to introduce the concept of Programmable Logic Controllers (PLCs) that co-ordinate the real-time control functions.  The purpose of this module is to introduce the background related to open architecture software systems to implement intelligent machining.", "Welcome to the Advanced Linear Models for Data Science Class 1: Least Squares. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:\n\n- A basic understanding of linear algebra and multivariate calculus.\n- A basic understanding of statistics and regression models.\n- At least a little familiarity with proof based mathematics.\n- Basic knowledge of the R programming language.\n\nAfter taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models. Background One and two parameter regression Linear regression General least squares Least squares examples Bases and residuals We cover some basic matrix algebra results that we will need throughout the class. This includes some basic vector derivatives. In addition, we cover some some basic uses of matrices to create summary statistics from data. This includes calculating and subtracting means from observations (centering) as well as calculating the variance.\n In this module, we cover the basics of regression through the origin and linear regression. Regression through the origin is an interesting case, as one can build up all of multivariate regression with it. In this lecture, we focus on linear regression, the most standard technique for investigating unconfounded linear relationships.  We now move on to general least squares where an arbitrary full rank design matrix is fit to a vector outcome. Here we give some canonical examples of linear models to relate them to techniques that you may already be using. Here we give a very useful kind of linear model, that is decomposing a signal into a basis expansion.", "In this Capstone Project, you'll bring together all the new skills and insights you've learned through the four courses. You'll be given a 'mock' client problem and a data set. You'll need to analyze the data to gain business insights, research the client's domain area, and create recommendations. You'll then need to visualize the data in a client-facing presentation. You'll bring it all together in a recorded video presentation.\n\nThis course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. Understanding the Business Problem  Analyzing the Business Problem Creating a visual representation of your analysis results  Building a presentation for the client meeting  Presenting your results to the client In this course, you will be using the skills you\u00e2\u0080\u0099ve been developing throughout the series of PwC courses to create and present a deliverable for a client.You\u00e2\u0080\u0099re going to determine your approach for analysis, use the information provided by the client to conduct an analysis, determine the best way to show your findings and then develop a plan to deliver your results and recommendations to the client in a video presentation.In this course, you will be acting as a consultant interacting with a client, Electric Growers.  The client needs your strong data analytics and Excel skills to analyze data and help them make smart decisions to enable the company\u00e2\u0080\u0099s move into the security devices and services market.  Each week, the client, represented by Michele Tran, a Vice President of Operations from Electric Growers, and the management team, will ask you to use and analyze the data, provided by the client, to answer a series of questions that will help them make decisions.You\u00e2\u0080\u0099ll receive peer feedback that you can use to enhance future presentations. This course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017. This week, you will be using the results of last week\u00e2\u0080\u0099s analysis to come up with some hypotheses. You will be answering questions such as: Who are the ideal customers that should be targeted? How should they be approached to maximize the sales of the client\u00e2\u0080\u0099s new home security systems?  Using the data, figure out the attributes of the customer who wants to install an advanced, hi-tech security system and the attributes of a person who would switch security system providers. Continue to use the knowledge you\u00e2\u0080\u0099ve acquired in the other courses that you\u00e2\u0080\u0099ve taken in this series. This week, the client has more questions for you to answer to further develop the profile of potential customers.  This includes finding which cities to target and how to evaluate them.  You will have access to crime rates, population, housing growth city wide data and segmentation distribution dumps to help in your analysis. This week, you will be taking your findings and detailed analysis on plans to launch the new home security system and put them into a PowerPoint presentation deck for Michele Tran and the management team. The presentation should be about 10 slides and should follow the guidelines and best practices of presenting to a client that were covered in the earlier courses within this specialization. Ms. Tran wants to review it and give you feedback, if needed, before having it presented to the management team.This is a chance to use what you learned about creating a presentation, using PowerPoint tools, creating charts and other methods of enhancing a presentation. This week, Ms. Tran has reviewed your draft presentation so it is now time to refine and make changes or additions to enhance the presentation before presenting it to the management team via video.\n\nOnce you\u00e2\u0080\u0099ve finalized your presentation, you will present it in a video using your smartphone or computer. When you\u00e2\u0080\u0099re satisfied with the PowerPoint presentation and video, you will be submitting both for peer review. You can use this feedback for current and future presentations that you will make during your career.", "This course introduces students to the science of business analytics while casting a keen eye toward the artful use of numbers found in the digital space. The goal is to provide businesses and managers with the foundation needed to apply data analytics to real-world challenges they confront daily in their professional lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize data; and utilize data in decision making for their agencies, organizations or clients. Course Introduction Module 2 Module 3 Module 4 With the first module, we will measure and identify satisfied customers to adjust product or service accordingly. To measure the customer satisfaction we will measure expectations, performance and disconfirmation of the offered product or service. We will also provide an overview of marketing analytics used to gauge the effectiveness of different marketing activities. Finally, we will go through measurement and scaling techniques. We will explore the marketing world through process of A/B testing, design of experiments, data analysis, and hypothesis testing. Next, we study Analysis of Variance (ANOVA) which is used to determine significant differences between two or more categorical groups. We will study ANOVA\u00e2\u0080\u0099s assumptions, test inference and different types of ANOVA. We also spend some time in designing experiments. We will learn about the Binary Outcome model using Logit function. The Logistic regression is sued when the dependent variable has a binary outcome. Next, we cover Multidimensional Scaling (MDS). MDS is used in marketing to understand the pair-wise similarity of the individual cases of data-set.  This technique maps the individual cases onto a 2-dimensional Cartesian graph for visual analysis. We will learn about Conjoint Analysis to understand how individuals combine and weigh different attributes.  We will introduce the concept of conjoint analysis and part-worth utilities.  We will survey different approaches before focusing our attention to the classical conjoint analysis. We will reinforce our understanding by studying an example in detail before running examples of the analysis in R", "This course aims to provide a succinct overview of the emerging discipline of Materials Informatics at the intersection of materials science, computational science, and information science. Attention is drawn to specific opportunities afforded by this new field in accelerating materials development and deployment efforts. A particular emphasis is placed on materials exhibiting hierarchical internal structures spanning multiple length/structure scales and the impediments involved in establishing invertible process-structure-property (PSP) linkages for these materials. More specifically, it is argued that modern data sciences (including advanced statistics, dimensionality reduction, and formulation of metamodels) and innovative cyberinfrastructure tools (including integration platforms, databases, and customized tools for enhancement of collaborations among cross-disciplinary team members) are likely to play a critical and pivotal role in addressing the above challenges. Welcome Accelerating Materials Development and Deployment  Materials Knowledge and Materials Data Science  Materials Knowledge Improvement Cycles Case Study in Homogenization: Plastic Properties of Two-Phase Composites Materials Innovation Cyberinfrastructure and Integrated Workflows What you should know before you start the course \u00e2\u0080\u00a2\tLearn and appreciate historical paradigms of advanced materials development while emphasizing the critical need for new approaches that employ data sciences and informatics as the glue to connect computational simulation and experiments to speed up the processes of materials discovery and development.\n\u00e2\u0080\u00a2\tLearn about the emergence of key national and international 21st century initiatives in accelerated materials discovery and development and how they are expected to bring about a disruptive transformation of new product capabilities and time to market. \u00e2\u0080\u00a2\tUnderstand property, structure and process spaces\n\u00e2\u0080\u00a2\tLearn about Process-Structure-Property Linkages \n\u00e2\u0080\u00a2\tLearn what does Materials Knowledge mean\n\u00e2\u0080\u00a2\tLearn about a role of Data Science in Materials Knowledge System\n\u00e2\u0080\u00a2\tOverview approaches and main components of Data Science\n\u00e2\u0080\u00a2\tLearn about a new discipline - Materials Data Sciences \u00e2\u0080\u00a2\tLearn material structure and its digital representation\n\u00e2\u0080\u00a2\tLearn how to calculate 2-point statistics \n\u00e2\u0080\u00a2\tLearn how Principal Component Analysis can be used to reduce dimensionality\n\u00e2\u0080\u00a2\tUnderstand Homogenization and Localization concepts\n This module demonstrates a homogenization problem based on an example of two-phase composites \u00e2\u0080\u00a2\tLearn about materials innovation system and cyberinfrastructure\n\u00e2\u0080\u00a2\tReview Materials Databases, e-collaboration platforms and code repositories\n\u00e2\u0080\u00a2\tLearn why integrated workflows are needed\n\u00e2\u0080\u00a2\tDefine Metadata, Structured and Unstructured data\n\u00e2\u0080\u00a2\tLearn about available services for e-collaborations\n", "Are you interested in predicting future outcomes using your data? This course helps you do just that! Machine learning is the process of developing, testing, and applying predictive algorithms to achieve this goal. Make sure to familiarize yourself with course 3 of this specialization before diving into these machine learning concepts. Building on Course 3, which introduces students to integral supervised machine learning concepts, this course will provide an overview of many additional concepts, techniques, and algorithms in machine learning, from basic classification to decision trees and clustering. By completing this course, you will learn how to apply, test, and interpret machine learning algorithms as alternative methods for addressing your research questions. Decision Trees Random Forests Lasso Regression K-Means Cluster Analysis In this session, you will learn about decision trees, a type of data mining algorithm that can select from among a large number of variables those and their interactions that are most important in predicting the target or response variable to be explained. Decision trees create segmentations or subgroups in the data, by applying a series of simple rules or criteria over and over again, which choose variable constellations that best predict the target variable. In this session, you will learn about random forests, a type of data mining algorithm that can select from among a large number of variables those that are most important in determining the target or response variable to be explained. Unlike decision trees, the results of random forests generalize well to new data. Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero. Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model. Variables with non-zero regression coefficients variables are most strongly associated with the response variable. Explanatory variables can be either quantitative, categorical or both. In this session, you will apply and interpret a lasso regression analysis. You will also develop experience using k-fold cross validation to select the best fitting model and obtain a more accurate estimate of your model\u00e2\u0080\u0099s test error rate. \nTo test a lasso regression model, you will need to identify a quantitative response variable from your data set if you haven\u00e2\u0080\u0099t already done so, and choose a few additional quantitative and categorical predictor (i.e. explanatory) variables to develop a larger pool of predictors.  Having a larger pool of predictors to test will maximize your experience with lasso regression analysis. Remember that lasso regression is a machine learning method, so your choice of additional predictors does not necessarily need to depend on a research hypothesis or theory. Take some chances, and try some new variables. The lasso regression analysis will help you determine which of your predictors are most important. Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. The cross-validation method you apply is designed to eliminate the need to split your data when you have a limited number of observations.  Cluster analysis is an unsupervised machine learning method that partitions the observations in a data set into a smaller set of clusters where each observation belongs to only one cluster. The goal of cluster analysis is to group, or cluster, observations into subsets based on their similarity of responses on multiple variables. Clustering variables should be primarily quantitative variables, but binary variables may also be included. In this session, we will show you how to use k-means cluster analysis to identify clusters of observations in your data set. You will gain experience in interpreting cluster analysis results by using graphing methods to help you determine the number of clusters to interpret, and examining clustering variable means to evaluate the cluster profiles. Finally, you will get the opportunity to validate your cluster solution by examining differences between clusters on a variable not included in your cluster analysis.  \nYou can use the same variables that you have used in past weeks as clustering variables. If most or all of your previous explanatory variables are categorical, you should identify some additional quantitative clustering variables from your data set. Ideally, most of your clustering variables will be quantitative, although you may also include some binary variables. In addition, you will need to identify a quantitative or binary response variable from your data set that you will not include in your cluster analysis. You will use this variable to validate your clusters by evaluating whether your clusters differ significantly on this response variable using statistical methods, such as analysis of variance or chi-square analysis, which you learned about in Course 2 of the specialization (Data Analysis Tools). Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. \n", "This course enables learners to develop 3D vision applications using a stereo imaging system. They are introduced to stereo vision theory, dense motion and visual tracking. They are able to discuss techniques used to obtain the 3D structure of objects. Topics include epipolar geometry, optical flow, structure from motion, multi-object tracking, 3D vision and visual odometry.    \n\nThis course is ideal for anyone curious about or interested in exploring the concepts of computer vision. It is also useful for those who desire a refresher course in mathematical concepts of computer vision. Learners should have basic programming skills and experience (understanding of for loops, if/else statements), specifically in MATLAB (Mathworks provides the basics here: https://www.mathworks.com/learn/tutorials/matlab-onramp.html).  Learners should also be familiar with the following: basic linear algebra (matrix vector operations and notation), 3D co-ordinate systems and transformations, basic calculus (derivatives and integration) and basic probability (random variables).   \n  \nMaterial includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing computer vision programs through online labs using MATLAB* and supporting toolboxes.\n\nThis is the third course in the Computer Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a video overview at https://youtu.be/OfxVUSCPXd0. \n\n * A free license to install MATLAB for the duration of the course is available from MathWorks. Stereo Vision Dense Motion & SFM Visual Tracking 3D Vision In this module, we will discuss the fundamentals and applications of stereo vision and multiple camera systems. In this module, we will discuss motion perception, optical flow, applications of dense motion, and structure from motion. This module provides information about visual tracking, including: object tracking, motion models, inferences used in tracking, and multi-object tracking. This module discusses the active methods used to develop 3D vision, as well as the applications of 3D vision.", "Statistical experiment design and analytics are at the heart of data science.  In this course you will design statistical experiments and analyze the results using modern methods.  You will also explore the common pitfalls in interpreting statistical arguments, especially those associated with big data.  Collectively, this course will help you internalize a core set of practical and effective machine learning methods and concepts, and apply them to solve some real world problems.\n\n  \nLearning Goals: After completing this course, you will be able to:\n1. Design effective experiments and analyze the results\n2. Use resampling methods to make clear and bulletproof statistical arguments without invoking esoteric notation\n3. Explain and apply a core set of classification methods of increasing complexity (rules, trees, random forests), and associated optimization methods (gradient descent and variants)\n4. Explain and apply a set of unsupervised learning concepts and methods\n5. Describe the common idioms of large-scale graph analytics, including structural query, traversals and recursive queries, PageRank, and community detection Practical Statistical Inference Supervised Learning Optimization Unsupervised Learning Learn the basics of statistical inference, comparing classical methods with resampling methods that allow you to use a simple program to make a rigorous statistical argument.  Motivate your study with current topics at the foundations of science: publication bias and reproducibility. Follow a tour through the important methods, algorithms, and techniques in machine learning.  You will learn how these methods build upon each other and can be combined into practical algorithms that perform well on a variety of tasks.  Learn how to evaluate machine learning methods and the pitfalls to avoid. You will learn how to optimize a cost function using gradient descent, including popular variants that use randomization and parallelization to improve performance.  You will gain an intuition for popular methods used in practice and see how similar they are fundamentally.  A brief tour of selected unsupervised learning methods and an opportunity to apply techniques in practice on a real world problem.", "In the last course of our specialization, Overview of Advanced Methods of Reinforcement Learning in Finance, we will take a deeper look into topics discussed in our third course, Reinforcement Learning in Finance.\n\nIn particular, we will talk about links between Reinforcement Learning, option pricing and physics, implications of Inverse Reinforcement Learning for modeling market impact and price dynamics, and perception-action cycles in Reinforcement Learning. Finally, we will overview trending and potential applications of Reinforcement Learning for high-frequency trading, cryptocurrencies, peer-to-peer lending, and more.\n\nAfter taking this course, students will be able to \n- explain fundamental concepts of finance such as market equilibrium, no arbitrage, predictability,\n- discuss market modeling,\n- Apply the methods of Reinforcement Learning to high-frequency trading, credit risk peer-to-peer lending, and cryptocurrencies trading. Black-Scholes-Merton model, Physics and Reinforcement Learning Reinforcement Learning for Optimal Trading and Market Modeling Perception - Beyond Reinforcement Learning Other Applications of Reinforcement Learning: P-2-P Lending, Cryptocurrency, etc.    ", "This if the final course in the specialization which builds upon the knowledge learned in Python Programming Essentials, Python Data Representations, and Python Data Analysis.  We will learn how to install external packages for use within Python, acquire data from sources on the Web, and then we will clean, process, analyze, and visualize that data. This course will combine the skills learned throughout the specialization to enable you to write interesting, practical, and useful programs.\n\nBy the end of the course, you will be comfortable installing Python packages, analyzing existing data, and generating visualizations of that data.  This course will complete your education as a scripter, enabling you to locate, install, and use Python packages written by others. You will be able to effectively utilize tools and packages that are widely available to amplify your effectiveness and write useful programs. Week 1  Week 2 Week 3 Week 4 This module will discuss the importance of using and writing documentation. The Python documentation is a valuable resource for learning about language features you haven't seen yet. This module will teach you about packages and modules in Python, including how to install packages and how to create your own modules. You will also learn to use the Pygal plotting library. This module will teach you about Python sets. Sets are used to hold unordered collections of data without duplicates. We will also discuss efficiency. The final project of the specialization will enable you to demonstrate mastery of the concepts you have learned up to this point. You will also be able to understand and compare different approaches to reconciling two data sets. ", "In this project-based course, you will design and execute a complete GIS-based analysis \u00e2\u0080\u0093 from identifying a concept, question or issue you wish to develop, all the way to final data products and maps that you can add to your portfolio. Your completed project will demonstrate your mastery of the content in the GIS Specialization and is broken up into four phases:\n\nMilestone 1: Project Proposal - Conceptualize and design your project in the abstract, and write a short proposal that includes the project description, expected data needs, timeline, and how you expect to complete it.\n\nMilestone 2: Workflow Design - Develop the analysis workflow for your project, which will typically involve creating at least one core algorithm for processing your data. The model need not be complex or complicated, but it should allow you to analyze spatial data for a new output or to create a new analytical map of some type.\n\nMilestone 3: Data Analysis \u00e2\u0080\u0093 Obtain and preprocess data, run it through your models or other workflows in order to get your rough data products, and begin creating your final map products and/or analysis.\n\nMilestone 4: Web and Print Map Creation \u00e2\u0080\u0093 Complete your project by submitting usable and attractive maps and your data and algorithm for peer review and feedback. Course Overview and Milestone 1: Project Proposal Milestone 1: Project Proposal Submission Milestone 2: Planning Your Workflow Milestone 3: Data Analysis Milestone 3: Data Analysis Continue Milestone 3: Data Analysis Submission Milestone 4: Creating Your Maps Milestone 4: Creating Your Maps Submission In this milestone, you will have weeks 1 and 2 to build a project proposal that contains your research question or hypothesis, background information, potential data sources and methods, and your expected results. This proposal will lead you into future milestones by providing a guide to help keep your analysis on track. You will start by getting an overview of the entire project and the assignment for this first milestone. From there, you will learn about some sources for project ideas and data sources and look at an example project proposal. In this module, you will continue to work through Milestone 1, your project proposal as outlined in the first week. You will then submit your proposal for peer review. In this milestone, you will have week 3 to practice your algorithmic development. In the previous milestone, you posed a question you want to answer - now you'll develop a plan, your algorithm, for how to answer that question with GIS. In practice, this means you'll develop a ModelBuilder model that shows your planned analysis workflow, or some part of it. For those of you who are conducting an analysis that's not conducive to making a model, you can write out your series of steps instead. Regardless, by the end of this module, you'll have a plan for how to produce your results. For this milestone, you will have weeks 4, 5, and 6 to process your data according to the model you created in the previous milestone, reinforcing your data analysis concepts and practice. When you complete your analysis, you will add metadata to any resulting layers, and you will also write an assessment of what the results mean and how they answer your research question. In this module, you will continue to work through Milestone 3, analyzing your data as outlined in the fourth week. Pay close attention to data quality issues and your metadata, as reviewed in this week's videos. You will have one more week to complete your data analysis. In this module, you will continue to work through Milestone 3, analyzing your data as outlined in the fourth and fifth week. You will then submit your data analysis for peer review. In this module, you will have weeks 7 and 8 to hone your map-making skills, building at least two maps that visually interpret the results of your analysis. In making both a web map and a print-layout map, as well as through extra practice materials, you'll refine cartographic techniques that you previously learned as well as new ones to help you to better display information in map form. Should you choose to, you will also build a small website for your project by the time you complete this module. In this module, you will continue to work through Milestone 4, creating your maps as outlined in the seventh week. You will then submit your maps for peer review.", "In this course you will learn a variety of matrix factorization and hybrid machine learning techniques for recommender systems.  Starting with basic matrix factorization, you will understand both the intuition and the practical details of building recommender systems based on reducing the dimensionality of the user-product preference space.  Then you will learn about techniques that combine the strengths of different algorithms into powerful hybrid recommenders. Preface Matrix Factorization (Part 1) Matrix Factorization (Part 2) Hybrid Recommenders Advanced Machine Learning Advanced Topics  This is a two-part, two-week module on matrix factorization recommender techniques.  It includes an assignment and quiz (both due in the second week), and an honors assignment (also due in the second week).  Please pace yourself carefully -- it will be difficult to finish in two weeks unless you start the assignments during the first week.    This is a three-part, two-week module on hybrid and machine learning recommendaton algorithms and advanced recommender techniques.  It includes a quiz (due in the second week), and an honors assignment (also due in the second week).  Please pace yourself carefully -- it will be difficult to finish the honors track in two weeks unless you start the assignments during the first week.    ", "This course will teach you how to perform data analysis using MongoDB's powerful Aggregation Framework.\n\nYou'll begin this course by building a foundation of essential aggregation knowledge. By understanding these features of the Aggregation Framework you will learn how to ask complex questions of your data. This will lay the groundwork for the remainder of the course where you'll dive deep and learn about schema design, relational data migrations, and machine learning with \nMongoDB.\n\nBy the end of this course you'll understand how to best use MongoDB and its Aggregation Framework in your own data science workflow. The Fundamentals of MongoDB Aggregation Leveraging MongoDB's Flexible Schema Machine Learning with MongoDB In this module you'll learn the fundamentals of MongoDB's Aggregation Framework. This will cover basics like filtering and sorting, as well as how to transform array data, how to group documents together, how to join data, and how to traverse graph data. This module is going to be focused on the different ways you can leverage MongoDB's flexible schema. You'll learn how to migrate a relational schema, how to enhance existing schemas, and how to merge datasets via an entity resolution technique. This module is focused on demonstrating how MongoDB can be used in different machine learning workflows. You'll learn how to perform machine learning  directly in MongoDB, how to prepare data for machine learning with MongoDB, and how to analyze data with MongoDB in preparation of doing machine learning in Python.", "Biostatistics is the application of statistical reasoning to the life sciences, and it's the key to unlocking the data gathered by researchers and the evidence presented in the scientific public health literature. In this course, you'll extend simple regression to the prediction of a single outcome of interest on the basis of multiple variables. Along the way, you'll be introduced to a variety of methods, and you'll practice interpreting data and performing calculations on real data from published studies.  Topics include multiple logistic regression, the Spline approach, confidence intervals, p-values, multiple Cox regression, adjustment, and effect modification. An Overview of Multiple Regression for Estimation, Adjustment, and Basic Prediction, and Multiple Linear Regression Multiple Logistic Regression Multiple Cox Regression Course Project Within this module, an overview of multiple regression will be provided. Additionally, examples and applications will be examined. A practice quiz is provided to test your knowledge before completing the graded quiz.  Module two covers examples of multiple logistic regression, basics of model estimates, and a discussion of effect modification. In addition to lectures, you will also be completing a practice quiz and graded quiz.  The last module for this class focuses on multiple Cox regression, the \u00e2\u0080\u009cLinearity\u00e2\u0080\u009d assumption, examples, and applications. You will complete a practice quiz, graded quiz, and project.  During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about self-administration of injectable contraception and one about medical appointment scheduling in Brazil. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken other courses in this specialization, then this scenario will be familiar. ", "This course is an introduction to 3D scientific data visualization, with an emphasis on science communication and cinematic design for appealing to broad audiences. You will develop visualization literacy, through being able to interpret/analyze (read) visualizations and create (write) your own visualizations.\n\nBy the end of this course, you will:\n-Develop visualization literacy.\n-Learn the practicality of working with spatial data.\n-Understand what makes a scientific visualization meaningful.\n-Learn how to create educational visualizations that maintain scientific accuracy.\n-Understand what makes a scientific visualization cinematic.\n-Learn how to create visualizations that appeal to broad audiences.\n-Learn how to work with image-making software. (for those completing the Honors track) Course Orientation Week 1: Introduction Week 2: Data Week 3: Meaningful Communication Week 4: Cinematic Presentation Conclusion You will become familiar with the course, your classmates, and our learning environment. Week 1 is an introduction to the field of data visualization, as well as related fields like computational science and computer graphics. You will learn about different types of data visualization, and visualization best practices. Week 2 is all about data - how are spatial data represented in a computer? How is it formatted? Where can you find it, and how do you work with it? Week 3 is all about the human side of things. How do people learn? How do we perceive visual information? What makes certain methods of communication and education more effective? How do you find a story in a dataset, and how do you tell that story clearly and concisely? Week 4 is about presenting your visualization in an engaging way to broad audiences with techniques like camera design, lighting, compositing, digital cosmetics, and other tricks from Hollywood. You\u00e2\u0080\u0099ll also learn how to package your visualization with sound, titles, and credits, and you\u00e2\u0080\u0099ll learn how to distribute it to various types of audiences. Congratulations on reaching the end of the course!", "Este curso acelerado on demand de 1 semana de duraci\u00c3\u00b3n presenta a los participantes las funciones de macrodatos y aprendizaje autom\u00c3\u00a1tico de Google Cloud Platform (GCP). Ofrece una descripci\u00c3\u00b3n general breve de Google Cloud Platform y profundiza en las funciones de procesamiento de datos.\n\nAl final del curso, los participantes podr\u00c3\u00a1n realizar lo siguiente:\n\u00e2\u0080\u00a2 Identificar el prop\u00c3\u00b3sito y el valor de los productos clave de Google Cloud Platform relacionados con los macrodatos y el aprendizaje autom\u00c3\u00a1tico\n\u00e2\u0080\u00a2 Usar CloudSQL y Cloud Dataproc para migrar cargas de trabajo existentes de MySQL y Hadoop/Pig/Spark/Hive a Google Cloud Platform\n\u00e2\u0080\u00a2 Utilizar BigQuery y Cloud Datalab para realizar an\u00c3\u00a1lisis interactivos de datos\n\u00e2\u0080\u00a2 Elegir entre Cloud SQL, BigTable y Datastore\n\u00e2\u0080\u00a2 Entrenar y usar una red neuronal mediante TensorFlow\n\u00e2\u0080\u00a2 Elegir entre diferentes productos de procesamiento de datos en Google Cloud Platform\n\nPara inscribirse en este curso, los participantes deben tener aproximadamente un (1) a\u00c3\u00b1o de experiencia en uno o m\u00c3\u00a1s de los siguientes:\n\u00e2\u0080\u00a2 Un lenguaje de consultas com\u00c3\u00ban, como SQL\n\u00e2\u0080\u00a2 Actividades de extracci\u00c3\u00b3n, transformaci\u00c3\u00b3n y carga\n\u00e2\u0080\u00a2 Modelado de datos\n\u00e2\u0080\u00a2 Aprendizaje autom\u00c3\u00a1tico o estad\u00c3\u00adsticas\n\u00e2\u0080\u00a2 Programaci\u00c3\u00b3n en Python\n\nNotas sobre la cuenta de Google:\n\u00e2\u0080\u00a2 los servicios de Google no est\u00c3\u00a1n disponibles en China en la actualidad.\n\nBuscando la versi\u00c3\u00b3n en espa\u00c3\u00b1ol de este curso? Visita https://www.coursera.org/learn/gcp-big-data-ml-fundamentals-es/\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u0097\u00a5\u00e6\u009c\u00ac\u00e8\u00aa\u009e\u00e7\u0089\u0088\u00e3\u0082\u0092\u00e3\u0081\u008a\u00e6\u008e\u00a2\u00e3\u0081\u0097\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0081\u008b\u00ef\u00bc\u009fhttps://www.coursera.org/learn/gcp-big-data-ml-fundamentals-jp/ Introducci\u00c3\u00b3n a la Especializaci\u00c3\u00b3n en datos y  aprendizaje autom\u00c3\u00a1tico de Google Cloud Platform Introducci\u00c3\u00b3n a Google\u00c2\u00a0Cloud\u00c2\u00a0Platform y a sus productos para macrodatos M\u00c3\u00b3dulo 2: Fundamentos de Google Cloud Platform: Compute y Storage M\u00c3\u00b3dulo 3: An\u00c3\u00a1lisis de Datos en la Nube Escalamiento del an\u00c3\u00a1lisis de datos: Procesamiento con GCP Arquitecturas de procesamiento de datos: Escalabilidad en la transferencia, la transformaci\u00c3\u00b3n y la carga  Resumen de Google\u00c2\u00a0Cloud\u00c2\u00a0Platform, macrodatos y AA  En este m\u00c3\u00b3dulo, presentaremos qu\u00c3\u00a9 es Google Cloud Platform y los aspectos de la plataforma relacionados con el manejo de datos. En este m\u00c3\u00b3dulo, presentaremos los fundamentos de Google Cloud Platform: Compute y Storage. Explicaremos c\u00c3\u00b3mo trabajan para ofrecer transferencia de datos, almacenamiento y an\u00c3\u00a1lisis federado. En este m\u00c3\u00b3dulo, presentaremos los casos de uso m\u00c3\u00a1s comunes de Big Data que Google administrar\u00c3\u00a1 por usted. Se trata de las actividades que la industria lleva a cabo m\u00c3\u00a1s a menudo en la actualidad y para las que ofrecemos una migraci\u00c3\u00b3n sencilla a la nube. Este m\u00c3\u00b3dulo aborda las tecnolog\u00c3\u00adas m\u00c3\u00a1s transformadoras de Google Cloud Platform, que podr\u00c3\u00adan no tener puntos en com\u00c3\u00ban evidentes con las tecnolog\u00c3\u00adas que los asistentes usan actualmente (\u00e2\u0080\u009cpr\u00c3\u00b3ximas tendencias\u00e2\u0080\u009d). En este m\u00c3\u00b3dulo, se presentan las arquitecturas de procesamiento de datos de Google Cloud Platform: Procesamiento as\u00c3\u00adncrono con TaskQueues, arquitecturas orientadas a los mensajes con Pub/Sub y creaci\u00c3\u00b3n de canalizaciones con Dataflow. ", "In this course, you will learn the fundamental techniques for making personalized recommendations through nearest-neighbor techniques.  First you will learn user-user collaborative filtering, an algorithm that identifies other people with similar tastes to a target user and combines their ratings to make recommendations for that user.  You will explore and implement variations of the user-user algorithm, and will explore the benefits and drawbacks of the general approach.  Then you will learn the widely-practiced item-item collaborative filtering algorithm, which identifies global product associations from user ratings, but uses these product associations to provide personalized recommendations based on a user's own product ratings. Preface User-User Collaborative Filtering Recommenders Part 1 User-User Collaborative Filtering Recommenders Part 2 Item-Item Collaborative Filtering Recommenders Part 1 Item-Item Collaborative Filtering Recommenders Part 2 Advanced Collaborative Filtering Topics Note that this course is structured into two-week chunks.  The first chunk focuses on User-User Collaborative Filtering; the second chunk on Item-Item Collaborative Filtering.  Each chunk has most of the lectures in the first week, and assignments/quizzes and advanced topics in the second week.  We encourage learners to treat each two-week chunk as one unit, starting the assignments as soon as they feel they have learned enough to get going.     ", "Health data are notable for how many types there are, how complex they are, and how serious it is to get them straight. These data are used for treatment of the patient from whom they derive, but also for other uses. Examples of such secondary use of health data include population health (e.g., who requires more attention), research (e.g., which drug is more effective in practice), quality (e.g., is the institution meeting benchmarks), and translational research (e.g., are new technologies being applied appropriately). By the end of this course, students will recognize the different types of health and healthcare data, will articulate a coherent and complete question, will interpret queries designed for secondary use of EHR data, and will interpret the results of those queries. Introduction to Databases and Data Types Data Sources and Data Challenges Formulating Data Questions Real World Applications of Data Science in Health Informatics In this module, we will begin by introducing and defining databases, and placing the role of databases within the context of clinical informatics. We will continue by introducing the common health data types such as demographics, diagnosis, medications, procedures, and utilization data. We will finish this module by reviewing the emerging health data such as lab orders/results, vital signs, social data, and patient-generated data. In this module, we review the data specifications extracted from insurance claims and electronic health records. We will then discuss the common challenges in using health data, specifically issues with data quality, data interoperability, and data system architectures. Finally, we will describe the \u00e2\u0080\u009cBig Data\u00e2\u0080\u009d challenges of health data and explain some of the data problems that may hinder analytical efforts. With this understanding of the data available, it\u00e2\u0080\u0099s time to see how to turn questions you and your colleagues will have into queries the database can understand. Besides getting rules of thumb for doing this translation, you will also be introduced to three online tools available to test some of these skills. You will also watch an interview with Sam Meiselman, course instructor and the data manager in charge of the Johns Hopkins Enterprise Data Warehouse, who has to use these skills on a daily basis.  To send home the recurring message on the challenges and art of translating questions into queries, you will see interviews with two professionals: One who comes from the data management side of the equation, and one who comes from the domain. They will give you perspectives that are both similar (the need to understand the problem for which the data are being retrieved) and different (the multiplicity of data available vs the richness of the domain problem). ", "This join course created by SPSU and ETU  includes 5 modules dedicated to different stages of the system development. Its modules represent several widely separated fields of biomedical engineering. We interconnect them by applying the knowledge from them all to a common task \u00e2\u0080\u0093 the development of a prototype of an mHealth ECG system with built-in data-driven signal processing and analysis. Working on this task throughout the course, you will acquire a knowledge on how these branches of science, including electronics, mathematics, data science and programming are applied together in a real project. Pieces of hardware and software, as well as the data sets that we utilize in this course are the same components that we use in our work developing prototypes of devices and algorithms for our tasks in science and engineering.\nThe course is a joint work of Saint Petersburg State University and Saint Petersburg Electrotechnical University ETU (\"LETI\").\nNote that the goal of the course is not to provide you with fundamental knowledge on any of the topics highlighted in the modules, but to give you some useful skills on implementing them in practical tasks. Remote health monitoring system hardware Data Exchange Between Device And Personal Computer Preprocessing of Biomedical Signals Event Detection in Biomedical Signals Developing Data-Driven Recommendation System Welcome to Module 1! Medical systems for remote monitoring of patients have become extremely popular in recent years. Most of them have a similar structure, which will be discussed in detail in this module using the example of an electrocardiogram signal registration device. We will talk about hardware part of modern ECG recorders, and problems, connected with processing of biomedical signals. Welcome to Module 2! The implementation of the protocol for transferring data from a patient\u00e2\u0080\u0099s wearable device to a computer is an extremely important step in the entire development of a telemedicine system. This module will consider the easiest and most affordable wired data transfer method using the RS-232 interface, virtual Com ports and the MatLab software environment. Welcome to Module 3! Use you may know, biomedical signals are corrupted by a significant amount of noise. So, noise removal is used in order to increase signal quality. We will talk about basics method to prepare your signal for future analysis. In the Programming part of the Module we will learn how to evaluate and analyze ECG-signal spectrum and create a digital filter using MATLAB. Welcome to Module 4! In most cases, biomedical signal analysis assumes that we have some reference or basic events in the signal. It can be QRS-complexes (for ECG), breaths (for spirogram), eyes movements (for EEG) or steps (for accelerometric signal). We will look closely to this task in the context of ECG-analysis. You will learn different QRS-detection algorithms and create QRS-detector using MATLAB. Welcome to Module 5! In this module you will further develop your mobile-based health monitoring system. How to deal with extracted features and how can they help you in creating recommendations \u00e2\u0080\u0093 these are the primary questions for this module. This is a very broad topic, involving methods from statistical analysis, machine learning and medical practice. We will study a practical approach to use these methods in developing monitoring systems on the example, which is, in our case, a recognition of noisy ECG complexes and their removal.", "Choosing an appropriate study design is a critical decision that can largely determine whether your study will successfully answer your research question. A quick look at the contents page of a biomedical journal or even at the health news section of a news website is enough to tell you that there are many different ways to conduct epidemiological research. \nIn this course, you will learn about the main epidemiological study designs, including cross-sectional and ecological studies, case-control and cohort studies, as well as the more complex nested case-control and case-cohort designs. The final module is dedicated to randomised controlled trials, which is often considered the optimal study design, especially in clinical research. You will also develop the skills to identify strengths and limitations of the various study designs. By the end of this course, you will be able to choose the most suitable study design considering the research question, the available time, and resources. Introduction to Study Designs: Ecological and Cross-Sectional Studies Case Control Studies Cohort Studies and Nested Studies Randomised Controlled Trials The range of different study designs can be quite confusing. However, to help you navigate the maze of study designs, we can split them into groups which share common characteristics. In this module, you will be introduced to these common characteristics, and you will learn the main principles of ecological and cross-sectional studies, as well as when it is appropriate to use them. By the end of the module, you will be able to identify and critically consider the advantages and disadvantages of these study designs. This module focuses on case-control studies, which is one of the best known epidemiological study designs. Case-control studies are particularly useful when you don\u00e2\u0080\u0099t have the luxury of waiting for a long follow-up period to conclude. In this module, you will learn the key elements of case-control study design, and you will learn how to estimate the appropriate measure of association when presented with data from a case-control study. \n Cohorts were ancient Roman military units, but in modern epidemiology the word \u00e2\u0080\u009ccohort\u00e2\u0080\u009d is used to describe a group with a shared characteristic. In cohort studies, we follow groups of people over time, we collect data on their exposure and outcome, and try to estimate whether there is an association between the group-defining characteristic and the outcome of interest. In this module, you will learn how to design such a study, the kind of problems which may arise and how it compares with case-control studies. You will also learn about nested case-control and case-cohort studies, which allow us to harness the advantages of cohort studies in more efficient ways. By the end of the module, you will be able to choose the best study design in a variety of contexts.\n\n Randomised controlled trials are often seen as the gold standard of epidemiological research, especially in clinical settings, and in this module you will learn why. You will learn the main design features of randomised clinical trials, why they are so important, and the difficulties and limitations in applying these principles in real life. By the end of this module, you will know how to design a randomised clinical trial and how to decide which is the best analytical approach for the data you have obtained.\n", "An introduction to data integration and statistical methods used in contemporary Systems Biology, Bioinformatics and Systems Pharmacology research. The course covers methods to process raw data from genome-wide mRNA expression studies (microarrays and RNA-seq) including data normalization, differential expression, clustering, enrichment analysis and network construction. The course contains practical tutorials for using tools and setting up pipelines, but it also covers the mathematics behind the methods applied within the tools. The course is mostly appropriate for beginning graduate students and advanced undergraduates majoring in fields such as biology, math, physics, chemistry, computer science, biomedical and electrical engineering. The course should be useful for researchers who encounter large datasets in their own research. The course presents software tools developed by the Ma\u00e2\u0080\u0099ayan Laboratory (http://labs.icahn.mssm.edu/maayanlab/) from the Icahn School of Medicine at Mount Sinai, but also other freely available data analysis and visualization tools. The ultimate aim of the course is to enable participants to utilize the methods presented in this course for analyzing their own data for their own projects. For those participants that do not work in the field, the course introduces the current research challenges faced in the field of computational systems biology. Course Overview and Introductions Topological and Network Evolution Models Types of Biological Networks Data Processing and Identifying Differentially Expressed Genes Gene Set Enrichment and Network Analyses Deep Sequencing Data Processing and Analysis Principal Component Analysis, Self-Organizing Maps, Network-Based Clustering and Hierarchical Clustering Resources for Data Integration Crowdsourcing: Microtasks and Megatasks Final Exam The 'Introduction to Complex Systems' module discusses complex systems and leads to the idea that a cell can be considered a complex system or a complex agent living in a complex environment just like us. The 'Introduction to Biology for Engineers' module provides an introduction to some central topics in cell and molecular biology for those who do not have the background in the field. This is not a comprehensive coverage of cell and molecular biology. The goal is to provide an entry point to motivate those who are interested in this field, coming from other disciplines, to begin studying biology. In the 'Topological and Network Evolution Models' module, we provide several lectures about a historical perspective of network analysis in systems biology. The focus is on in-silico network evolution models. These are simple computational models that, based of few rules, can create networks that have a similar topology to the molecular networks observed in biological systems.  The 'Types of Biological Networks' module is about the various types of networks that are typically constructed and analyzed in systems biology and systems pharmacology. This lecture ends with the idea of functional association networks (FANs). Following this lecture are lectures that discuss how to construct FANs and how to use these networks for analyzing gene lists.   This set of lectures in the 'Data Processing and Identifying Differentially Expressed Genes' module first discusses data normalization methods, and then several lectures are devoted to explaining the problem of identifying differentially expressed genes with the focus on understanding the inner workings of a new method developed by the Ma'ayan Laboratory called the Characteristic Direction.  In the 'Gene Set Enrichment and Network Analyses' module the emphasis is on tools developed by the Ma'ayan Laboratory to analyze gene sets. Several tools will be discussed including: Enrichr, GEO2Enrichr, Expression2Kinases and DrugPairSeeker. In addition, one lecture will be devoted to a method we call enrichment vector clustering we developed, and two lectures will describe the popular gene set enrichment analysis (GSEA) method and an improved method we developed called principal angle enrichment analysis (PAEA). A set of lectures in the 'Deep Sequencing Data Processing and Analysis' module will cover the basic steps and popular pipelines to analyze RNA-seq and ChIP-seq data going from the raw data to gene lists to figures. These lectures also cover UNIX/Linux commands and some programming elements of R, a popular freely available statistical software. Note that since these lectures were developed and recorded during the Fall of 2013, it is possible that there are better tools that should be used now since the field is rapidly advancing.   This module is devoted to various method of clustering: principal component analysis, self-organizing maps, network-based clustering and hierarchical clustering. The theory behind these methods of analysis are covered in detail, and this is followed by some practical demonstration of the methods for applications using R and MATLAB. The lectures in the 'Resources for Data Integration' module are about the various types of networks that are typically constructed and analyzed in systems biology and systems pharmacology. These lectures start with the idea of functional association networks (FANs). Following this lecture are several lectures that discuss how to construct FANs from various resources and how to use these networks for analyzing gene lists as well as to construct a puzzle that can be used to connect genomic data with phenotypic data.  The final set of lectures presents the idea of crowdsourcing. MOOCs provide the opportunity to work together on projects that are difficult to complete alone (microtasks) or compete for implementing the best algorithms to solve hard problems (megatasks). You will have the opportunity to participate in various crowdsourcing projects: microtasks and megatasks. These projects are designed specifically for this course. The final exam consists of multiple choice questions from topics covered in all of modules of the course. Some of the questions may require you to perform some of the analysis methods you learned throughout the course on new datasets. ", "Machine learning is transforming the world around us. To become successful, you\u00e2\u0080\u0099d better know what kinds of problems can be solved with machine learning, and how they can be solved. Don\u00e2\u0080\u0099t know where to start? The answer is one button away.\n \nDuring this course you will:\n- Identify practical problems which can be solved with machine learning\n- Build, tune and apply linear models with Spark MLLib\n- Understand methods of text processing\n- Fit decision trees and boost them with ensemble learning\n- Construct your own recommender system.\n \nAs a practical assignment, you will \n- build and apply linear models for classification and regression tasks; \n- learn how to work with texts; \n- automatically construct decision trees and improve their performance with ensemble learning; \n- finally, you will build your own recommender system!\n\nWith these skills, you will be able to tackle many practical machine learning tasks.\n \nWe provide the tools, you choose the place of application to make this world of machines more intelligent.\n\nSpecial thanks to:\n- Prof. Mikhail Roytberg, APT dept., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the road.\n- Oleg Sukhoroslov (PhD, Senior Researcher at IITP RAS), who has been teaching  MapReduce, Hadoop and friends since 2008. Now he is leading the infrastructure team.\n- Oleg Ivchenko (PhD student APT dept., MIPT), Pavel Akhtyamov (MSc. student at APT dept., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl State University), superbrains who have developed and now maintain the infrastructure used for practical assignments in this course.\n- Asya Roitberg, Eugene Baulin, Marina Sudarikova. These people never sleep to babysit this course day and night, to make your learning experience productive, smooth and exciting. Welcome (Optional) Machine Learning: Introduction Spark MLLib and Linear Models Machine Learning with Texts & Feature Engineering Decision Trees & Ensemble Learning Recommender Systems Recommender Systems (practice week)       ", "In this course, you will develop and test hypotheses about your data. You will learn a variety of statistical tests, as well as strategies to know how to apply the appropriate one to your specific data and question. Using your choice of two powerful statistical software packages (SAS or Python), you will explore ANOVA, Chi-Square, and Pearson correlation analysis. This course will guide you through basic statistical principles to give you the tools to answer questions you have developed. Throughout the course, you will share your progress with others to gain valuable feedback and provide insight to other learners about their work. Hypothesis Testing and ANOVA Chi Square Test of Independence Pearson Correlation Exploring Statistical Interactions This session starts where the Data Management and Visualization course left off. Now that you have selected a data set and research question, managed your variables of interest and visualized their relationship graphically, we are ready to test those relationships statistically. The first group of videos describe the process of hypothesis testing which you will use throughout this course to test relationships between different kinds of variables (quantitative and categorical). Next, we show you how to test hypotheses in the context of Analysis of Variance (when you have one quantitative variable and one categorical variable). Your task will be to write a program that manages any additional variables you may need and runs and interprets an Analysis of Variance test. Note that if your research question does not include one quantitative variable, you can use one from your data set just to get some practice with the tool. If your research question does not include a categorical variable, you can categorize one that is quantitative. This session shows you how to test hypotheses in the context of a Chi-Square Test of Independence (when you have two categorical variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a Chi-Square Test of Independence. Note that if your research question only includes quantitative variables, you can categorize those just to get some practice with the tool.  This session shows you how to test hypotheses in the context of a Pearson Correlation (when you have two quantitative variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a correlation coefficient. Note that if your research question only includes categorical variables, you can choose other variables from your data set just to get some practice with the tool.  In this session, we will discuss the basic concept of statistical interaction (also known as moderation). In statistics, moderation occurs when the relationship between two variables depends on a third variable. The effect of a moderating variable is often characterized statistically as an interaction; that is, a third variable that affects the direction and/or strength of the relation between your explanatory (X) and response (Y) variable. Your task will be to test your own research question in the context of one or more potential moderating variables. ", "Welcome to the specialization course Business Intelligence and Data Warehousing. This course will be completed on six weeks, it will be supported with videos and various documents that will allow you to learn in a very simple way how to identify, design and develop analytical information systems, such as Business Intelligence with a descriptive analysis on data warehouses. You will be able to understand the problem of integration and predictive analysis of high volume of unstructured data (big data) with data mining and the Hadoop framework.\n\nAfter completing this course, a learner will be able to\n\u00e2\u0097\u008f\tCreate a Star o Snowflake data model Diagram through the Multidimensional Design from analytical business requirements and OLTP system\n\u00e2\u0097\u008f\tCreate a physical database system \n\u00e2\u0097\u008f\tExtract, Transform and load data to a data-warehouse.\n\u00e2\u0097\u008f\tProgram analytical queries with SQL using MySQL\n\u00e2\u0097\u008f\tPredictive analysis with RapidMiner\n\u00e2\u0097\u008f\tLoad relational or unstructured data to Hortonworks HDFS\n\u00e2\u0097\u008f\tExecute Map-Reduce jobs to query data on HDFS for analytical purposes\n\n\nProgramming languages:\nFor course 2 you will use the MYSQL language.\n\nSoftware to download:\nRapidminer\nMYSQL\nExcel\nHortonworks Hadoop framework\n\nIn case you have a Mac / IOS operating system you will need to use a virtual Machine (VirtualBox, Vmware). Introduction to Business Intelligence as Analytical System Designing a Data Warehouse The ETL process and Analytical queries with SQL  Predictive Analytics with Data mining The problem of integration and analysis of unstructured data  Big Data and Hadoop Framework In the first module named Introduction to Business Intelligence as Analytical System, we will learn how the steps of the process of datawarehousing to automate analytical processes that companies need for their business strategies. Let's start! After completing this module, a learner will be able to identify the entire process of datawarehousing, which consist on OLAP design concepts and multidimensional modelling. The learner will be able to design and create a data warehouse from OLAP requirements. After completing this module, a learner will differentiate from structured and unstructured data and will be able to extract, transform and load data into a datawarehouse. The student will also be able to program and execute OLAP queries with SQL. After completing this module, a learner will identify the main data mining tasks and some algorithms for classification, regression and clustering  for predictive and descriptive analysis on business intelligence.  After completing this module, a learner will learn the types of data according to structure and how to integrate, store and analyze unstructured data. After completing this module, a learner will understand the problem of big data, a possible solution to the analysis of big data with the Hadoop ecosystem and under which conditions should be apply each element of this ecosystem.", "Welcome to the specialization course of NoSQL Systems. \nThis course will be completed on six weeks, it will be supported with videos and exercises that will allow you to identify the differences between the relational and NoSQL databases. \nAs part of these alternative technologies the student will learn the main characteristics and how to implement the typical NoSQL databases, such as Key-value, columnar, document and graph. \nLet's start!\n\nAfter completing this course, a learner will be able to\n\u00e2\u0097\u008f\tIdentify what type of NoSQL database to implement based on business requirements (key-value, document, full text, graph, etc.)\n\u00e2\u0097\u008f\tApply NoSQL data modeling from application specific queries\n\u00e2\u0097\u008f\tUse Atomic Aggregates and denormalization as data modelling techniques to optimize query processing\n\nSoftware to download:\nMongoDB\nNeo4j\nSAPIQ\nCassandra\n\nIn case you have a Mac / IOS operating system you will need to use a virtual Machine (VirtualBox, Vmware). NOSQL Systems Key-value database  Columnar Databases Document databases with MongoDB  Graph Databases How to design reliable, scalable and maintainable applications Welcome to the specialization course of NoSQL Systems. \nThis course will be completed on six weeks, it will be supported with videos and exercises that will allow you to identify the differences between the relational and NoSQL databases. \nAs part of these alternative technologies the student will learn the main characteristics and how to implement the typical NoSQL databases, such as Key-value, columnar, document and graph. \nLet's start! Welcome to the module key-value database. We will learn the components and types of a key-value database, its properties, scalability and indexing. Let's start! Welcome to the session columnar databases from the NoSQL course.\nThe learner will understand why a columnar database performs better than a relational in the case of analytical queries.\n Welcome to the session document databases with MongoDB.\nThe learner will identify the advantages of storing semistructured data with MongoDB.\n Welcome to the session graph databases from the NoSQL course.\nThe learner will understand that a graph database is a perfect solution for information systems where the relationships between entities are more like graphs or trees which are structures more flexibles. Welcome to the session How to design reliable, scalable and maintainable applications. The student will identify which database or repository is the best option according to response time, amount of data, type of data and analysis. \nThe student will learn the last database technologies such as in-memory database, multi-model database, etc. and how these approaches can help to design reliable, scalable and maintainable applications. ", "This is the second course in the four-course specialization Python Data Products for Predictive Analytics, building on the data processing covered in Course 1 and introducing the basics of designing predictive models in Python. In this course, you will understand the fundamental concepts of statistical learning and learn various methods of building predictive models. At each step in the specialization, you will gain hands-on experience in data manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization. Week 1: Supervised Learning & Regression Week 2: Features  Week 3: Classification Week 4: Gradient Descent Final Project Welcome to the second course in this specialization! This week, we will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of supervised learning and regression. This week, we will learn what features are in a dataset and how we can work with them through cleaning, manipulation, and analysis in Jupyter notebooks. This week, we will learn about classification and several ways you can implement it, such as K-nearest neighbors, logistic regression, and support vector machines. This week, we will learn the importance of properly training and testing a model. We will also implement gradient descent in both Python and TensorFlow. In the final week of this course, you will continue building on the project from the first course of Python Data Products for Predictive Analytics with simple predictive machine learning algorithms. Find a dataset, clean it, and perform basic analyses on the data.", "In this course you will learn how to apply satisfiability (SAT/SMT) tools to solve a wide range of problems.\nSeveral basic examples are given to get the flavor of the applications: fitting rectangles to be applied for printing posters, scheduling problems, solving puzzles, and program correctness. Also underlying theory is presented: resolution as a basic approach for propositional satisfiability, the CDCL framework to scale up for big formulas, and the simplex method to deal with linear inequallities.\n\nThe light weight approach to following this course is just watching the lectures and do the corresponding quizzes. To get a flavor of the topic this may work out fine. However, the much more interesting approach is to use this as a basis to apply SAT/SMT yourself on several problems, for instance on the problems presented in the honor's assignment. SAT/SMT basics, SAT examples SMT applications Theory and algorithms for CNF-based SAT Theory and algorithms for SAT/SMT This module introduces SAT (satisfiability) and SMT (SAT modulo theories) from scratch, and gives a number of examples of how to apply SAT. This module shows a number of applications of satisfiability modulo the theory of linear inequalities (SMT) This module describes how a rule called Resolution serves to determine whether a propositional formula in conjunctive normal form (CNF) is unsatisfiable. It is shown how an approach called DPLL does the same job, and how it is related to resolution. Finally, it is shown how current SAT solvers essentially implement and optimize DPLL. This module consists of two parts.\nThe first part is about transforming arbitrary propositional formulas to CNF, leading to the Tseitin transformation doing this job such that the size of the transformed formula is linear in the size of the original formula.\nThe second part is about extending SAT to SMT, in particular to dealing with linear inequalities. It is shown how the Simplex method for linear optimization serves for this job; the Simplex method itself is explained in detail.", "Relational Database Support for Data Warehouses is the third course in the Data Warehousing for Business Intelligence specialization. In this course, you'll use analytical elements of SQL for answering business intelligence questions. You'll learn features of relational database management systems for managing summary data commonly used in business intelligence reporting. Because of the importance and difficulty of managing implementations of data warehouses, we'll also delve into storage architectures, scalable parallel processing, data governance, and big data impacts. DBMS Extensions and Example Data Warehouses SQL Subtotal Operators SQL Analytic Functions Materialized View Processing and Design Physical Design and Governance Module 1 introduces the course and covers concepts that provide a context for the remainder of this course. In the first two lessons, you\u00e2\u0080\u0099ll understand the objectives for the course and know what topics and assignments to expect. In the remaining lessons, you will learn about DBMS extensions, a review of schema patterns, data warehouses used in practice problems and assignments, and examples of data warehouses in education and health care. This informational module will ensure that you have the background for success in later modules that emphasize details and hands-on skills.You should also read about the software requirements in the lesson at the end of module 1. I recommend that you try to install the Oracle software this week before assignments begin in week 2. If you have taken other courses in the specialization, you may already have installed the Oracle software. Now that you have the informational context for relational database support of data warehouses, you\u00e2\u0080\u0099ll start using relational databases to write business intelligence queries! In module 2, you will learn an important extension of the SQL SELECT statement for subtotal operators. You\u00e2\u0080\u0099ll apply what you\u00e2\u0080\u0099ve learned in practice and graded problems using Oracle SQL for problems involving the CUBE, ROLLUP, and GROUPING SETS operators. Because the subtotal operators are part of the SQL standard, your learning will readily apply to other enterprise DBMSs. At the end of this module, you will have solid background to write queries using the SQL subtotal operators as a data warehouse analyst. After your experience using the SQL subtotal operators, you are ready to learn another important SQL extension for business intelligence applications. In module 3, you will learn about an extended processing model for SQL analytic functions that support common analysis in business intelligence applications. You\u00e2\u0080\u0099ll apply what you\u00e2\u0080\u0099ve learned in practice and graded problems using Oracle SQL for problems involving qualitative ranking of business units, window comparisons showing relationships of business units over time, and quantitative contributions showing performance thresholds and contributions of individual business units to a whole business. Because analytic functions are part of the SQL standard, your learning will apply to other enterprise DBMSs. At the end of this module, you will have solid background to write queries using the SQL analytic functions as a data warehouse analyst. After acquiring query formulation skills for development of business intelligence applications, you are ready to learn about DBMS extensions for efficient query execution. Business intelligence queries can use lots of resources so materialized view processing and design has become an important extension of DBMSs. In module 4, you will learn about an SQL statement for creating materialized views, processing requirements for materialized views, and rules for rewriting queries using materialized views. To gain insight about the complexity of query rewriting, you will practice rewriting queries using materialized views. To provide closure about relational database support for data warehouses, you will learn about about Oracle tools for data integration, the Oracle Data Integrator, along with two SQL statements useful for specific data integration tasks. After this module, you will have a solid background to use materialized views to improve query performance and deploy the Extraction, Loading, and Transformation approach for data integration as a data warehouse administrator or analyst. Module 5 finishes the course with a return to conceptual material about physical design technologies and data governance practices. You will learn about storage architectures, scalable parallel processing, big data issues, and data governance. After this module, you will have background about conceptual issues important for data warehouse administrators.", "This course focuses on one of the most important tools in your data analysis arsenal: regression analysis. Using either SAS or Python, you will begin with linear regression and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify confounding variables, which can tell a more compelling story about your results. You will learn the assumptions underlying regression analysis, how to interpret regression coefficients, and how to use regression diagnostic plots and other tools to evaluate the quality of your regression model. Throughout the course, you will share with others the regression models you have developed and the stories they tell you. Introduction to Regression Basics of Linear Regression Multiple Regression Logistic Regression This session starts where the Data Analysis Tools course left off. This first set of videos provides you with some conceptual background about the major types of data you may work with, which will increase your competence in choosing the statistical analysis that\u00e2\u0080\u0099s most appropriate given the structure of your data, and in understanding the limitations of your data set. We also introduce you to the concept of confounding variables, which are variables that may be the reason for the association between your explanatory and response variable. Finally, you will gain experience in describing your data by writing about your sample, the study data collection procedures, and your measures and data management steps.  In this session, we discuss more about the importance of testing for confounding, and provide examples of situations in which a confounding variable can explain the association between an explanatory and response variable. In addition, now that you have statistically tested the association between an explanatory variable and your response variable, you will test and interpret this association using basic linear regression analysis for a quantitative response variable. You will also learn about how the linear regression model can be used to predict your observed response variable. Finally, we will also discuss the statistical assumptions underlying the linear regression model, and show you some best practices for coding your explanatory variables\nNote that if your research question does not include one quantitative response variable, you can use one from your data set just to get some practice with the tool. \n Multiple regression analysis is tool that allows you to expand on your research question, and conduct a more rigorous test of the association between your explanatory and response variable by adding additional quantitative and/or categorical explanatory variables to your linear regression model. In this session, you will apply and interpret a multiple regression analysis for a quantitative response variable, and will learn how to use confidence intervals to take into account error in estimating a population parameter. You will also learn how to account for nonlinear associations in a linear regression model. Finally, you will develop experience using regression diagnostic techniques to evaluate how well your multiple regression model predicts your observed response variable. \nNote that if you have not yet identified additional explanatory variables, you should choose at least one additional explanatory variable from your data set. When you go back to your codebooks, ask yourself a few questions like \u00e2\u0080\u009cWhat other variables might explain the association between my explanatory and response variable?\u00e2\u0080\u009d; \u00e2\u0080\u009cWhat other variables might explain more of the variability in my response variable?\u00e2\u0080\u009d, or even \u00e2\u0080\u009cWhat other explanatory variables might be interesting to explore?\u00e2\u0080\u009d Additional explanatory variables can be either quantitative, categorical, or both. Although you need only two explanatory variables to test a multiple regression model, we encourage you to identify more than one additional explanatory variable. Doing so will really allow you to experience the power of multiple regression analysis, and will increase your confidence in your ability to test and interpret more complex regression models. If your research question does not include one quantitative response variable, you can use the same quantitative response variable that you used in Module 2, or you may choose another one from your data set.  In this session, we will discuss some things that you should keep in mind as you continue to use data analysis in the future. We will also teach also you how to test a categorical explanatory variable with more than two categories in a multiple regression analysis. Finally, we introduce you to logistic regression analysis for a binary response variable with multiple explanatory variables. Logistic regression is simply another form of the linear regression model, so the basic idea is the same as a multiple regression analysis. But, unlike the multiple regression model, the logistic regression model is designed to test binary response variables. You will gain experience testing and interpreting a logistic regression model, including using odds ratios and confidence intervals to determine the magnitude of the association between your explanatory variables and response variable.   \nYou can use the same explanatory variables that you used to test your multiple regression model with a quantitative outcome, but your response variable needs to be binary (categorical with 2 categories). If you have a quantitative response variable, you will have to bin it into 2 categories. Alternatively, you can choose a different binary response variable from your data set that you can use to test a logistic regression model. If you have a categorical response variable with more than two categories, you will need to collapse it into two categories.\n", "This course has a singular and clear aim, to empower you to do statistical tests, ready for incorporation into your dissertations, research papers, and presentations.  The ability to summarize data, create plots and charts, and to do the tests that you commonly see in the literature is a powerful skill indeed.  Not only will it further your career, but it will put you in the position to contribute to the advancement of humanity through scientific research. \n\nWe live in a wonderful age with great tools at our disposal, ready to achieve this goal.  None are quite as easy to learn, yet as powerful to use, as the Wolfram Language.  Knowledge is literally built into the language.  With its well-structured and consistent approach to creating code, you will become an expert in no time. \n\nThis course follows the modern trend of learning statistical analysis through the use of a computer language.  It requires no prior knowledge of coding.  An exciting journey awaits. If you wanting even more, there are optional Honors lessons on machine learning that cover the support in the Wolfram Language for deep learning. Week 1 Week 2 Week 3 Week 4 This first week establishes the aims of the course and motivation for using the Wolfram Language.  We aim to support you in gaining a remarkable new set of skills for doing statistical analysis that you can continue to use long after you complete the course. We will also describe the process of procuring the software that you will use in the course.  The first is the absolutely free version, which is software as a service, meaning it runs in any web browser. The second is desktop desktop version. If you work or study at an institution with a site licence, you will be able to get the software for free. There is also the option to purchase your own licence. In week 2, we start with some actual coding, now that you know about the Wolfram Language and its different coding environments. We start off with a demonstration of a completed project, though.  It is just a little teaser, showcasing what you will be able to do at the end. Since statistical tests are all about math (don't worry, this course is not about the math),  in module/chapter five we are going to learn to code by doing simple arithmetic.  That is addition, subtraction, multiplication, and so on. Once you have realized just how simple these tasks are, you will be introduced to the way in which data is stored in a computer language in module/chapter six.  This is the stepping stone required to bringing in your own data, ready for the analyses in weeks three and four. In week 3, its time to start analyzing data, now that you can write some code and import your data. The two most important steps to understand the message hidden in data, are to summarize and visualize it. Descriptive statistics turn rows and columns of data into something that we as humans can understand.  By summarizing values and replacing them with single values, we start to get an idea of what our analyses might show. Visualizing the data is an even better way of getting to grips with data.  Box-and-whisker plots, scatter plots, bar charts, and the like are wonderful ways to augment your understanding of the data. The Wolfram Language makes summary statistics easy but it really shines when creating plots.  There are almost no limits to customizing plots.  No matter what your project requirements, you will learn to create plots that work for you. Starting this week is an optional Honors lessons that introduce machine learning using the Wolfram Language.  This final week covers all the common statistical tests - going from Student's t-test to analysis of variance to chi-squared tests.  We conclude the course with a run-through of the demonstration research project that you saw at the beginning of week two.  This brings together all the skills that you have acquired during the course and prepares you for the final exam. You will also have the opportunity to create your own computational essay, if you are not content with just working through the demonstration project. For those following the optional Honors lessons there is an introduction to deep learning using the Wolfram Language. ", "Mathematical Matrix Methods lie at the root of most methods of machine learning and data analysis of tabular data.  Learn the basics of Matrix Methods, including matrix-matrix multiplication, solving linear equations, orthogonality, and best least squares approximation.   Discover the Singular Value Decomposition that plays a fundamental role in dimensionality reduction, Principal Component Analysis, and noise reduction.  Optional examples using Python are used to illustrate the concepts and allow the learner to experiment with the algorithms. Matrices as Mathematical Objects Matrix Multiplication and other Operations Systems of Linear Equations Linear Least Squares Singular Value Decomposition     ", "This MOOC \u00e2\u0080\u0093 a joint initiative between EIT Digital, Universit\u00c3\u00a9 de Nice Sophia-Antipolis / Universit\u00c3\u00a9 C\u00c3\u00b4te d'Azur and INRIA - introduces the Linked Data standards and principles that provide the foundation of the Semantic web. You will learn how to publish, obtain and use structured data directly from the Web. Learning the principles, languages and standards to exchange Data on the Web will enable you to design and produce new applications, products and services that leverage the volume and variety of data the Web holds.\n\nWe divided this course into four parts that cover the core technical skills and competencies you need to master to be able to use the Web as a space for giant structure data exchange:\n\u00e2\u0080\u00a2    in the first part, \u00e2\u0080\u009cPrincipals of a Web of Linked Data\u00e2\u0080\u009d: you will learn and practice the principles to publish and obtain data directly on the Web instead of Web pages; \n\u00e2\u0080\u00a2    in the second part, \u00e2\u0080\u009cThe RDF Data Model\u00e2\u0080\u009d: you will learn the standard data model for the Web and its syntaxes to publish and link data on the Web in your applications and services;\n\u00e2\u0080\u00a2    in the third part, \u00e2\u0080\u009cSPARQL Query Language\u00e2\u0080\u009d: you will learn how to directly query and access data sources on the Web and obtain structured data relevant to your activity and domain;\n\u00e2\u0080\u00a2    in the fourth and final part, \u00e2\u0080\u009cIntegration of other Data Formats and Sources\u00e2\u0080\u009d: you will learn how the Web standards interact and interoperate with other data formats to allow the integration of a variety of data sources.\n\nEach week alternates short videos and quizzes, as well as supplementary resources and forums to gradually progress through the different principles and standards.\n\nAfter following this course successfully, you will have the skills to obtain focused and structured datasets from the Web that you can then use to augment you own datasets, enrich their dimensions, feed your applications, perform data mining, machine learning and training, data analysis, AI processing and reasoning and other data management. PRINCIPLES OF A WEB OF LINKED DATA The RDF data model SPARQL Query Language Integration of other data formats and sources This first week is dedicated to introducing the notion of a Web of Linked Data. Dr. Fabien Gandon will start with some background and general knowledge about the Web, its history and its architecture.  This second week is dedicated to the Resource Description Framework (RDF), the standard data model to publish and link data on the Web.  This third week presents the SPARQL Query Language that enables users to perform information retrieval by querying RDF datasets on the Web of Data. This last week offers an open conclusion of the MOOC by addressing several initiatives to integrate more and more data on the Web.", "In the final capstone project you will apply the skills you learned by building a large data-intensive application using real-world data.\n\nYou will implement a complete application processing several gigabytes of data. This application will show interactive visualizations of the evolution of temperatures over time all over the world.\n\nThe development of such an application will involve:\n \u00e2\u0080\u0094 transforming data provided by weather stations into meaningful information like, for instance, the average temperature of each point of the globe over the last ten years ;\n \u00e2\u0080\u0094 then, making images from this information by using spatial and linear interpolation techniques ;\n \u00e2\u0080\u0094 finally, implementing how the user interface will react to users\u00e2\u0080\u0099 actions. Project overview Raw data display Interactive visualization Data manipulation Value-added information visualization Interactive user interface Get an overview of the project and all the information to get started. Transform data provided by weather stations into meaningful information. Transform temperature data into images, using various interpolation techniques. Generate images compatible with most Web-based mapping libraries. Get more meaning from your data: compute temperature deviations compared to normals. Generate images using bilinear interpolation. Implement how the user interface will react to users\u00e2\u0080\u0099 actions", "Epidemiological studies can provide valuable insights about the frequency of a disease, its potential causes and the effectiveness of available treatments. Selecting an appropriate study design can take you a long way when trying to answer such a question. However, this is by no means enough. A study can yield biased results for many different reasons. This course offers an introduction to some of these factors and provides guidance on how to deal with bias in epidemiological research. In this course you will learn about the main types of bias and what effect they might have on your study findings. You will then focus on the concept of confounding and you will explore various methods to identify and control for confounding in different study designs. In the last module of this course we will discuss the phenomenon of effect modification, which is key to understanding and interpreting study results. We will finish the course with a broader discussion of causality in epidemiology and we will highlight how you can utilise all the tools that you have learnt to decide whether your findings indicate a true association and if this can be considered causal. Module 1: Introduction to Validity and Bias MODULE 2: Confounding MODULE 3: Dealing with Confounding MODULE 4: Effect Modification Every time you conduct a study, the most important questions to ask are whether your results are an accurate reflection of the truth both within your sample and in the broader population of interest. This is called validity of the study and more or less determines if your study is of any value. In this module we will discuss what validity actually means and we will describe the different types of systematic error, or bias that may undermine the validity of a study. You will learn how to identify and prevent selection bias and information bias and their variations. Studies often focus on the association between two variables; for instance, between a risk factor and a disease. However, reality is usually complex and there are many other variables that may influence this association. Sometimes, the presence of a third variable can either exaggerate the association between the two variables we study or mask an underlying true association. This is called confounding and is any researcher\u00e2\u0080\u0099s nightmare. In this module, you will learn multiple methods to detect confounding in a study, so that you can prepare to deal with it. By the end of the module, you will be able to apply these methods to actual data and conclude whether there is confounding. This module is dedicated to dealing with confounding. Confounding can be addressed either at the design stage, before data is collected, or at the analysis stage. You will learn the main approaches to dealing with confounding and you will see practical examples on how to do this in your own studies. We will also briefly discuss about the Directed Acyclic Graphs, which is a novel way to detect bias and confounding and control for them. This is the final module of the course. We start by discussing what happens when the effect of an exposure on an outcome differs across levels of another variable. This is called effect modification. We will discuss how to approach effect modification and we will highlight the distinction between confounding and effect modification. We will close the course by revisiting causal inference in epidemiology, discussing how we can go through all potential explanations of an association before deciding whether it is of causal nature.", "Enterprises that seek to become proficient in advanced manufacturing must incorporate manufacturing management tools and integrate data throughout the supply chain to be successful. This course will make students aware of what a digitally connected enterprise is, as they learn about the operational complexity of enterprises, business process optimization and the concept of an integrated product-process-value chain. \n\nStudents will become acquainted with the available tools, technologies and techniques for aggregation and integration of data throughout the manufacturing supply chain and entire product life-cycle. They will receive foundational knowledge to assist in efforts to facilitate design, planning, and production scheduling of goods and services by applying product life cycle data.  \n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the sixth course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA The Concept of a Connected and Collaborative Enterprise  How to Build a Digitally Connected Enterprise Introduction to a Set of Supply Chain Management Tools & Integrated Capabilities Ensure a Robust Infrastructure  The purpose of this module is to educate students on why a holistic approach is necessary for analyzing the impact of advanced manufacturing on the success of an enterprise.  The purpose of this module is to provide an overview of product lifecycle and describe the challenges and opportunities that organizations face in adoption of advanced manufacturing technologies. We will discuss the desire for collection of product lifecycle data as well as outline the required features for a highly connected enterprise. In addition, we will provide several examples of information-sharing infrastructures and will elaborate the concept of Product Lifecycle Management (PLM) system. Finally, we will discuss several examples of effective data collection technologies. In this module, we will introduce the current enterprise management tools (such as ERP, MRP, and MES) that are often employed to integrate capabilities of various entities through the supply chain. We will provide a broad overview of these tools, and will review the capabilities of each of them. In this module, we discuss the importance of measuring the performance of supply chains. We will also discuss decision analysis methods and techniques that facilitate decision making through the entire product lifecycle. ", "Interprofessional Healthcare Informatics is a graduate-level, hands-on interactive exploration of real informatics tools and techniques offered by the University of Minnesota and the University of Minnesota's National Center for Interprofessional Practice and Education. We will be incorporating technology-enabled educational innovations to bring the subject matter to life. Over the 10 modules, we will create a vital online learning community and a working healthcare informatics network. \n\nWe will explore perspectives of clinicians like dentists, physical therapists, nurses, and physicians in all sorts of practice settings worldwide. Emerging technologies, telehealth, gaming, simulations, and eScience are just some of the topics that we will consider. \n\nThroughout the course, we\u00e2\u0080\u0099ll focus on creativity, controversy, and collaboration - as we collectively imagine and create the future within the rapidly evolving healthcare informatics milieu. All healthcare professionals and IT geeks are welcome! Introduction Informatics Theory Data, Information, and Knowledge Electronic Health Record (EHR) Components, Evidence-Based Practice Quality Improvement/ Workflow Analysis/ Redesign Telehealth/ Consumer Health/ Mobile Technology Community/ Population Health Informatics, Gaming, and Simulation Informatics and Ethics Data Exchange and Interoperability Informatics and the Foundation of Knowledge  Week 1 begins! This week, we explore and apply theories of healthcare informatics to professional practice. By the end of this week, you will be able to: describe informatics theory, analyze informatics theory related to practice and analyze health topics of interest to healthcare. This module explores and applies standardized terminologies to professional practice. By the end of this module, learners will be able to: analyze the transformation of data to information to knowledge and explore and apply standardized terminologies to professional practice. This module links EHR use to evidence-based practice. By the end of this module, learners will be able to: identify the benefits and goals of an electronic health record and analyze evidence-based practice within the context of the electronic health record. Week 5 begins! This week we examine informatics in relationship to new technologies in healthcare. Telehealth and technology are creating new ways to link people, and care, and health information. By the end of this week, you will be able to: examine applications of telehealth technologies and describe methods of engaging consumers in using health information technologies. This module examines informatics in relationship to new technologies in healthcare. By the end of this module, learners will be able to: examine applications of telehealth technologies and describe methods of engaging consumers in using health information technologies. This module relates informatics to community and population health. By the end of this module, learners will be able to: relate informatics to community and population health and analyze applications of geospatial information systems and health.\n This module describes applications of gaming, simulation, and virtual reality tools in healthcare. By the end of this module, learners will be able to: analyze informatics and gaming in relationship to health and healthcare and describe use of simulations and informatics to improve healthcare quality.\n This module explores ethical issues related to healthcare informatics in the interprofessional context. By the end of this module, learners will be able to: explore ethical issues related to healthcare informatics in the interprofessional context and analyze security and privacy challenges related to healthcare informatics. This module explores interprofessional aspects of healthcare data exchange and interoperability. By the end of this module, learners will be able to: describe information exchange and interoperability and analyze interprofessional aspects of information exchange and interoperability in healthcare. This module explores the contribution of healthcare informatics to the foundation of knowledge in healthcare. By the end of this module, learners will be able to: analyze implications of Big Data for healthcare research and synthesize insights related to interprofessional healthcare informatics.", "Digital health is rapidly being realised as the future of healthcare. While this is placing emphasis on the input of quality health data in digital records and systems, the delivery of safe and quality healthcare relies not only on the input of data, but also the ability to access and derive meaning from data to generate evidence, inform decision making and drive better health outcomes.\n\nThis course provides insight into the use of healthcare data, including an overview of best practices and the practical realities of obtaining useful information from digital health systems via the understanding of the fundamental concepts of health data analytics.  \n\nLearners will understand why data quality is essential in modern healthcare, as they are guided through various stages of the data life cycle, starting with the generation of quality health data, through to discovering patterns and extracting knowledge from health data using common methodologies and tools in the basic analysis, visualisation and communication of health data. In doing so, learners explore current healthcare delivery contexts, and future and emerging digital health data systems and applications that are rapidly becoming tomorrow\u00e2\u0080\u0099s reality.\n\nOn completion of this course, you will be able to:\n1.\tIdentify digital health technologies, health data sources, and the evolving roles of health workforce in digital health environments\n2.\tUnderstand key health data concepts and terminology, including the significance of data integrity and stakeholder roles in the data life cycle\n3.\tUse health data and basic data analysis to inform and improve decision making and practice.\n4.\tApply effective methods of communication of health data to facilitate safe and quality care.\n\nDuring this course, you will interact with learning content contributed by:\n\u00e2\u0080\u00a2\tDigital Health Cooperative Research Centre\n\u00e2\u0080\u00a2\tAustralian Digital Health Agency\n\u00e2\u0080\u00a2\teHealth NSW\n\u00e2\u0080\u00a2\tSydney Local Health District\n\u00e2\u0080\u00a2\tThe NSW Ministry of Health\n\u00e2\u0080\u00a2\tHealth Education and Training Institute\n\u00e2\u0080\u00a2\tClinical Excellence Commission \n\u00e2\u0080\u00a2\tChris O\u00e2\u0080\u0099Brien Lifehouse\n\u00e2\u0080\u00a2\tMonash Partners / Australian Health Research Alliance\n\u00e2\u0080\u00a2\tAustralian Research Data Commons\n\u00e2\u0080\u00a2\tJustice Health & Forensic Mental Health Network\n\u00e2\u0080\u00a2\tSouth Eastern Sydney Local Health District\n\u00e2\u0080\u00a2\tWestern Sydney Local Health District\n\u00e2\u0080\u00a2\tWestmead Breast Cancer Institute\n\u00e2\u0080\u00a2\tAgency for Clinical Innovation\n\u00e2\u0080\u00a2\tWestern NSW Local Health District\n\u00e2\u0080\u00a2\tSydney Children\u00e2\u0080\u0099s Hospital Network\n\nThis course is a collaborative venture between NSW Health, the University of Sydney and the Digital Health Cooperative Research Centre, including dedicated resources from eHealth NSW, Health Education and Training Institute, and the Research in Implementation Science & eHealth group. While many learning resources and case examples are drawn from the NSW Health service context, this course has relevance for all existing and future health workforce, regardless of role or work context.\nNote: Materials used are for learning purposes and content may not reflect your organisation\u00e2\u0080\u0099s policies. When working with data, make sure you act within the guidelines and policies of your organisation. An introduction to digital health Everyone plays a role in health data Interpret health data - turn information into new insights  Share and integrate data into practice This module explores current digital health environments, identifying the many uses of digital health technologies and health data sources. We look at the evolving roles of the health workforce in digital health environments, considering roles and responsibilities. The importance of data analytics for decision making and healthcare outcomes is introduced.                                                                                                                                                           \n\nNote: Materials used are for learning purposes and content may not reflect your organisation\u00e2\u0080\u0099s policies. When working with data, make sure you act within the guidelines and policies of your organisation. In this module, we review key health data concepts and terminology. We emphasise the importance of data integrity and the benefits and consequences of high or low quality data. We look at the data life cycle and the roles and responsibilities of all stakeholders in the provision of quality healthcare. There are some case studies illustrating the consequences of poor quality data and we also look at the fundamentals of digital health legislation and policy.                                      Note: Materials used are for learning purposes and content may not reflect your organisation\u00e2\u0080\u0099s policies. When working with data, make sure you act within the guidelines and policies of your organisation. In this module, we look at the use of health data and basic data analysis to inform and improve decision making and practice. We explore some common methods and tools used in the analysis of health data. There is an opportunity to develop a data query and to practice working with data.                                                                                                                         Note: Materials used are for learning purposes and content may not reflect your organisation\u00e2\u0080\u0099s policies. When working with data, make sure you act within the guidelines and policies of your organisation. In the final module, we look at the various modes of communication appropriate for sharing the results produced by data analysis.  We consider how effective communication of digital health data contributes to greater consumer engagement and well-being as well as more effective, evidence-based decision making within the healthcare system.                                                                                                                                                                                                   Note: Materials used are for learning purposes and content may not reflect your organisation\u00e2\u0080\u0099s policies. When working with data, make sure you act within the guidelines and policies of your organisation.", "Who is this course for ?\nThis course is RESTRICTED TO LEARNERS ENROLLED IN  Strategic Business Analytics SPECIALIZATION as a preparation to the capstone project. During the first two MOOCs, we focused on specific techniques for specific applications. Instead, with this third MOOC, we provide you with different examples  to open your mind to different applications from different industries and sectors.\nThe objective is to give you an helicopter overview on what's happening in this field. You will see how the tools presented in the two previous courses of the Specialization are used in real life projects. \nWe want to ignite your reflection process. Hence, you will best make use of the Accenture cases by watching first the MOOC and then investigate by yourself on the different concepts, industries, or challenges that are introduced during the videos.\n\nAt the end of this course learners will be able to: \n- identify the possible applications of business analytics,\n- hence, reflect on the possible solutions and added-value applications that could be proposed for their capstone project.\n\nThe cases will be presented by senior practitioners from Accenture with different backgrounds in term of industry, function, and country.  Special attention will be paid to the \"value case\" of the issue raised to prepare you for the capstone project of the specialization.\n\nAbout Accenture\nAccenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions\u00e2\u0080\u0094underpinned by the world\u00e2\u0080\u0099s largest delivery network\u00e2\u0080\u0094Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With more than 358,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com. Introduction to case studies in business analytics with Accenture Digital Transformation in the Media, the Financial Services and the Retail Sector Advanced Analytics in Healthcare and the Pharmaceutical industry / Wrap up and Introduction to capstone In this introductory module, Fabrice Marque, Managing Director Customer Strategy Practice Lead for France, Belgium and the Netherlands, also in charge of the ESSEC-Accenture Strategic Business Analytics Chair, will first introduce the MOOC in general. Then Michael Svilar, Global Accenture Data Science Group Lead, will identify the general trends in this sector. In this module, we will cover three different real-life examples. First, Rohit Banerji, Accenture business lead responsible for big data analytics for the resource sector, will present an example from a water utilities company. Second, Cian O\u00e2\u0080\u0099Hare, Managing Director at Accenture Digital, will present a case study from a global communication provider. Finally, Christopher Gray, public service expert at Accenture, will discuss challenges arising in the public sector where Analytics and Big Data can provide effective solutions.   \n\nAt the end of each example there will be quiz questions. Note that those questions may require you to collect additional information from that which was delivered during the videos. Do not hesitate to consult additional books, websites and examples about this topic: some of the answers can actually be found directly thanks to open access research engines or online encyclopedias! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field. During this module, different real-life examples will be discussed. Christine Removille, Digital Marketing Lead at the European Level, will present a data-centric digital transformation at a French TV company: Canal +. Edwin Van der Ouderaa, Financial Services Lead, will then explain how digital developments and data are disrupting the financial service sector.At the end of each video there will be  quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field. During this module, two different real-life examples will be discussed. First, Paul Pierotti, Managing Director at Accenture Digital, will explain how Analytics can transform how health services are delivered. Second Xavier Cimino, Managing Director in charge of the Analytics Practice in the Life Science industry for Europe, will present an award-winning project in this sector. At the end of each video, there will be quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field.Finally, Michael Svilar, Global Accenture Data Science Group Lead, will conclude the MOOC.", "In this course you will learn how to evaluate recommender systems.  You will gain familiarity with several families of metrics, including ones to measure prediction accuracy, rank accuracy, decision-support, and other factors such as diversity, product coverage, and serendipity.  You will learn how different metrics relate to different user goals and business goals.  You will also learn how to rigorously conduct offline evaluations (i.e., how to prepare and sample data, and how to aggregate results).  And you will learn about online (experimental) evaluation.  At the completion of this course you will have the tools you need to compare different recommender system alternatives for a wide variety of uses. Preface Basic Prediction and Recommendation Metrics Advanced Metrics and Offline Evaluation Online Evaluation Evaluation Design     ", "This course will enable you mastering machine-learning approaches in the area of investment management. It has been designed by two thought leaders in their field, Lionel Martellini from EDHEC-Risk Institute and John Mulvey from Princeton University. Starting from the basics, they will help you build practical skills to understand data science so you can make the best portfolio decisions.\n\nThe course will start with an introduction to the fundamentals of machine learning, followed by an in-depth discussion of the application of these techniques to portfolio management decisions, including the design of more robust factor models, the construction of portfolios with improved diversification benefits, and the implementation of more efficient risk management models. \n\nWe have designed a 3-step learning process: first, we will introduce a meaningful investment problem and see how this problem can be addressed using statistical techniques. Then, we will see how this new insight from Machine learning can complete and improve the relevance of the analysis.\n\nYou will have the opportunity to capitalize on videos and recommended readings to level up your financial expertise, and to use the quizzes and Jupiter notebooks to ensure grasp of concept.\n\nAt the end of this course, you will master the various machine learning techniques in investment management. Introducing the fundamentals of machine learning Machine learning techniques for robust estimation of factor models Machine learning techniques for efficient portfolio diversification Machine learning techniques for regime analysis  Identifying recessions, crash regimes and feature selection     ", "This course presents the principles of evolution and ecology for citizens and students interested in studying biology and environmental sciences. It discusses major ideas and results. Recent advances have energised these fields with evidence that has implications beyond their boundaries: ideas, mechanisms, and processes that should form part of the toolkit of all biologists and educated citizens.\n\nMajor topics covered by the course include fundamental principles of ecology, how organisms interact with each other and their environment, evolutionary processes, population dynamics, communities, energy flow and ecosystems, human influences on ecosystems, and the integration and scaling of ecological processes through systems ecology.\n \nThis course will also review major ecological concepts, identify the techniques used by ecologists, provide an overview of local and global environmental issues, and examine individual, group and governmental activities important for protecting natural ecosystems. The course has been designed to provide information, to direct the student toward pertinent literature, to identify problems and issues, to utilise research methodology for the study of ecology and evolution, and to consider appropriate solutions and analytical techniques. \n\nNeeded Learner Background: general biology and a good understanding of English.\n\nThis course has the following expectations and results:\n1) covers the theoretical and practical issues involved in ecology and evolution,\n2) conducting surveys and inventories in ecology, \n3) analyzing the information gathered, \n4) and applying their analysis to ecological and conservation problems. Welcome to the Course Module 1. The Scope of Ecology   Module 2. The Ecosystem  Module 3. Energy in Ecological Systems  Module 4. Population Ecology and Evolution  Final module  In this module, after an introduction about the meaning and a brief history of Ecology, we will see how plant and animal adapt and interact with their environment and how these interactions changes life histories and populations. Then we will focus on interspecific competition and we will understand that the avoidance of competition is a more common pattern in ecology than pure competition. In this module, we will talk about agonistic and foraging interactions between species (such as predation, herbivory and parasitism) and mutualistic interactions (such as symbiosis, commensalism, endosymbiosis, etc.). Then we will see how these interactions influence the evolutionary ecology of species and their diversity. In the last lesson of this module we will analyse the energy flux and biogeochemical cycles that keep alive Earth\u00e2\u0080\u0099s ecosystems and the whole biosphere (e.g. Gaia). In this module, we will discuss some fundamental ecological processes, such as those must be present in any Gaian planet. We will consider the Gaian effects of parasites and predators, biodiversity and hypercycles and we will see how these processes regulates our planet. Finally, we will consider the global ecological role of biomass, photosynthesis and carbon sequestration. In this final module, we will explore the possibility of a new ecology by exploring the concept of  sustainability, evaluating the human impacts on ecosystems and providing some solutions for nature conservation. Finally, we will see how to organise a citizen science event, called Bio-blitz, which can improve the scientific knowledge of our planet and, at the same time, rise the ecological awareness of citizens. This module allows you to learn how to organise a Bio-blitz \u00e2\u0080\u0093 an intensive biological investigation, which aims to record all the species living within a designated area, comprising groups of specialists supported by non-experts. ", "Welcome to Supply Chain Analytics - the art and science of applying data analytics to assess and improve supply chain performance!\n\nA supply chain is a complex system with conflicting objectives of cost efficiency and customer satisfaction. Supply chain management is becoming increasingly data driven. Through the real-life story and data of a major US telecommunication company, you will learn the analytics tools / skills to diagnose and optimize a supply chain. Upon completion of this course, you will be able to\n\n1. Use data analytics to assess the impact of various strategies on all aspects of a supply chain, from inventory, shipping, to warehouse order fulfillment, store operations and customer satisfaction. \n2. Customize the supply chain strategy by product to improve the overall cost efficiency without sacrificing customer service. \n3. Obtain hands-on experience on the application and financial impact of analytics in integrated supply chain and logistics planning.\n\nVASTA (name disguised) is a major wireless carrier in the US selling cell phones through a national network of retail stores. Recently, it wrote off a huge amount of obsolete inventory each year and was suffering a significant cost inefficiency in an increasingly stagnant market. VASTA must assess the competitive environment, and renovate its supply chain to stay competitive. At the end of this course, you will help VASTA save $billions on supply chain cost and retain its leadership in a stagnant and saturated market.\n\nI hope you enjoy the course! Welcome! General Principles and Intuition Data Collection, Cost Estimation and Supply Chain Analytics Customer Experience, Implementation and Project Welcome to Supply Chain Analytics! In this week, I shall first tell you the story of VASTA and the challenge it faces, then provide an overview of the course, learning outcomes and weekly topics. You will then learn competitive analysis and benchmarking to assess a firm\u00e2\u0080\u0099s competitive environment and identify the business opportunity. In Week 2, you will understand the general principles of supply chain planning, and develop intuitions on the benefits and concerns of the push / pull strategies. The intuitions and insights will guide you in the quantitative supply chain analysis in Week 3. In Week 3, you will learn what data to collect, how to estimate various types of costs, and how to use data analysis to assess the impact of various strategies on different aspects of a supply chain.  In Week 4, you will learn how to assess the impact of the strategies on customer experience. You will get hands-on experience on the implementation and financial impact of supply chain analytics in a real-life example. You will complete this course by conducting a project to analyze the geographic difference for the strategies.", "This course is an introduction into formal concept analysis (FCA), a mathematical theory oriented at applications in knowledge representation, knowledge acquisition, data analysis and visualization. It provides tools for understanding the data by representing it as a hierarchy of concepts or, more exactly, a concept lattice. FCA can help in processing a wide class of data types providing a framework in which various data analysis and knowledge acquisition techniques can be formulated. In this course, we focus on some of these techniques, as well as cover the theoretical foundations and algorithmic issues of FCA.\nUpon completion of the course, the students will be able to use the mathematical techniques and computational tools of formal concept analysis in their own research projects involving data processing. Among other things, the students will learn about FCA-based approaches to clustering and dependency mining.\nThe course is self-contained, although basic knowledge of elementary set theory, propositional logic, and probability theory would help.\nEnd-of-the-week quizzes include easy questions aimed at checking basic understanding of the topic, as well as more advanced problems that may require some effort to be solved.\n\nDo you have technical problems? Write to us: coursera@hse.ru Formal concept analysis in a nutshell Concept lattices and their line diagrams Constructing concept lattices Implications Interactive algorithms for learning implications Working with real data This week we will learn the basic notions of formal concept analysis (FCA). We'll talk about some of its typical applications, such as conceptual clustering and search for implicational dependencies in data. We'll see a few examples of concept lattices and learn how to interpret them. The simplest data structure in formal concept analysis is the formal context. It is used to describe objects in terms of attributes they have. Derivation operators in a formal context link together object and attribute subsets; they are used to define formal concepts. They also give rise to closure operators, and we'll talk about what these are, too. We'll have a look at software called Concept Explorer, which is good for basic processing of formal contexts. We'll also talk a little bit about many-valued contexts, where attributes may have many values. Conceptual scaling is used to transform many-valued contexts into \"standard\", one-valued, formal contexts. This week we'll talk about some mathematical properties of concepts. We'll define a partial order on formal concepts, that of \"being less general\". Ordered in this way, the concepts of a formal concept constitute a special mathematical structure, a complete lattice. We'll learn what these are, and we'll see, through the basic theorem on concept lattices, that any complete lattice can, in a certain sense, be modelled by a formal context. We'll also discuss how a formal context can be simplified without loosing the structure of its concept lattice. We will consider a few algorithms that build the concept lattice of a formal context: a couple of naive approaches, which are easy to use if one wants to build the concept lattice of a small context; a more sophisticated approach, which enumerates concepts in a specific order; and an incremental strategy, which can be used to update the concept lattice when a new object is added to the context. We will also give a formal definition of implications, and we'll see how an implication can logically follow from a set of other implications. This week we'll continue talking about implications. We'll see that implication sets can be redundant, and we'll learn to summarise all valid implications of a formal context by its canonical (Duquenne\u00e2\u0080\u0093Guigues) basis. We'll study one concrete algorithm that computes the canonical basis, which turns out to be a modification of the Next Closure algorithm from the previous week. We'll also talk about what is known in database theory as functional dependencies, and we'll show how they are related to implications. What if we don't have a direct access to a formal context, but still want to compute its concept lattice and its implicational theory? This can be done if there is a domain expert (or an oracle) willing to answer our queries about the domain. We'll study an approach known as learning with queries that addresses this setting. We'll get to know a few standard types of queries, and we'll see how an implication set can be learnt in time polynomial of its size with so called membership and equivalence queries. We'll then introduce attribute exploration, a method from formal concept analysis, which may require exponential time, but which uses different queries, more suitable for building implicational theories and representative samples of subject domains. A concept lattice can be exponentially large in the size of its formal context. Sometimes this can be due to noise in data. We'll study a few heuristics to filter out noisy concepts or select the most interesting concepts in a large lattice built from real data: stability and separation indices, concept probability, iceberg lattices. We will also talk about association rules, which is a name for implications that are supported by strong evidence, but may still have counterexamples in data. ", "This course provides an unique opportunity for you to learn key components of text mining and analytics aided by the real world datasets and the text mining toolkit written in Java. Hands-on experience in core text mining techniques including text preprocessing, sentiment analysis, and topic modeling help learners be trained to be a competent data scientists. \n\nEmpowered by bringing lecture notes together with lab sessions based on the y-TextMiner toolkit developed for the class, learners will be able to develop interesting text mining applications. Course Logistics and the Text Mining Tool for the Course Text Preprocessing Text Analysis Techniques Term Weighting and Document Classification Sentiment Analysis Topic Modeling      ", "This course will help you recognize how the \"digital thread\" is the backbone of the digital manufacturing and design (DM&D) transformation, turning manufacturing processes from paper-based to digital-based. You will have a working understanding of the digital thread \u00e2\u0080\u0093 the stream that starts at product concept and continues to accumulate information and data throughout the product\u00e2\u0080\u0099s life cycle \u00e2\u0080\u0093 and identify opportunities to leverage it. \n\nGain an understanding of how \"the right information, in the right place, at the right time\" should flow. This is one of the keys to unlocking the potential of a digital design process. Acknowledging this will enable you to be more involved in a product\u00e2\u0080\u0099s development cycle, and to help a company become more flexible. \n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the second course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Digital Thread Defined Data Storage in the Digital Thread Data Sharing and The Digital Thread The purpose of this module is to introduce learners to the context and definition of the digital thread. Details of individual lessons in this module are provided below. The purpose of this module is to explore the different strategies and components affecting data storage in the enterprise.  Building on effective information technology practices, the module will develop learners' knowledge of well developed data storage strategies. Upon completion, students will be able to analyze an organization\u00e2\u0080\u0099s data storage strategy and make recommendations for improvement in performance and robustness. Details of individual lessons in this module are provided below. The purpose of this module is to explain factors impacting the ability of organizations to share data (internally and externally).  Upon completion of the module, learners will be able to evaluate data sharing strategies and point out potential strengths and weaknesses of different approaches. Details of individual lessons in this module are provided below", "In this capstone course, you will apply everything you have learned by designing and then completing your own GIS project. You will plan out your project by writing a brief proposal that explains what you plan to do and why. You will then find data for a topic and location of your choice, and perform analysis and create maps that allow you to try out different tools and data sets. The results of your work will be assembled into an Esri story map, which is a web site with maps, images, text, and video. The goal is for you to have a finished product that you can share, and that demonstrates what you have learned. Introduction to Story Maps Data Discovery and Project Proposal Data Acquisition and Preparation Spatial Analysis Map Design Story Map      ", "This course immerses learners in deep learning, preparing them to solve computer vision problems. Learners plunge into the field of computer vision that deals with recognizing, identifying and understanding visual information from visual data, whether the information is from a single image or video sequence. Topics include object detection, face detection and recognition (using Adaboost and Eigenfaces), and the progression of deep learning techniques (CNN, AlexNet, REsNet, and Generative Models.)      \n  \nThis course is ideal for anyone curious about or interested in exploring the concepts of visual recognition and deep learning computer vision. Learners should have basic programming skills and experience (understanding of for loops, if/else statements), specifically in MATLAB (free introductory tutorial: https://www.mathworks.com/learn/tutorials/matlab-onramp.html). Learners should also be familiar with the following: basic linear algebra (matrix vector operations and notation), 3D co-ordinate systems and transformations, basic calculus (derivatives and integration) and basic probability (random variables). It is highly recommended that learners take the \u00e2\u0080\u009cDeep Learning Onramp\u00e2\u0080\u009d course available at https://matlabacademy.mathworks.com/.   \n\nMaterial includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing computer vision programs through online labs using MATLAB* and supporting toolboxes.\n\nThis is the fourth course in the Computer Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a video overview at https://youtu.be/OfxVUSCPXd0. \n\n * A free license to install MATLAB for the duration of the course is available from MathWorks. Introduction to Visual Recognition & Understanding Early Techniques Deep Learning Overview Deep Learning in Computer Vision: Applications This module provides an introduction to visual recognition and understanding in Computer Vision. This module discusses optical character recognition, face detection, face recognition, and other early techniques used for visual recognition. In this module, we will discuss the history of Deep Learning, how it is used, and how it is revolutionizing the field of Computer Vision. This module provides information about the various applications of Deep Learning in Computer Vision.", "How can we know if the differences in wages between men and women are caused by discrimination or differences in background characteristics? In this PhD-level course we look at causal effects as opposed to spurious relationships. We will discuss how they can be identified in the social sciences using quantitative data, and describe how this can help us understand social mechanisms. The Nature of Causal Effects and How to Measure Them The Multivariate Regression Model and Mediating Factors Randomized Controlled Trials Instrumental Variables Difference in Difference Welcome to the first week of the course! Th\u00c4\u00b1s week we are looking at the nature of causal effects and how to measure them.  This second module introduces you multivariate regression model and the concept of mediating factors. In this third week of the course we are having a closer look at causality and the randomzied controlled trial. The fourth week of the course we will go through the concept of instrumental variables. The final module of the course deals with the difference in difference. We hope you enjoyed the course and have learned something that you can use in your future work and research. ", "The Business Statistics and Analysis Capstone is an opportunity to apply various skills developed across the four courses in the specialization to a real life data. The Capstone, in collaboration with an industry partner uses publicly available \u00e2\u0080\u0098Housing Data\u00e2\u0080\u0099 to pose various questions typically a client would pose to a data analyst.\nYour job is to do the relevant statistical analysis and report your findings in response to the questions in a way that anyone can understand.\nPlease remember that this is a Capstone, and has a degree of difficulty/ambiguity higher than the previous four courses. The aim being to mimic a real life application as close as possible. Business Statistics and Analysis Capstone: An Introduction Business Statistics and Analysis Capstone: Assessments 1 & 2 Business Statistics and Analysis Capstone: Assessment 3 Business Statistics and Analysis Capstone: Assessment 4    ", "The course presents an overview of the theory behind biological diversity evolution and dynamics and of methods for diversity calculation and estimation. We will become familiar with the major alpha, beta, and gamma diversity estimation techniques.\n\nUnderstanding how biodiversity evolved and is evolving on Earth and how to correctly use and interpret biodiversity data is important for all students interested in conservation biology and ecology, whether they pursue careers in academia or as policy makers and other professionals (students graduating from our programs do both). Academics need to be able to use the theories and indices correctly, whereas policy makers must be able to understand and interpret the conclusions offered by the academics.\n\nThe course has the following expectations and results:\n\n- covering the theoretical and practical issues involved in biodiversity theory,\n- conducting surveys and inventories of biodiversity,\n- analyzing the information gathered,\n- and applying their analysis to ecological and conservation problems.\n\nNeeded Learner Background:\n\n- basics of Ecology and Calculus\n- good understanding of English Welcome to the course \"Biological Diversity (Theories, Measures and Data sampling techniques)\" Biodiversity and evolution Importance of biodiversity and anthropogenic impacts  Analyse and measure biodiversity Species-abundance distributions and comparisons Alternative measures of biodiversity Statistics applied to the analysis of biodiversity  This module represents the course content and its author as well as contains the additional materials to the course In this module we will explore the evolution of biodiversity. In particular we will understand what is the web of life and how species interact to coexist.Moreover, we will understand the main processes that allow the evolution of biodiversity and how it is structured and structures itself. Finally, we will review the distribution patterns of biodiversity in macroscale. After having analysed the distribution of biodiversity in macroscale in the previous module we will see it in microscale.\nThen I will explain you the importance of biodiversity: first we will see what are the effect of anthropogenic impacts, and second we will see why biodiversity is important for us. We will try to answer the important question about what are the causes of biodiversity decline and we will analyse the effect of climate change on species diversity, ecosystems and the whole planet. With a global perspective we will explore the implication for biodiversity of the Gaia theory.\n In this module we will move from a theoretical discussion to a more practical point of view and we will see hot to analyse and measure biodiversity. I will show you some sampling techniques and how to avoid the most common sampling errors. We will talk about the relevant problem of pseudoreplication.\nThen I will show you some metrics to estimate \u00ce\u00b1 and \u00ce\u00b2 \u00e2\u0080\u0093diversity and how to use them in particular cases and specific situations.\n In this module we will talk about the most common species-abundance distribution models and I will show you how to compare different communities and samples in order to achieve a quantitative and statistical measure of the changes in biological diversity due to treatments.\nI will explain some Evenness measures and how to represent them in form of curves of biodiversity. This will help to discriminate communities\u00e2\u0080\u0099 diversity and to better analyse the anthropogenic impacts on biodiversity.\n In this module I will show you some of the most useful alternative measures of biodiversity. We will explore the meaning of functional and taxonomic (phylogenetic) diversity and I will explain you how to use these metrics for a more complete understanding of the patterns governing communities\u00e2\u0080\u0099 diversity. Then I will provide you some sketches about qualitative measures of biodiversity. The last module (n\u00c2\u00b0 6) of this course will be dedicated to statistics applied to the analysis of biodiversity. We will see how to apply the information gathered in the previous modules to obtain a statistical significance. We will explore parametric and non-parametric tests, the useful chi-square test, the correct application of correlation and the regression analysis, and some hints about the multivariate analysis techniques, such as ANOVA.", "Variability is a fact of life in manufacturing environments, impacting product quality and yield. Through this course, students will learn why performing advanced analysis of manufacturing processes is integral for diagnosing and correcting operational flaws in order to improve yields and reduce costs.   \n\nGain insights into the best ways to collect, prepare and analyze data, as well as computational platforms that can be leveraged to collect and process data over sustained periods of time. Become better prepared to participate as a member of an advanced analysis team and share valuable inputs on effective implementation.    \n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the fourth course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Introduction to Advanced Manufacturing Process Analysis Data Collection Data Analysis: Computational Techniques and Platforms The purpose of this module is to introduce the concept of advanced analysis in improvement of manufacturing processes. Also, this module will help you to understand the difference between discrete manufacturing and continuous manufacturing. Storing big data is quite different from handling traditional data. This difference is explained in this module. The purpose of this module is to introduce various steps involved in data analysis. Data Collection, Data Storage, Data Organization and Data Pre-processing concepts are explained.  The purpose of this module is to introduce various techniques used in advanced analysis, like Determination of Significant Variables/Factors, Data Visualization, and Anomaly Detection. Also, this module will introduce various computational platforms (HPC, Cloud computing techniques) that exist for carrying out advanced analysis.", "This course teaches you the fundamentals of computational phenotyping, a biomedical informatics method for identifying patient populations. In this course you will learn how different clinical data types perform when trying to identify patients with a particular disease or trait. You will also learn how to program different data manipulations and combinations to increase the complexity and improve the performance of your algorithms. Finally, you will have a chance to put your skills to the test with a real-world practical application where you develop a computational phenotyping algorithm to identify patients who have hypertension. You will complete this work using a real clinical data set while using a free, online computational environment for data science hosted by our Industry Partner Google Cloud. Introduction: Identifying Patient Populations Tools: Clinical Data Types Techniques: Data Manipulations and Combinations Techniques: Algorithm Selection and Portability Practical Application: Develop a Computational Phenotyping Algorithm to Identify Patients with Hypertension Learn about computational phenotyping and how to use the technique to identify patient populations.  Understand how different clinical data types can be used to identify patient populations. Begin developing a computational phenotyping algorithm to identify patients with type II diabetes. Learn how to manipulate individual data types and combine multiple data types in computational phenotyping algorithms. Develop a more sophisticated computational phenotyping algorithm to identify patients with type II diabetes. Understand how to select a single \"best\" computational phenotyping algorithm. Finalize and justify a phenotyping algorithm for type II diabetes. Put your new skills to the test - develop an computational phenotyping algorithm to identify patients with hypertension. ", "This course covers the analysis of Functional Magnetic Resonance Imaging (fMRI) data. It is a continuation of the course \u00e2\u0080\u009cPrinciples of fMRI, Part 1\u00e2\u0080\u009d Week 1 Week 2 Week 3 Week 4 This week we will discuss psychological and behavioral inference, as well as advanced experimental design.  This week we will continue with advanced experimental design, and also discuss advanced GLM modeling. This week we will focus on brain connectivity. This week we will focus on multi-voxel pattern analysis.", "This course gives an overview of the changing regulatory environment since the 1997 Asian and 2008 global financial crisis. Following these two major crises, governments around the globe enacted a set of far-reaching new financial regulations that are aimed towards safeguarding financial stability. However, banks find it increasingly difficult to be profitable in this new regulatory environment. Technology, at the same time, has taken important leaps forward with the emergence of sophisticated models of artificial intelligence and the invention of the blockchain. These two developments fuel the emergence of fintech companies around the world. \n\nThis course discusses fintech regulation in emerging markets using case studies from China and South Africa. The course pays special attention to the socioeconomic environment in emerging markets, as well as to political risk as a major source of uncertainty for fintech entrepreneurs. Peer-to-peer lending and remittances are used as leading examples for fintech innovation in emerging markets. Financial Crises Post-crisis regulations Fintech regulation in emerging markets Rise in Fintech Companies So you\u00e2\u0080\u0099re interested in the opportunities for fintech in emerging markets?  I\u00e2\u0080\u0099ll start by taking you back to the key events in financial crises of the late 1990s and early 2000s that will enable you to  understand the context of fintech and regulation in emerging markets. We\u00e2\u0080\u0099ll be looking at the basic mechanisms behind banking and what happened when the financial markets crashed. It\u00e2\u0080\u0099s essential for us to become familiar with the new financial regulations and how they create opportunities and constraints for fintech today. Over the four weeks of the course, you will be invited to think about how new financial regulations have spurred the rise of fintech companies in emerging economies. This week we dive into detail about financial systems -  with a focus on how banks work and on banking regulation. Using the Great Depression of the United States as starting point, I describe what happened to cause the collapse of banks and the introduction of the post-depression financial regulations. The new financial regulations included the establishment of international financial institutions which control current monetary policies. Being familiar with the current financial regulatory framework and where it comes from provides the basis for understanding the disruptions from the fintech innovations we will be covering next week. Now we reach the heart of the course - looking at the context of fintech in countries with emerging economies and what makes it different from industrialised countries. We will look at Initial Coin Offerings (ICO) which are one of the key mechanisms used fintech companies to finance themselves.  We are going to focus particularly on South Africa, China and Brazil, and I have a number of interviews with people involved in banking, financial regulation and fintech from these countries.  In the last week of the course, you will be looking applications of fintech in emerging economies. I have selected two of the most successful applications to explore in detail - peer-to-peer lending and remittances. Because of the costs of banking and the exclusion of millions from the formal banking systems, entrepreneurs have started creating innovative mechanisms to finance business by borrowing from peers. Using fintech as fast and secure mechanism to transfer remittances is being widely used across the world - and we will look at some examples.  I also invited guests from within the banking sector to describe how large formal financial institutions are considering the impact of blockchain and other new technologies. At the end of this week, you will have a chance to apply what you\u00e2\u0080\u0099ve learnt in a final assignment.", "Welcome to the Capstone Project for Big Data! In this culminating project, you will build a big data ecosystem using tools and methods form the earlier courses in this specialization. You will analyze a data set simulating big data generated from a large number of users who are playing our imaginary game \"Catch the Pink Flamingo\". During the five week Capstone Project, you will walk through the typical big data science steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will introduce you to the data set and guide you through some exploratory analysis using tools such as Splunk and Open Office. Then we will move into more challenging big data problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and slide presentations. As a result of our collaboration with Splunk, a software company focus on analyzing machine-generated big data, learners with the top projects will be eligible to present to Splunk and meet Splunk recruiters and engineering leadership. Simulating Big Data for an Online Game Acquiring, Exploring, and Preparing the Data Data Classification with KNIME Clustering with Spark Graph Analytics of Simulated Chat Data With Neo4j Reporting and Presenting Your Work Final Submission This week we provide an overview of the Eglence, Inc. Pink Flamingo game, including various aspects of the data which the company has access to about the game and users and what we might be interested in finding out. Next, we begin working with the simulated game data by exploring and preparing the data for ingestion into big data analytics applications. This week we do some data classification using KNIME.  This week we do some clustering with Spark.  This week we apply what we learned from the 'Graph Analytics With Big Data' course to simulated chat data from Catch the Pink Flamingos using Neo4j. We analyze player chat behavior to find ways of improving the game.   ", "This module aims at introducing fundamental concepts of visual perception applied to information visualization. These concepts help the student ideate and evaluate visualization designs in terms of how well they leverage the capabilities of the human perceptual machinery. Applied Perception for Information Visualization Effectiveness of Visual Channels Color Perception and Color Spaces Using Color in Visualization    ", "If I Googled you, what would I find?\n\nAs we move around the online world we leave tracks and traces of our activity all the time: social media accounts, tagged images, professional presences, scraps of text, but also many artefacts we don't always realise we are leaving behind, or that others leave about us.  \n\nIn this course you will hear from a range of experts and you will have an opportunity to explore and reflect on your own online tracks and traces, to understand why your digital footprint is important. We will introduce you to some of the tools and approaches to effectively manage your online presence (or digital footprint).  \n\nThe course will focus on the different dimensions of a digital footprint, including developing an effective online presence, managing your privacy, creating opportunities for networking, balancing and managing professional and personal presences (eprofessionalism). By the end of this course (MOOC) you should be equipped to ensure that your digital footprint works for you, whether you want to be more private online, or are looking to create a more effective and impactful presence.  \n\nYou can also join the conversation on Twitter using the hashtag #DFMOOC and follow us @DFMOOC\n\nWe hope you enjoy the course! What makes an online presence effective? Why does your digital footprint matter? What does it mean to be an effective online professional? In week 1 we provide information on how to engage with the course as well as our Twitter account, using #DFMOOC.  This week, we will introduce you to the topic of Digital Footprint. There are a range of activities, videos and resources for you to work through. By the end of week 1, you will have the opportunity to critically reflect on your own online presence and consider how you can make better informed choices as well as setting appropriate personal goals around your digital footprint.  This week, there is a range of experts who focus on why a digital footprint matters and how to create an effective online presence. The themes explored include data after death, privacy online and managing your online data. The activity and quizzes will help you to consider your own online presence and what you might do to make it more effective and work for you! This week, we examine the idea of a professional online presence and what this might mean for different people and professions. Understanding whether your personal and professional online presence should blur or be kept separate can be a challenge. We will hear from experts in Business, Nursing, Science, Education, and a Careers Consultant. They will provide useful advice on what you can do to make your online presence more professional and potentially help you with finding a job,  standing out from the crowd, complying with professional bodies' guidelines, and much more. The activities, including the quizzes and peer assessment will help you to reflect on your own online presence. There will also be an opportunity to get advice from your peers and consider putting into action what you have learned during this course.", "This course will help us to evaluate and compare the models we have developed in previous courses. So far we have developed techniques for regression and classification, but how low should the error of a classifier be (for example) before we decide that the classifier is \"good enough\"? Or how do we decide which of two regression algorithms is better?\n\nBy the end of this course you will be familiar with diagnostic techniques that allow you to evaluate and compare classifiers, as well as performance measures that can be used in different regression and classification scenarios. We will also study the training/validation/test pipeline, which can be used to ensure that the models you develop will generalize well to new (or \"unseen\") data. Week 1: Diagnostics for Data Week 2: Codebases, Regularization, and Evaluating a Model Week 3: Validation and Pipelines Final Project For this first week, we will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of diagnostics for the results of supervised learning. This week, we will learn how to create a simple bag of words for analysis. We will also cover regularization and why it matters when building a model. Lastly, we will evaluate a model with regularization, focusing on classifiers. This week, we will learn about validation and how to implement it in tandem with training and testing. We will also cover how to implement a regularization pipeline in Python and introduce a few guidelines for best practices. In the final week of this course, you will continue building on the project from the first and second courses of Python Data Products for Predictive Analytics with simple predictive machine learning algorithms. Find a dataset, clean it, and perform basic analyses on the data. Evaluate your model, validate your analyses, and make sure you aren't overfitting the data.", "The data science revolution has produced reams of new data from a wide variety of new sources. These new datasets are being used to answer new questions in way never before conceived. Visualization remains one of the most powerful ways draw conclusions from data, but the influx of new data types requires the development of new visualization techniques and building blocks. This course provides you with the skills for creating those new visualization building blocks. We focus on the ggplot2 framework and describe how to use and extend the system to suit the specific needs of your organization or team. Upon completing this course, learners will be able to build the tools needed to visualize a wide variety of data types and will have the fundamentals needed to address new data types as they come about. Welcome to Building Data Visualization Tools Plotting with ggplot2 Mapping and interactive plots The grid Package Building New Graphical Elements Before we get started, we'll take a quick overview of the course. Now, we'll dive into creating and customizing ggplot2 plots. Mapping is a critical part of many data visualizations. During this module, we'll teach you how to create simple and dynamic maps with ggplot2 and ggmap, how to overlay data, and how to create chloropleth maps of US counties. The grid package in R implements the primitive graphical functions that underly the ggplot2 plotting system. In this module, you'll learn how to work with grid to build graphics. Building and modifying a theme in ggplot2 is a key feature of the ggplot2 package and system for building data graphics. In this final module, you'll learn to build a new theme and modifying existing themes with new features.", "\u00c2\u00bfQu\u00c3\u00a9 es el aprendizaje autom\u00c3\u00a1tico? \u00c2\u00bfQu\u00c3\u00a9 tipos de problemas puede solucionar? En Google, tenemos una perspectiva ligeramente distinta sobre el aprendizaje autom\u00c3\u00a1tico: no se trata solo de los datos, sino tambi\u00c3\u00a9n de la l\u00c3\u00b3gica. Hablamos de por qu\u00c3\u00a9 un marco de este tipo es \u00c3\u00batil cuando pensamos en la creaci\u00c3\u00b3n de una canalizaci\u00c3\u00b3n de modelos de aprendizaje autom\u00c3\u00a1tico. Luego, analizamos cinco fases para convertir un posible caso pr\u00c3\u00a1ctico en un recurso que pueda aprovechar la tecnolog\u00c3\u00ada de aprendizaje autom\u00c3\u00a1tico y por qu\u00c3\u00a9 es importante no saltarse fases. Finalizamos con un reconocimiento de los sesgos que puede amplificar el aprendizaje autom\u00c3\u00a1tico y c\u00c3\u00b3mo reconocerlos. Presentaci\u00c3\u00b3n de la especializaci\u00c3\u00b3n Qu\u00c3\u00a9 significa tener como prioridad a la IA C\u00c3\u00b3mo Google utiliza el AA AA inclusivo Blocs de notas de Python en la nube Resumen Se presenta la especializaci\u00c3\u00b3n y a los expertos de Google que dar\u00c3\u00a1n las clases. En este m\u00c3\u00b3dulo, conocer\u00c3\u00a1 a qu\u00c3\u00a9 nos referimos cuando decimos que la estrategia empresarial de Google es que la IA sea la prioridad, adem\u00c3\u00a1s de lo que conlleva en la pr\u00c3\u00a1ctica. En este m\u00c3\u00b3dulo, se describen los conocimientos empresariales que Google adquiri\u00c3\u00b3 con los a\u00c3\u00b1os. En este m\u00c3\u00b3dulo, analizaremos por qu\u00c3\u00a9 los sistemas de aprendizaje autom\u00c3\u00a1tico no son imparciales de forma predeterminada y otros aspectos que se deben tener en cuenta al usar el AA en los productos. En este m\u00c3\u00b3dulo, se describe Cloud\u00c2\u00a0Datalab, el entorno de desarrollo que usar\u00c3\u00a1 en esta especializaci\u00c3\u00b3n. ", "The analytical process does not end with models than can predict with accuracy or prescribe the best solution to business problems. Developing these models and gaining insights from data do not necessarily lead to successful implementations. This depends on the ability to communicate results to those who make decisions. Presenting findings to decision makers who are not familiar with the language of analytics presents a challenge. In this course you will learn how to communicate analytics results to  stakeholders who do not understand the details of analytics but want evidence of analysis and data. You will be able to choose the right vehicles to present quantitative information, including those based on principles of data visualization. You will also learn how to develop and deliver data-analytics stories that provide context, insight, and interpretation. Introduction to the Course Best  Practices in Data Visualization Interpreting, Telling, and Selling Acting on Data In this module we\u00e2\u0080\u0099ll briefly review the Information-Action Value Chain we introduced in Course 1.  Then we\u00e2\u0080\u0099ll see how analytical techniques are applied in business problems, first by looking at some \u00e2\u0080\u009cclassic\u00e2\u0080\u009d business problems that have been around for a long time, then by looking at some \u00e2\u0080\u009cemergent\u00e2\u0080\u009d business problems that have resulted from more recent advances in technology. In this module we\u00e2\u0080\u0099ll learn about a variety of visualizations used to illustrate and communicate data. We will start with the different vehicles used  to present quantitative information. We will then look at a set of examples of data visualizations and discuss what makes them effective or ineffective.  Finally, we discuss Excel charts and why most of them should be avoided.  After completing this module, you will be able to better understand the characteristics of good data visualization and avoid common mistakes when creating your own graphs.\n In this module we\u00e2\u0080\u0099ll cover a number of topics around interpreting data, gathering additional data, and pitching our recommendations based on our analysis.  First, we\u00e2\u0080\u0099ll discuss ways in which we misinterpret or misrepresent data and how to avoid them, such as mistaking correlation with causation, allowing cognitive biases to influence how we see data, and visualizing data in misleading ways.  We\u00e2\u0080\u0099ll also learn how experimentation can help us obtain more data, including compromises we may need to make in measurement.  Finally, we\u00e2\u0080\u0099ll discuss how we communicate our results and recommendations, with a focus on knowing our audience, telling compelling stories, and creating clear and effective communication materials.\n In our final module we\u00e2\u0080\u0099ll walk through two case studies and illustrate the ideas we\u00e2\u0080\u0099ve covered in the course and in the specialization as a whole.  The first case shows how experimentation can be used to create data, sometimes with surprising results.  The second case presents a comprehensive analysis that illustrates the entire analytic lifecycle, and shows how different methods and both quantitative and qualitative analysis can be brought together to solve one strategically important analytical problem.", "The Executive Data Science Capstone, the specialization\u00e2\u0080\u0099s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a real-world scenario developed in collaboration with Zillow, a data-driven online real estate and rental marketplace, and DataCamp, a web-based platform for data science programming. Your task will be to lead a virtual data science team and make key decisions along the way to demonstrate that you have what it takes to shepherd a complex analysis project from start to finish.  For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your fellow capstone participants.\n\nCourse cover image by Luckey_sun. Creative Commons BY-SA https://flic.kr/p/bx1jvU Executive Data Science Capstone It's time to put your skills to the test managing a data science project at Zillow, a data-driven online real estate and rental marketplace. Along the way, you'll make important decisions as you lead your team through the project. ", "SAS Viya is an in-memory distributed environment used to analyze big data quickly and efficiently. In this course, you\u00e2\u0080\u0099ll learn how to use the SAS Viya APIs to take control of SAS Cloud Analytic Services from a Jupyter Notebook using R or Python. You\u00e2\u0080\u0099ll learn to upload data into the cloud, analyze data, and create predictive models with SAS Viya using familiar open source functionality via the SWAT package -- the SAS Scripting Wrapper for Analytics Transfer. You\u00e2\u0080\u0099ll learn how to create both machine learning and deep learning models to tackle a variety of data sets and complex problems. And once SAS Viya has done the heavy lifting, you\u00e2\u0080\u0099ll be able to download data to the client and use native open source syntax to compare results and create graphics. Course Overview SAS\u00c2\u00ae  Viya\u00c2\u00ae  and Open Source Integration Machine Learning Text Analytics Deep Learning Time Series Image Classification Factorization Machines In this module, you meet the instructor and learn about course logistics, such as how to access the software for this course. In this module you learn about the analytical processing engine behind SAS Viya, the Cloud Analytic Services server. You also learn how to submit data processing commands to SAS Viya from the open source languages R and Python. In this module you learn how to use R and Python to create, optimize, and assess SAS Viya predictive models. You also learn how to use R and Python to efficiently manage the creation and assessment of these models. In this module you learn how natural language processing is used to analyze collections of text documents. You also learn how to turn blocks of unstructured text into numeric inputs suitable for predictive modeling. In this module you learn how deep learning methods extend traditional neural network models with new options and architectures. You also learn how recurrent neural networks are used to model sequence data like time series and text strings, and how to create these models using R and Python APIs for SAS Viya. In this module you learn how to model time series using two popular methods, exponential smoothing and ARIMAX. You also learn how to use the R and Python APIs for SAS Viya to create forecasts using these classical methods and using recurrent neural networks for more complex problems. In this module you learn how convolutional neural networks are used to classify images and how to use the R and Python APIs for SAS Viya to create convolutional neural networks. In this module you learn how factorization machines are used to create recommendation engines and how to build factorization machine models in SAS Viya using the R and Python APIs.", "Career prospects are bright for those qualified to work in healthcare data analytics. Perhaps you work in data analytics, but are considering a move into healthcare where your work can improve people\u00e2\u0080\u0099s quality of life. If so, this course gives you a glimpse into why this work matters, what you\u00e2\u0080\u0099d be doing in this role, and what takes place on the Path to Value where data is gathered from patients at the point of care, moves into data warehouses to be prepared for analysis, then moves along the data pipeline to be transformed into valuable insights that can save lives, reduce costs, to improve healthcare and make it more accessible and affordable. Perhaps you work in healthcare but are considering a transition into a new role. If so, this course will help you see if this career path is one you want to pursue. You\u00e2\u0080\u0099ll get an overview of common data models and their uses. You\u00e2\u0080\u0099ll learn how various systems integrate data, how to ensure clear communication, measure and improve data quality. Data analytics in healthcare serves doctors, clinicians, patients, care providers, and those who carry out the business of improving health outcomes. This course of study will give you a clear picture of data analysis in today\u00e2\u0080\u0099s fast-changing healthcare field and the opportunities it holds for you. Introduction to Healthcare Data Models Data Models and Use Cases They Support Working with Data across Systems Improving the Quality of Healthcare Data In this module, you will be able to define the foundational terms used in discussing and building healthcare data models. You'll be able to describe the conceptual model showing how data flows from operations to analysis. You will compare and contrast common data models used in healthcare data systems. You will also be able to identify common measures used in healthcare data analysis.  In this module, you'll be able to describe the Star Schema Data Model, distinguish it from the hierarchical and relational model, list some pros and cons and explain situations in which it could be appropriately used. You should also recognize when another type of data model might be better suited to a particular use case. In this module, you'll be able to explain how information is stored in data models and how we assemble relevant information to analyze an interesting problem that can improve our healthcare systems. We'll review how we normalize data and how that facilitates analysis. We'll go on to discuss how to bring together information from different sources and across various functional systems. We will also consider how to measure it accurately.  In this module, you will be able to examine the data that goes into these models and explain how we work with the information that comes from the practice and business of medicine. We will transition from raising the data quality to focusing on finding and correcting data errors by validation and verification. You will also be able to describe several ways data is checked to eliminate errors and improve data quality. \n", "Learn fundamental concepts in data analysis and statistical inference, focusing on one and two independent samples. Hypothesis Testing Two Binomials Discrete Data Settings Techniques In this module, you'll get an introduction to hypothesis testing, a core concept in statistics. We'll cover hypothesis testing for basic one and two group settings as well as power. After you've watched the videos and tried the homework, take a stab at the quiz. In this module we'll be covering some methods for looking at two binomials. This includes the odds ratio, relative risk and risk difference. We'll discussing mostly confidence intervals in this module and will develop the delta method, the tool used to create these confidence intervals. After you've watched the videos and tried the homework, take a crack at the quiz! In this module, we'll discuss testing in discrete data settings. This includes the famous Fisher's exact test, as well as the many forms of tests for contingency table data. You'll learn the famous observed minus expected squared over the expected formula, that is broadly applicable.  This module is a bit of a hodge podge of important techniques. It includes methods for discrete matched pairs data as well as some classical non-parametric methods.", "In this course, you will gain a thorough understanding of the blockchain and distributed ledger technologies, including an introduction to the necessary foundations in cryptography. The course will discuss blockchain as a distributed ledger and introduce distributed consensus as a mechanism to maintain the integrity of the blockchain. The other revolutionary technologies that are changing the world as we speak are artificial intelligence and machine learning. You will learn about the three major types of AI algorithms: supervised and unsupervised machine learning, as well as reinforcement learning. \n\nYou will learn about the application of blockchain outside of finance. In particular, how blockchain fundamentally changes the way we deal with our personal data. You will see how the web 2.0 model--where big companies like facebook and google collect as much of your personal data as possible to sell it to third parties--is coming to an end. The new web 3.0 is decentralized and uses the power of the blockchain to put users in full control over their own data. Finally, we will look at the benefits and considerations of blockchain and whether blockchain is the right solution for your problem.\n\n#UCTFintech Cryptographic foundations of the blockchain Mastering the blockchain technology How Artificial Intelligence Changes Data Analytics Preparing your company for the fintech revolution This week looks at the plumbing of current and future financial systems. Firstly we will look at our current financial infrastructure and how it facilitates the safe and efficient functioning of the financial system. We will also look at how our current infrastructure falls short and why this is an opportunity for fintech companies. We start with what banks and other financial intermediaries are, and why we regulate them. Then we will talk about how our financial infrastructure connects financial intermediaries and ensures that they can trade with one another. Secondly this week, we turn to the plumbing of the financial system of the future, which can be summarized in one word really, blockchain. Before going into details about how exactly the blockchain works, it makes sense to talk about how blockchain will disrupt our existing financial system. It's then time to take a deep dive into the cryptographic foundations of the blockchain. We will talk about public private key cryptography and explain exactly why cryptography makes the blockchain so secure. We end off the week with a complete overview of the data structures used in almost all blockchains. This will give you a thorough understanding of the innermost workings of the blockchain. Equipped with these tools, we can then take on the remainder of the course.  Welcome to the second week of our course. This is the week where it gets real. We will finally have all the ingredients we need to fully grasp what the blockchain is. We start the week off by explaining single and double entry bookkeeping. It's important to understand why double entry bookkeeping improved the integrity of ledges in Italy in the 14th century, to truly appreciate how novel and revolutionary the blockchain is. For hundreds of years, double-entry bookkeeping was the method of choice when it comes to financial accounting. In double-entry bookkeeping, the two parties in a transaction each update their books, which provides some security against fraudulent actors. With the emergence of the distributed ledger technology and the blockchain however, we have seen a fundamental paradigm shift in accounting. The accounting perspective is very useful when we want to understand the blockchain as a distributed ledger. Distributed ledger is shared among a number of counterparties who want to transact with one another. A blockchain collects a number of transactions in the block, and through clever combination of cryptography and incentive schemes, ensures that these blocks cannot be altered. Each block contains a reference to the previous block, which is how a chain of blocks turns into a blockchain. We will discuss the different consensus mechanisms and cover both proof of work and proof-of-stake in detail. You will understand both the theory behind them and how they are applied in different block chains. This is an exciting week so let's get started. Welcome to the third week of our course. This week we will explore more technology that is also disrupting the financial services industry, specifically artificial intelligence or AI. We will actually get our hands dirty and cover the three streams of AI, supervised and unsupervised machine learning, as well as reinforcement learning. All three methods are widely used in the financial services industry already, and we kick the week off by giving you a bird's-eye view of the possible applications of AI in finance. Then we turn to unsupervised machine learning, which is mainly used to cluster large amounts of high dimensional and unstructured data. This has many applications in the financial services industry. Once we have covered unsupervised learning, we then dive into supervised machine learning, which is a brilliant way to classify large amounts of data. A supervised machine learning algorithm must be trained on the training data set, which is often historical data. Once properly trained, the algorithm can then correctly classify patterns in new data, which is why supervised machine learning is so powerful. The idea of training data is then taken one step further in reinforcement learning, where algorithms learn from their interactions with the environment just like a human would. Reinforcement learning is an immensely powerful tool, and many big Wall Street firms are heavily investing into AI trading algorithms that really use reinforcement learning at their heart. Once we have all this heavy material covered, it's time to treat ourselves. So, at the end of this week, you will meet a number of guest lecturers from the financial services industry who will explain to you how their companies are successfully using AI in their business. This is a big week, where we look beyond the blockchain and to the second big pillar of the Fintech revolution, AI and machine learning. Welcome to the fourth week of our course. So, this week, we turn to applications of the blockchain outside of the traditional financial services industry. Since this is a course on FinTech, we will still focus on applications that have ties either with payment services or other financial services. But the main applications are really outside of finance. The first such application is the new architecture of the Internet, the Web 3.0. The current Internet paradigm, the Web 2.0 is built on uses handing that data to large corporations like Facebook or Twitter, and exchange for free services. But as we've seen with a fake news scandal around the 2016 US presidential election and the Brexit referendum, this business model comes with its disadvantages. The biggest disadvantage is privacy. Once a user post his personal data to for example a social media website, he has no longer control over the data. This is where the blockchain comes in. With a blockchain, is possible to give uses ownership of their data to ensure that they have full control over their personal sensitive information. So, this week, you will learn how the blockchain changes at prevailing Web 2.0 business model, and provides endless possibilities for truly decentralized Internet. We will then talk about possible drawbacks to blockchains, and when it is appropriate to use a blockchain and when it is not. There is as much to do for us this week. So, let's dive right in.", "The purpose of this course is to summarize new directions in Chinese history and social science produced by the creation and analysis of big historical datasets based on newly opened Chinese archival holdings, and to organize this knowledge in a framework that encourages learning about China in comparative perspective.\n\nOur course demonstrates how a new scholarship of discovery is redefining what is singular about modern China and modern Chinese history. Current understandings of human history and social theory are based largely on Western experience or on non-Western experience seen through a Western lens. This course offers alternative perspectives derived from Chinese experience over the last three centuries. We present specific case studies of this new scholarship of discovery divided into two stand-alone parts, which means that students can take any part without prior or subsequent attendance of the other part.\n\nPart 1 (this course) focuses on comparative inequality and opportunity and addresses two related questions \u00e2\u0080\u0098Who rises to the top?\u00e2\u0080\u0099 and \u00e2\u0080\u0098Who gets what?\u00e2\u0080\u0099.\n\nPart 2 (https://www.coursera.org/learn/understanding-china-history-part-2) turns to an arguably even more important question \u00e2\u0080\u0098Who are we?\u00e2\u0080\u0099 as seen through the framework of comparative population behavior - mortality, marriage, and reproduction \u00e2\u0080\u0093 and their interaction with economic conditions and human values. We do so because mortality and reproduction are fundamental and universal, because they differ historically just as radically between China and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of human behavior and values.\n\nCourse Overview video: https://youtu.be/dzUPRyJ4ETk Orientation and Module 1: Social Structure and Education in Late Imperial China Module 2: Education and Social Mobility in Contemporary China Module 3: Social Mobility and Wealth Distribution in Late Imperial and Contemporary China Module 4: Wealth Distribution and Regime Change in Twentieth Century China Final Exam and Farewell Before you start with the content for Module 1, please watch the Course Overview, review the Assignments and Grading page, and introduce yourself to other learners who will be studying this course with you.    Now is time to test your understanding on the entire course. Take the final exam and complete the post-course survey. Your valuable feedback will certainly help us improve future iterations of the course.", "How can innovators understand if their idea is worth developing and pursuing? In this course, we lay out a systematic process to make strategic decisions about innovative product or services that will help entrepreneurs, managers and innovators to avoid common pitfalls. We teach students to assess the feasibility of an innovative idea through problem-framing techniques and rigorous data analysis labelled \u00e2\u0080\u0098a scientific approach\u00e2\u0080\u0099. The course is highly interactive and includes exercises and real-world applications. We will also show the implications of a scientific approach to innovation management through a wide range of examples and case studies. THE INNOVATION DECISION THEORY AND DATA FOR INNOVATION MANAGEMENT DATA ANALYSIS ADVANCED TOOLS FOR INNOVATION MANAGEMENT DECISIONS  FINAL PROJECT We provide a general discussion of innovation as problem-solving and we link the discuss the building blocks of the scientific approach to innovation decisions \u00e2\u0080\u0093 from how to formulate the problem, to how to formulate the hypotheses and the theory, and how to test them. The whole discussion will be framed and applied to concrete managerial problems, including a discussion of the specific managerial tools that facilitate the application of a scientific approach to innovation management. We provide more details about the scientific approach and we introduce probabilities to understand how and why certain decisions lead to some outcomes instead of others and how to make better decisions. We also focus on how to formulate and test hypotheses in practice, and how to interpret these tests. We finally discuss how to design and run experiments. \nNB: some videos may contain a downloadable database; please, download it and follow the in-video instructions We cover the basics of data analysis, beginning with the distinction between correlation and causality in the analysis of data. We also teach how to make predictions using regression analysis and link these methods to the scientific approach, showing what role these analyses play, how they help to make scientific decisions and why. \nWe complement this with real examples of companies using data to make innovation decisions. We close by discussing how to interpret these analyses and results critically to make sure we understand what we really learn from the analyses and when, how and why we should interpret our results cautiously and critically. This is s a more advanced part in which we discuss causality and provide the students with some broad exposure to big data and machine learning, and we discuss what they can do for managerial decisions.We provide a general wrap-up and conclusion of the course, including a discussion of when the scientific approach is most appropriate or has limitations. This helps to see when to apply it, or when to apply other approaches, including our own gut feelings. \n\nNB: some videos may contain a downloadable database; please, download it and follow the in-video instructions ", "Writing good code for data science is only part of the job. In order to maximizing the usefulness and reusability of data science software, code must be organized and distributed in a manner that adheres to community-based standards and provides a good user experience. This course covers the primary means by which R software is organized and distributed to others. We cover R package development, writing good documentation and vignettes, writing robust software, cross-platform development, continuous integration tools, and distributing packages via CRAN and GitHub. Learners will produce R packages that satisfy the criteria for submission to CRAN. Getting Started with R Packages Documentation and Testing Licensing, Version Control, and Software Design Continuous Integration and Cross Platform Development    ", "\u00d8\u00a5\u00d8\u00b0\u00d8\u00a7 \u00d9\u0083\u00d9\u0086\u00d8\u00aa \u00d8\u00aa\u00d8\u00b1\u00d8\u00ba\u00d8\u00a8 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d8\u00ae\u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d9\u0082 \u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u0083\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b5\u00d8\u00b7\u00d9\u0086\u00d8\u00a7\u00d8\u00b9\u00d9\u008a \u00d8\u00b4\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b7\u00d9\u0088\u00d8\u00b1\u00d8\u008c \u00d9\u0081\u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af\u00d9\u0083 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00aa\u00d8\u00ad\u00d9\u0082\u00d9\u008a\u00d9\u0082 \u00d8\u00b0\u00d9\u0084\u00d9\u0083. \u00d8\u00a5\u00d9\u0086 \u00d9\u0085\u00d9\u0087\u00d9\u0086\u00d8\u00af\u00d8\u00b3\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d9\u0088\u00d9\u0086 \u00d8\u00a8\u00d8\u00b4\u00d8\u00af\u00d8\u00a9\u00d8\u008c \u00d9\u0083\u00d9\u0085\u00d8\u00a7 \u00d8\u00a3\u00d9\u0086 \u00d8\u00a5\u00d8\u00aa\u00d9\u0082\u00d8\u00a7\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d9\u008a\u00d9\u0085\u00d9\u0086\u00d8\u00ad\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0086 \u00d9\u0081\u00d8\u00b1\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d9\u0082\u00d8\u00a8\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0086\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d8\u00af\u00d8\u00a9. \u00d8\u00a5\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d9\u008a\u00d8\u00b9\u00d8\u00af \u00d8\u00a8\u00d9\u0085\u00d8\u00ab\u00d8\u00a7\u00d8\u00a8\u00d8\u00a9 \"\u00d9\u0082\u00d9\u0088\u00d8\u00a9 \u00d8\u00b9\u00d8\u00b8\u00d9\u0085\u00d9\u0089\" \u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d8\u00af\u00d8\u00a9 \u00d9\u0083\u00d8\u00b0\u00d9\u0084\u00d9\u0083 \u00d8\u00aa\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af\u00d9\u0083 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a3\u00d9\u0086\u00d8\u00b8\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u0083\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b5\u00d8\u00b7\u00d9\u0086\u00d8\u00a7\u00d8\u00b9\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u0084\u00d9\u0085 \u00d9\u008a\u00d9\u0083\u00d9\u0086 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d9\u0085\u00d9\u0083\u00d8\u00a7\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b5\u00d9\u0088\u00d9\u0084 \u00d8\u00a5\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d9\u0085\u00d9\u0086\u00d8\u00b0 \u00d8\u00b9\u00d8\u00af\u00d8\u00a9 \u00d8\u00b3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d9\u0084\u00d8\u00a9 \u00d9\u0085\u00d8\u00b6\u00d8\u00aa. \n\n\u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d8\u00b3\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082. \u00d8\u00b9\u00d9\u0086\u00d8\u00af\u00d9\u0085\u00d8\u00a7 \u00d8\u00aa\u00d9\u0086\u00d8\u00aa\u00d9\u0087\u00d9\u008a \u00d9\u0085\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00b5\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u008c \u00d8\u00b3\u00d9\u008a\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d8\u00a8\u00d8\u00a5\u00d9\u0085\u00d9\u0083\u00d8\u00a7\u00d9\u0086\u00d9\u0083 \u00d9\u0085\u00d8\u00a7 \u00d9\u008a\u00d9\u0084\u00d9\u008a:\n- \u00d9\u0081\u00d9\u0087\u00d9\u0085 \u00d8\u00a7\u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d9\u0087\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0086\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00af\u00d9\u0081\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0081\u00d9\u0083\u00d9\u008a\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d9\u008b\u00d8\u00a7\n- \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1 \u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00aa\u00d8\u00b5\u00d9\u0084\u00d8\u00a9 \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d9\u0083\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0088\u00d8\u00aa\u00d8\u00aa\u00d8\u00b3\u00d9\u0085 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0082 \u00d9\u0088\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8 \u00d8\u00aa\u00d9\u0084\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d9\u0087\u00d8\u00a7 \n- \u00d8\u00a5\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d9\u0083 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d9\u0086\u00d9\u0081\u00d9\u008a\u00d8\u00b0 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d8\u00a9 (\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0088\u00d8\u00ac\u00d9\u0087\u00d8\u00a9) \n- \u00d8\u00a5\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00a8\u00d9\u0086\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \n\n\u00d9\u0083\u00d9\u0085\u00d8\u00a7 \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d9\u0083 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0083\u00d8\u00b0\u00d9\u0084\u00d9\u0083 \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d9\u008a\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d8\u00a8\u00d8\u00b4\u00d9\u0083\u00d9\u0084 \u00d9\u0081\u00d8\u00b9\u00d9\u0084\u00d9\u008a \u00d9\u0083\u00d8\u00b0\u00d9\u0084\u00d9\u0083\u00d8\u008c \u00d8\u00a8\u00d8\u00af\u00d9\u0084\u00d8\u00a7\u00d9\u008b \u00d9\u0085\u00d9\u0086 \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u008a\u00d9\u0085 \u00d9\u0088\u00d8\u00b5\u00d9\u0081 \u00d8\u00b3\u00d8\u00b1\u00d9\u008a\u00d8\u00b9 \u00d8\u00a3\u00d9\u0088 \u00d8\u00b3\u00d8\u00b7\u00d8\u00ad\u00d9\u008a \u00d9\u0081\u00d9\u0082\u00d8\u00b7. \u00d9\u0084\u00d8\u00b0\u00d8\u00a7\u00d8\u008c \u00d8\u00a8\u00d8\u00b9\u00d8\u00af \u00d8\u00a5\u00d9\u0083\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0082\u00d8\u00a7\u00d8\u00af\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0083. \u00d8\u00a5\u00d8\u00b0\u00d8\u00a7 \u00d9\u0083\u00d9\u0086\u00d8\u00aa \u00d8\u00aa\u00d8\u00a8\u00d8\u00ad\u00d8\u00ab \u00d8\u00b9\u00d9\u0086 \u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00ac\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u0083\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b5\u00d8\u00b7\u00d9\u0086\u00d8\u00a7\u00d8\u00b9\u00d9\u008a\u00d8\u008c \u00d8\u00a8\u00d8\u00b9\u00d8\u00af \u00d8\u00a5\u00d8\u00aa\u00d9\u0085\u00d8\u00a7\u00d9\u0085 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d8\u00aa\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0082\u00d8\u00a7\u00d8\u00af\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0083\u00d8\u00b0\u00d9\u0084\u00d9\u0083 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a6\u00d9\u0084\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d8\u00a8\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00ae\u00d8\u00b5\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9. \n\n\u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0087\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084\u00d9\u0089 \u00d9\u0081\u00d9\u008a \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082. \u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00ad\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d8\u00b7\u00d8\u00ad\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00b4\u00d8\u00b1\u00d8\u00ad \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d9\u0087\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00a4\u00d8\u00af\u00d9\u008a \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00b2\u00d9\u008a\u00d8\u00a7\u00d8\u00af\u00d8\u00a9 \u00d8\u00b4\u00d8\u00a3\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082\u00d8\u008c \u00d9\u0088\u00d9\u0081\u00d9\u0087\u00d9\u0085 \u00d9\u0085\u00d8\u00aa\u00d9\u0089 \u00d9\u008a\u00d8\u00aa\u00d9\u0085 \u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d9\u0087 \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d9\u0088\u00d9\u0085.\u00e2\u0080\u008f \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a5\u00d8\u00b9\u00d8\u00af\u00d8\u00a7\u00d8\u00af \u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a9 \u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d9\u0083\u00d9\u008a\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b9\u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a9 \u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9. \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00ac\u00d9\u008a\u00d9\u0087 \u00d9\u0084\u00d8\u00aa\u00d8\u00b3\u00d8\u00b1\u00d9\u008a\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0085\u00d8\u00a7\u00d8\u00b0\u00d8\u00ac \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0083.\u00e2\u0080\u008f\n \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1 \u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a9 \u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00ad\u00d8\u00aa\u00d9\u0088\u00d9\u008a \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00b7\u00d8\u00a8\u00d9\u0082\u00d8\u00a9 \u00d9\u0085\u00d8\u00ae\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d8\u00ad\u00d8\u00af\u00d8\u00a9\u00d8\u008c \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a3\u00d8\u00b3\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d9\u0086\u00d8\u00aa\u00d8\u00b4\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0085\u00d8\u00a7\u00d9\u0085\u00d9\u008a \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d9\u0086\u00d8\u00aa\u00d8\u00b4\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d9\u0084\u00d9\u0081\u00d9\u008a.\u00e2\u0080\u008f \u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d9\u0082\u00d9\u0088\u00d9\u0085 \u00d8\u00b9\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b5\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u0087\u00d8\u00a7\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00b1\u00d8\u00a4\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d9\u0085\u00d8\u00a8\u00d9\u008a\u00d9\u0088\u00d8\u00aa\u00d8\u00b1.\u00e2\u0080\u008f", "The course is a compendium of the must-have expertise in data science for executive and middle-management to foster data-driven innovation. It consists of introductory lectures spanning big data, machine learning, data valorization and communication. Topics cover the essential concepts and intuitions on data needs, data analysis, machine learning methods, respective pros and cons, and practical applicability issues.\t\n\nThe course covers terminology and concepts, tools and methods, use cases and success stories of data science applications. \nThe course explains what is Data Science and why it is so hyped. It discusses the value that Data Science can create, the main classes of problems that Data Science can solve, the difference is between descriptive, predictive and prescriptive analytics, and the roles of machine learning and artificial intelligence.\n\nFrom a more technical perspective, the course covers supervised, unsupervised and semi-supervised methods, and explains what can be obtained with classification, clustering, and regression techniques. It discusses the role of NoSQL data models and technologies, and the role and impact of scalable cloud-based computation platforms.\nAll topics are covered with example-based lectures, discussing use cases, success stories and realistic examples. Introduction to Data-driven Business  Terminology and Foundational Concepts Data Science Methods for Business Challenges and Conclusions This module introduces the course and offers some basic overview of the topics. It presents the crucial concepts related to data science and big data and provides an outlook on how to use them in real world settings for increasing business value. In this module, you will learn the foundational concepts of machine learning and data science. You will understand how these techniques can be useful in terms of increased business value for organizations, thanks to the discussion of a very well known success story, namely Netflix, which can be deemed as a completely data-driven business. You will also understand how machine learning is different from programming. In this module, you will learn the concepts and intuitions about the basic approaches for data analysis, including linear regression, naive Bayes, decision trees, clustering, and logistic regression. All the methods are presented starting from typical business uses and are covered in an intuitive way through a guided explanation of how the approach works on simple examples. This module summarizes the concepts learned so far and introduces a set of challenges and risks that data-savvy managers must take into account when deciding for a data-driven strategy. \n", "The practice of investment management has been transformed in recent years by computational methods. Instead of merely explaining the science, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the Python programming language. In this course, we cover the estimation, of risk and return parameters for meaningful portfolio decisions, and also introduce a variety of state-of-the-art portfolio construction techniques that have proven popular in investment management and portfolio construction due to their enhanced robustness.\n\nAs we cover the theory and math in lecture videos, we'll also implement the concepts in Python, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern computational methods in investment management, you'll have practical mastery in the implementation of those methods. If you follow along and implement all the lab exercises, you will complete the course with a powerful toolkit that you will be able to use to perform your own analysis and build your own implementations and perhaps even use your newly acquired knowledge to improve on current methods. Style & Factors Robust estimates for the covariance matrix Robust estimates for expected returns Portfolio Optimization in Practice    ", "There are opportunities throughout the design process of any product to make significant changes, and ultimately impact the future of manufacturing, by embracing the digital thread. In this course, you will dig into the transformation taking place in how products are designed and manufactured throughout the world. It is the second of two courses that focuses on the \"digital thread\" \u00e2\u0080\u0093 the stream that starts at the creation of a product concept and continues to accumulate information and data throughout the product life cycle.  \n\nHear about the realities of implementing the digital thread, directly from someone responsible for making it happen at a company. Learn how the digital thread can fit into product development processes in an office, on a shop floor, and even across an enterprise. Be prepared to talk about the benefits, and limitations, of enacting it.\n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the third course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Strategic issues in implementing the digital thread Cyberinfrastructure Components of the Digital Thread Technologies used in the Design Process Digital Thread on the Shop Floor Digital Thread and the Manufacturing Enterprise The purpose of this module is to outline the strategies, barriers, and business factors related to the implementation of a digital thread approach. Details of individual lessons in this module are provided below. The purpose of this module is to introduce students to the Information Technology components supporting the digital thread. Details of individual lessons in this module are provided below. The purpose of this module is to discuss individual digital analysis tools used in the design process. Details of individual lessons in this module are provided below. The purpose of this module is to introduce the growing number of digital tools available to support manufacturing operations. Details of individual lessons in this module are provided below. The purpose of this module is to introduce how a digital thread implementation impacts the design and manufacturing enterprise. Details of individual lessons in this module are provided below.", "Career prospects are bright for those qualified to work with healthcare data or as Health Information Management (HIM) professionals. Perhaps you work in data analytics but are considering a move into healthcare, or you work in healthcare but are considering a transition into a new role. In either case, Healthcare Data Quality and Governance will provide insight into how valuable data assets are protected to maintain data quality. This serves care providers, patients, doctors, clinicians, and those who carry out the business of improving health outcomes. \n\n\"Big Data\" makes headlines, but that data must be managed to maintain quality. High-quality data is one of the most valuable assets gathered and used by any business. This holds greater significance in healthcare where the maintenance and governance of data quality directly impact people\u00e2\u0080\u0099s lives. This course will explain how data quality is improved and maintained. You\u00e2\u0080\u0099ll learn why data quality matters, then see how healthcare professionals monitor, manage and improve data quality. You\u00e2\u0080\u0099ll see how human and computerized systems interact to sustain data quality through data governance. You\u00e2\u0080\u0099ll discover how to measure data quality with metadata, tracking data provenance, validating and verifying data, along with a communication framework commonly used in healthcare settings. \n\nThis knowledge matters because high-quality data will be transformed into valuable insights that can save lives, reduce costs, to improve healthcare and make it more accessible and affordable. You will make yourself more of an asset in the healthcare field by what you gain from this course. Why Data Quality Matters Measuring Data Quality  Monitoring, Managing and Improving Data Quality Sustaining Quality through Data Governance In this module, you will be able to define data quality and what drives it. You'll be able to recall and describe four key aspects of data quality. You'll be able to explain why data quality is important for operations, for patient care, and for the finances of healthcare providers. You'll be able to discuss how data may change over time, and how finding those changes allows us to recognize and work with the issues the changes cause. You will be able to explain why requirements for data quality depend on how we intend to use that data and understand four levels of quality that may be applied for different kinds of analysis. You will also be able to discuss how all of this supports our ability to do our best work in the best ways possible. This module focuses on measuring data quality. After this module, you will be able to describe metadata, list what metadata may include, give some examples of metadata and recall some of its uses as it relates to measuring data quality. We will describe data provenance to explains how knowing the origin of a data set can help data analysts determine if a data set is suitable for a particular use. We\u00e2\u0080\u0099ll also describe 5 components of data quality you can recall and use when evaluating data. You will also learn to be able to distinguish between data verification and validation, recalling 4 applicable data validation methods and 3 concepts useful to validate data. In addition to your video lessons, you will read and discuss a scholarly article on Methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research. We wrap up the module with a framework abbreviated as S-B-A-R that is often used in healthcare team situations to communicate about issues that must be solved. In this module, we focus on monitoring, managing, and improving data quality. You will be able to explain how to monitor data on a day-to-day basis to see that it remains consistent. You will explain how measures can help us monitor the patient health and the quality of care they receive over time.  Also, you will be able to discuss establishing the culture of quality throughout the data lifecycle and improving data quality from the baseline by posing questions to determine a baseline of data quality. You will be able to manage data quality through expected and unexpected changes, along with tracking monitoring strategies along the data pipeline. After this module, you will be able to identify and fix common deficiencies in the data and implement change control systems as a monitoring tool. You\u00e2\u0080\u0099ll also recall several best practices you can apply on the job to monitor data quality in the healthcare field.  IIn this module, we focus on sustaining quality through data governance. We will define data governance and consider why it matters in healthcare. You will discuss who makes up data governance committees, how these committees function relative to data analysts and describe how stakeholders work together to ensure data quality. You\u00e2\u0080\u0099ll be able to describe how high-quality data is a valuable asset for any business. You will also define data governance systems. You will recall several ways data can be repurposed and explain how data governance maintains data quality as it is repurposed for a use other than that for which it was originally gathered. In addition to your video lessons, you will read and discuss the article, Big Data, Bigger Outcomes and practice applying some of these important concepts.", "Welcome to the Advanced Linear Models for Data Science Class 2: Statistical Linear Models. This class is an introduction to least squares from a linear algebraic and mathematical perspective. Before beginning the class make sure that you have the following:\n\n- A basic understanding of linear algebra and multivariate calculus.\n- A basic understanding of statistics and regression models.\n- At least a little familiarity with proof based mathematics.\n- Basic knowledge of the R programming language.\n\nAfter taking this course, students will have a firm foundation in a linear algebraic treatment of regression modeling. This will greatly augment applied data scientists' general understanding of regression models. Introduction and expected values The multivariate normal distribution Distributional results Residuals In this module, we cover the basics of the course as well as the prerequisites. We then cover the basics of expected values for multivariate vectors. We conclude with the moment properties of the ordinary least squares estimates.  In this module, we build up the multivariate and singular normal distribution by starting with iid normals. In this module, we build the basic distributional results that we see in multivariable regression. In this module we will revisit residuals and consider their distributional results. We also consider the so-called PRESS residuals and show how they can be calculated without re-fitting the model.", "This course offers a rigorous mathematical survey of advanced topics in causal inference at the Master\u00e2\u0080\u0099s level.\n\nInferences about causation are of great importance in science, medicine, policy, and business.  This course provides an introduction to the statistical literature on causal inference that has emerged in the last 35-40 years and that has revolutionized the way in which statisticians and applied researchers in many disciplines use data to make inferences about causal relationships.  \n\nWe will study advanced topics in causal inference, including mediation, principal stratification, longitudinal causal inference, regression discontinuity, interference, and fixed effects models. Module 7: Introduction to Mediation Module 8: More on Mediation Module 9: Instrumental Variables, Principal Stratification, and Regression Discontinuity\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Module 10: Longitudinal Causal Inference\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Module 11: Interference and Fixed Effects\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t     ", "This course will cover the steps used in weighting sample surveys, including methods for adjusting for nonresponse and using data external to the survey for calibration.  Among the techniques discussed are adjustments using estimated response propensities, poststratification, raking, and general regression estimation.  Alternative techniques for imputing values for missing items will be discussed.  For both weighting and imputation, the capabilities of different statistical software packages will be covered, including R\u00c2\u00ae, Stata\u00c2\u00ae, and SAS\u00c2\u00ae. General Steps in Weighting Specific Steps Implementing the Steps Imputing for Missing Items Summary of Course 5 Weights are used to expand a sample to a population.  To accomplish this, the weights may correct for coverage errors in the sampling frame, adjust for nonresponse, and reduce variances of estimators by incorporating covariates. The series of steps needed to do this are covered in Module 1. Specific steps in weighting include computing base weights, adjusting if there are cases whose eligibility we are unsure of, adjusting for nonresponse, and using covariates to calibrate the sample to external population controls.  We flesh out the general steps with specific details here. Software is critical to implementing the steps, but the R system is an excellent source of free routines. This module covers several R packages, including sampling, survey, and PracTools that will select samples and compute weights. In most surveys there will be items for which respondents do not provide information, even though the respondent completed enough of the data collection instrument to be considered \"complete\".  If only the cases with all items present are retained when fitting a model, quite a few cases may be excluded from the analysis. Imputing for the missing items avoids dropping the missing cases.  We cover methods of doing the imputing and of reflecting the effects of imputations on standard errors in this module. We briefly summarize the methods of weighting and imputation that were covered in Course 5.", "In this final course you will complete a Capstone Project using data analysis to recommend a method for improving profits for your company, Watershed Property Management, Inc. Watershed is responsible for managing thousands of residential rental properties throughout the United States. Your job is to persuade Watershed\u00e2\u0080\u0099s management team to pursue a new strategy for managing its properties that will increase their profits. To do this, you will: (1) Elicit information about important variables relevant to your analysis; (2) Draw upon your new MySQL database skills to extract relevant data from a real estate database; (3) Implement data analysis in Excel to identify the best opportunities for Watershed to increase revenue and maximize profits, while managing any new risks; (4) Create a Tableau dashboard to show Watershed executives the results of a sensitivity analysis; and (5) Articulate a significant and innovative business process change for Watershed based on your data analysis, that you will recommend to company executives. \n\nAirbnb, our Capstone\u00e2\u0080\u0099s official Sponsor, provided input on the project design. The top 10 Capstone completers each year will have the opportunity to present their work directly to senior data scientists at Airbnb live for feedback and discussion.\n\n\"Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.\" Introduction Data Extraction and Visualization Modeling Cash Flow and Profits Data Dashboard Dashboard for Decision-makers Final Project The goal for this week is to learn about the Capstone Project you are tasked with, acquire background about the business problem, and begin to outline the steps of your analysis. The goal of this week is for you to extract the relevant data from the MySQL database you are given access to, and to look at it briefly in Tableau to get sense of what data you have.   The goal of this week is for you to create a financial model using Excel to analyze the data you extracted from the database, and to start to predict short-term rents for some of Watershed's existing properties.  The goal for this week is for you to use your projections about the Watershed properties to estimate cash flows and profits Watershed would experience if it converted properties to short-term rentals.  The goal of this week and next week is to build an analytical dashboard in Tableau using the data models and assumptions you have discovered in prior weeks.  This week complete your dashboard and add design elements so the dashboard is ready for stakeholders (Watershed executives, for example) to use it to test your model's assumptions.  This week, design and give a presentation for Watershed executives with your business recommendations, and complete a white paper template. Evaluate 3 peer's dashboards, white papers and presentations. ", "This course aims to introduce learners to advanced visualization techniques beyond the basic charts covered in Information Visualization: Fundamentals. These techniques are organized around data types to cover advance methods for: temporal and spatial data, networks and trees and textual data. In this module we also teach learners how to develop innovative techniques in D3.js.\n\nLearning Goals\nGoal: Analyze the design space of visualization solutions for various kinds of data visualization problems. Learn what designs are available for a given problem and what are their respective advantages and disadvantages.\n- Temporal\n- Spatial\n- Spatio-Temporal\n- Networks\n- Trees\n- Text\n\nThis is the fourth course in the Information Visualization Specialization. The course expects you to have some basic knowledge of programming as well as some basic visualization skills (as those introduced in the first course of the specialization). Visualizing Geographical Data Visualizing Network Data Visualizing Temporal Data Interaction and Multiple Views    ", "Este curso r\u00c3\u00a1pido sob demanda apresentar\u00c3\u00a1 as funcionalidades de Big Data e Machine Learning do Google Cloud Platform (GCP). Ele fornecer\u00c3\u00a1 uma vis\u00c3\u00a3o geral r\u00c3\u00a1pida do Google Cloud Platform e mostrar\u00c3\u00a1 em detalhes as funcionalidades do processamento de dados.\n\n Ao final deste curso, voc\u00c3\u00aa ter\u00c3\u00a1 aprendido a:\n\u00e2\u0080\u00a2 Identificar o objetivo e o valor dos principais produtos de Big Data e Machine Learning no Google Cloud Platform\n\u00e2\u0080\u00a2 Usar o Cloud SQL e o Cloud Dataproc para migrar as atuais cargas de trabalho MySQL e Hadoop/Pig/Spark/Hive para o Google Cloud Platform\n\u00e2\u0080\u00a2 Usar o BigQuery e o Cloud Datalab para fazer an\u00c3\u00a1lises de dados interativas\n\u00e2\u0080\u00a2 Escolher entre o Cloud SQL, o BigTable e o Datastore\n\u00e2\u0080\u00a2 Treinar e usar uma rede neural com o TensorFlow\n\u00e2\u0080\u00a2 Escolher entre diferentes produtos de processamento de dados no Google Cloud Platform\n\nPara se inscrever neste curso, voc\u00c3\u00aa deve ter aproximadamente um (1) ano de experi\u00c3\u00aancia em um ou mais destes itens:\n\u00e2\u0080\u00a2 Uma linguagem de consulta (como SQL)\n\u00e2\u0080\u00a2 Atividades de extra\u00c3\u00a7\u00c3\u00a3o, transforma\u00c3\u00a7\u00c3\u00a3o e carga\n\u00e2\u0080\u00a2 Modelagem de dados\n\u00e2\u0080\u00a2 Machine learning e/ou estat\u00c3\u00adsticas\n\u00e2\u0080\u00a2 Programa\u00c3\u00a7\u00c3\u00a3o em Python\n\nObserva\u00c3\u00a7\u00c3\u00b5es sobre a Conta do Google:\n\u00e2\u0080\u00a2 Para se inscrever na avalia\u00c3\u00a7\u00c3\u00a3o gratuita do Google Cloud Platform, voc\u00c3\u00aa precisa de uma Conta do Google/Gmail, al\u00c3\u00a9m de um cart\u00c3\u00a3o de cr\u00c3\u00a9dito ou uma conta banc\u00c3\u00a1ria. Os servi\u00c3\u00a7os do Google est\u00c3\u00a3o temporariamente indispon\u00c3\u00adveis na China.\n\u00e2\u0080\u00a2 Se voc\u00c3\u00aa for um cliente do Google Cloud Platform com endere\u00c3\u00a7o de faturamento na Uni\u00c3\u00a3o Europeia (UE) e na R\u00c3\u00bassia, leia a documenta\u00c3\u00a7\u00c3\u00a3o de vis\u00c3\u00a3o geral sobre o IVA em: https://cloud.google.com/billing/docs/resources/vat-overview.\n\u00e2\u0080\u00a2 Veja mais perguntas frequentes sobre a avalia\u00c3\u00a7\u00c3\u00a3o gratuita do Google Cloud Platform no site: https://cloud.google.com/free-trial/.\n\nBuscando la versi\u00c3\u00b3n en espa\u00c3\u00b1ol de este curso? Visita https://www.coursera.org/learn/gcp-big-data-ml-fundamentals-es/\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u0097\u00a5\u00e6\u009c\u00ac\u00e8\u00aa\u009e\u00e7\u0089\u0088\u00e3\u0082\u0092\u00e3\u0081\u008a\u00e6\u008e\u00a2\u00e3\u0081\u0097\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0081\u008b\u00ef\u00bc\u009fhttps://www.coursera.org/learn/gcp-big-data-ml-fundamentals-jp/ Introdu\u00c3\u00a7\u00c3\u00a3o \u00c3\u00a0 especializa\u00c3\u00a7\u00c3\u00a3o em dados e machine learning no Google Cloud Platform Introdu\u00c3\u00a7\u00c3\u00a3o ao Google Cloud Platform e seus produtos de Big Data Princ\u00c3\u00adpios b\u00c3\u00a1sicos de computa\u00c3\u00a7\u00c3\u00a3o e armazenamento do GCP An\u00c3\u00a1lise de dados no Cloud M\u00c3\u00b3dulo 5: Escalonamento de an\u00c3\u00a1lise de dados e machine learning Arquiteturas de processamento de dados: ingest\u00c3\u00a3o, transforma\u00c3\u00a7\u00c3\u00a3o e carga escalon\u00c3\u00a1vel Resumo do GCP, Big Data e ML  Neste m\u00c3\u00b3dulo, voc\u00c3\u00aa conhecer\u00c3\u00a1 o Google Cloud Platform e os aspectos de gerenciamento de dados da plataforma. Neste m\u00c3\u00b3dulo, apresentaremos os princ\u00c3\u00adpios b\u00c3\u00a1sicos do Google Cloud Platform, que s\u00c3\u00a3o computa\u00c3\u00a7\u00c3\u00a3o e armazenamento, e mostraremos como eles funcionam para ingest\u00c3\u00a3o de dados, armazenamento e an\u00c3\u00a1lise federada. Neste m\u00c3\u00b3dulo, apresentaremos os casos de uso comuns de Big Data que o Google gerenciar\u00c3\u00a1 para voc\u00c3\u00aa. S\u00c3\u00a3o tarefas amplamente realizadas no setor e que podemos migrar com facilidade para a nuvem. Este m\u00c3\u00b3dulo \u00c3\u00a9 sobre as tecnologias mais inovadoras do Google Cloud Platform que podem n\u00c3\u00a3o ter semelhan\u00c3\u00a7as imediatas com as tecnologias que os participantes est\u00c3\u00a3o usando (\u00e2\u0080\u009co que vem por a\u00c3\u00ad\u00e2\u0080\u009d). Neste m\u00c3\u00b3dulo, apresentaremos as arquiteturas de processamento de dados no Google Cloud Platform: processamento ass\u00c3\u00adncrono com TaskQueues. Arquiteturas orientadas por mensagens com o Pub/Sub Cria\u00c3\u00a7\u00c3\u00a3o de canais com o Dataflow ", "The capstone project will be an analysis using R that answers a specific scientific/business question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the application of a variety of methods and techniques introduced in the previous courses, including exploratory data analysis through data visualization and numerical summaries, statistical inference, and modeling as well as interpretations of these results in the context of the data and the research question. The analysis will implement both frequentist and Bayesian techniques and discuss in context of the data how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the data.\n \nA sampling of the final projects will be featured on the Duke Statistical Science department website.\n\nNote: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone. About the Capstone Project Exploratory Data Analysis (EDA) EDA and Basic Model Selection - Submission EDA and Basic Model Selection - Evaluation Model Selection and Diagnostics Out of Sample Prediction Final Data Analysis - Submission Final Data Analysis - Evaluation Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums. This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. \n\nIn this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. \n\nFeel free to post questions about this assignment on the discussion forum.  This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course.  Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning! We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together. In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge.  In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8. Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments. ", "Welcome to the specialization course of Designing data-intensive applications. \nThis course will be completed on four weeks, it will be supported with videos and exercises.\n\nBy the end of this specialization, learners will be able to propose, design, justify and develop high reliable information systems according to type of data and volume of information, response time, type of processing and queries in order to support scalability, maintainability, security and reliability considering the last information technologies.\n\nSoftware to download:\nMySQL Workbench \nRapidminer\nHadoop framework Hortonworks\nMongoDB\n\nIn case you have a Mac / IOS operating system you need to perform an action called VirtualBox. Designing a transaccional system Designing an analytical system Designing an alternative to relational databases Designing an analytical system within a data lake After completing this module, a learner will learn how to distinguish a transactional from an analytical information system according to consistency, concurrency and integrity, and how to propose an architecture that suits user requirements. After completing this module, a learner will learn how to distinguish a transactional from an analytical information system according to the queries required on a huge amount of historical structured data that requires fast processing. After completing this module, a learner will learn how to distinguish which database technology to use to suit the user requirements, detect frauds and support ACID properties. After completing this module, a learner will identify the architecture and technologies required to analyse a huge volume of structured and semistructured data.", "The nature of digital manufacturing and design (DM&D), and its heavy reliance on creating a digital thread of product and process data and information, makes it a prime target for hackers and counterfeiters. This course will introduce students to why creating a strong and secure infrastructure should be of paramount concern for anyone operating in the DM&D domain, and measures that can be employed to protect operational technologies, systems and resources. \n\nAcquire knowledge about security needs and the application of information security systems. Build the foundational skills needed in performing a risk assessment of operational and information technology assets. Gain valuable insights of implementing controls to mitigate identified risks.\n\nMain concepts of this course will be delivered through lectures, readings, discussions and various videos. \n\nThis is the seventh course in the Digital Manufacturing & Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related professional goal.\n\nTo learn more about the Digital Manufacturing and Design Technology specialization, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Introduction to Digital Manufacturing Security Guidance on Securing Digital Manufacturing Operations Protecting Operational Technologies and Intellectual Property Breach Response The purpose of this module is to introduce you to the information security need, framework and processes, as it applies to creating a strong and secure Digital Manufacturing and Design infrastructure. The purpose of this module is to introduced you to the various components of the digital manufacturing operation and the basic security concepts that can be used to protect these components. The purpose of this module is to describe how the various operational steps of digital manufacturing \u00e2\u0080\u0093 that includes - supply chain, shipping process, mobile device usage and the associated communication \u00e2\u0080\u0093 can be protected, along with the Intellectual Property (IP) items that arise during digital manufacturing and design.\n The purpose of this module is to teach you how to respond to security breaches when they happen. We will start with the threat landscape and system failures and investigate the interplay between security and reliability, which are essential for building dependable systems. The mechanism of continuous monitoring to detect security breaches, and strategies for forensics, breach response, and recovery will also be described.", "In this course, we\u00e2\u0080\u0099re going to go over analytical solutions to common healthcare problems. I will review these business problems and you\u00e2\u0080\u0099ll build out various data structures to organize your data. We\u00e2\u0080\u0099ll then explore ways to group data and categorize medical codes into analytical categories. You will then be able to extract, transform, and load data into data structures required for solving medical problems and be able to also harmonize data from multiple sources. Finally, you will create a data dictionary to communicate the source and value of data. Creating these artifacts of data processes is a key skill when working with healthcare data. Solving the Business Problems Algorithms and \"Groupers\" ETL (Extract, Transform, and Load) From Data to Knowledge In this module, you will explain why comparing healthcare providers with respect to quality can be beneficial, and what types of metrics and reporting mechanisms can drive quality improvement. You'll recognize the importance of making quality comparisons fairer with risk adjustment and be able to defend this methodology to healthcare providers by stating the importance of clinical and non-clinical adjustment variables, and the importance of high-quality data. You will distinguish the important conceptual steps of performing risk-adjustment; and be able to express the serious nature of medical errors within the US healthcare system, and communicate to stakeholders that reliable performance measures and associated interventions are available to help solve this tremendous problem. You will distinguish the traits that help categorize people into the small group of super-utilizers and summarize how this population can be identified and evaluated. You'll inform healthcare managers how healthcare fraud differs from other types of fraud by illustrating various schemes that fraudsters use to expropriate resources. You will discuss analytical methods that can be applied to healthcare data systems to identify potential fraud schemes.  In this module, you will define clinical identification algorithms, identify how data are transformed by algorithm rules, and articulate why some data types are more or less reliable than others when constructing the algorithms. You will also review some quality measures that have NQF endorsement and that are commonly used among health care organizations. You will discuss how groupers can help you analyze a large sample of claims or clinical data. You'll access open source groupers online, and prepare an analytical plan to map codes to more general and usable diagnosis and procedure categories. You will also prepare an analytical plan to map codes to more general and usable analytical categories as well as prepare a value statement for various commercial groupers to inform analytic teams what benefits they can gain from these commercial tools in comparison to the licensing and implementation costs. In this module, you will describe logical processes used by database and statistical programmers to extract, transform, and load (ETL) data into data structures required for solving medical problems. You will also harmonize data from multiple sources and prepare integrated data files for analysis. In this module, you will describe to an analytical team how risk stratification can categorize patients who might have specific needs or problems. You'll list and explain the meaning of the steps when performing risk stratification. You will apply some analytical concepts such as groupers to large samples of Medicare data, also use the data dictionaries and codebooks to demonstrate why understanding the source and purpose of data is so critical. You will articulate what is meant by the general phase -- \u00e2\u0080\u009cContext matters when analyzing and interpreting healthcare data.\u00e2\u0080\u009d  You will also communicate specific questions and ideas that will help you and others on your analytical team understand the meaning of your data.", "The Library of Integrative Network-based Cellular Signatures (LINCS) is an NIH Common Fund program. The idea is to perturb different types of human cells with many different types of perturbations such as: drugs and other small molecules; genetic manipulations such as knockdown or overexpression of single genes; manipulation of the extracellular microenvironment conditions, for example, growing cells on different surfaces, and more. These perturbations are applied to various types of human cells including induced pluripotent stem cells from patients, differentiated into various lineages such as neurons or cardiomyocytes. Then, to better understand the molecular networks that are affected by these perturbations, changes in level of many different variables are measured including: mRNAs, proteins, and metabolites, as well as cellular phenotypic changes such as changes in cell morphology. The BD2K-LINCS Data Coordination and Integration Center (DCIC) is commissioned to organize, analyze, visualize and integrate this data with other publicly available relevant resources. In this course we briefly introduce the DCIC and the various Centers that collect data for LINCS. We then cover metadata and how metadata is linked to ontologies. We then present data processing and normalization methods to clean and harmonize LINCS data. This follow discussions about how data is served as RESTful APIs. Most importantly, the course covers computational methods including: data clustering, gene-set enrichment analysis, interactive data visualization, and supervised learning. Finally, we introduce crowdsourcing/citizen-science projects where students can work together in teams to extract expression signatures from public databases and then query such collections of signatures against LINCS data for predicting small molecules as potential therapeutics. The Library of Integrated Network-based Cellular Signatures (LINCS) Program Overview Metadata and Ontologies Serving Data with APIs Bioinformatics Pipelines The Harmonizome Data Normalization Data Clustering Midterm Exam Enrichment Analysis Machine Learning Benchmarking Interactive Data Visualization Crowdsourcing Projects Final Exam This module provides an overview of the concept behind the LINCS program; and tutorials on how to get started with using the LINCS L1000 dataset. This module includes a broad high level description of the concepts behind metadata and ontologies and how these are applied to LINCS datasets. In this module we explain the concept of accessing data through an application programming interface (API). This module describes the important concept of a Bioinformatics pipeline. This module describes a project that integrates many resources that contain knowledge about genes and proteins. The project is called the Harmonizome, and it is implemented as a web-server application available at: http://amp.pharm.mssm.edu/Harmonizome/  This module describes the mathematical concepts behind data normalization. This module describes the mathematical concepts behind data clustering, or in other words unsupervised learning - the identification of patterns within data without considering the labels associated with the data.  The Midterm Exam consists of 45 multiple choice questions which covers modules 1-7. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets.  This module introduces the important concept of performing gene set enrichment analyses. Enrichment analysis is the process of querying gene sets from genomics and proteomics studies against annotated gene sets collected from prior biological knowledge. This module describes the mathematical concepts of supervised machine learning, the process of making predictions from examples that associate observations/features/attribute with one or more properties that we wish to learn/predict. This module discusses how Bioinformatics pipelines can be compared and evaluated. This module provides programming examples on how to get started with creating interactive web-based data visualization elements/figures. This final module describes opportunities to work on LINCS related projects that go beyond the course. The Final Exam consists of 60 multiple choice questions which covers all of the modules of the course. Some of the questions may require you to perform some analysis with the methods you learned throughout the course on new datasets. ", "What happens when creativity and science come together?  The power to design our world is unleashed, providing tools to inform choices about how we live!  Geodesign is the glue\u00e2\u0080\u0094it\u00e2\u0080\u0099s a process that deploys creativity to connect information to people, using collaboration to better inform how we design our world.\n\nThis course includes well-illustrated lectures by the instructor, but also guest lectures each week to ensure you are hearing a variety of viewpoints.  Each week you will also be able to examine what geodesign is through interactive mapping that showcases real-word Case Study examples of geodesign from around the globe.  As you move along in the course, you will discover the interrelationships of both the physical and human aspects that contribute to how geodesign strategies are composed.  The course concludes with you outlining your own Geodesign Challenge, and receiving feedback about that from your peers. Week 1: Shared Languages Week 2: The Three D's of Geodesign Week 3: The Three C's of Geodesign Week 4: The Influence of Context Week 5: Process and Framework This week we\u00e2\u0080\u0099ll begin your geodesign journey by helping you understand that the roots of geodesign are comprised of languages we all share.  By languages I mean commonalities everyone can relate to; for example: our awareness of the spaces around us, our connection to place (location), and the way we deal with problem solving nearly everyday. In this second week we begin piecing together some of the key components that are central to the geodesign process. These happen to all start with \"D\": Design, Decision, and Data. By the end of this lesson I hope you will recognize how these three key components are interrelated. In this third week you will begin to recognize the complexities inherent in the geodesign process.We will also get into two topics that without which, geodesign is likely to fail: computation and collaboration. As it happens these all begin with \u00e2\u0080\u009cC,\u00e2\u0080\u009d hence this week is the Three C's of Geodesign. By the end of this lesson I hope you will recognize how these three key topics are put into action to accomplish geodesign. In this fourth week you will gain an understanding about the interrelationships of the physical and human aspects that contribute to how geodesign strategies are composed. Those physical and human aspects comprise the context for a geodesign challenge, so this lesson is titled \"The Influence of Context.\" By the end of this lesson I hope you will recognize that through understanding the cultural and physical context of a place your geodesign team will be better equipped to propose sustainable solutions. We have been building up to this week, and now we are here-- putting all the pieces together in the geodesign process!  In this last week (so sad to say that!) you will gain awareness about the three-stage geodesign process.  The lecture includes a overview about the process through a discussion centered on one of the Case Study examples. We also have two guest lectures that share the geodesign process as applied to their work.  By the end of this week I hope you will be able to distinguish between the various components of the geodesign process and how each one's role contributes to and is valuable to the whole process.", "While telling stories with data has been part of the news practice since its earliest days, it is in the midst of a renaissance. Graphics desks which used to be deemed as \u00e2\u0080\u009cthe art department,\u00e2\u0080\u009d a subfield outside the work of newsrooms, are becoming a core part of newsrooms\u00e2\u0080\u0099 operation. Those people (they often have various titles: data journalists, news artists, graphic reporters, developers, etc.) who design news graphics are expected to be full-fledged journalists and work closely with reporters and editors. The purpose of this class is to learn how to think about the visual presentation of data, how and why it works, and how to doit the right way. We will learn how to make graphs like The New York Times, Vox, Pew, and FiveThirtyEight. In the end, you can share\u00e2\u0080\u0093embed your beautiful charts in publications, blog posts, and websites.\n\nThis course assumes you understand basic coding skills, preferably Python. However, we also provide a brief review on Python in Module 1, in case you want to refresh yourself on the basics and perform simple data analysis. Course Orientation Module 1: Visualization in Newsrooms Module 2: Data and Visual Perception Module 3: Narrative Storytelling Module 4: Cognitive Load and Color Perception Course Conclusion In this module, you will become familiar with the course, your classmates, and the learning environment.  This module starts with a summary of the history and emerging trends of data visualization in journalism. You will then explore various types of charts and compare their pros and cons. By doing so, you will be able to recognize a wide variety of graphical forms and evaluate their capabilities/shortcomings as well as what situations each chart type is typically used in storytelling. We will also go through the classic reading by Edward Tufte, The Visual Display of Quantitative Information, and learn how to locate and articulate errors and deception in data visualization. In this module, we will first look at some examples of successful data visualizations in journalism. We will then drill down on numbers, learning the process of transforming data into information. Next, we will explore theories in visual perception and concepts in visualization and familiarize ourselves with the visual channel ranking\u00e2\u0080\u0094a useful guideline in designing news visualizations. You will evaluate pre-attentive attributes and why they are important in visualizations. You will also have hands-on practice to learn how data wrangling helps us make informed decisions. In this module, we will learn about the frameworks and techniques that can be used to integrate visualizations into a narrative. You will examine the role messaging and interactions play in drawing readers into a story package that contains greater detail. For the hands-on exercise, you will start creating graphs in Python. You will apply design theories and concepts you previously learned to build column charts, bar charts, and scatterplots. In this final module, we will explore some related concepts of cognition and memory in visualization. You will examine the importance of using the \u00e2\u0080\u009cright\u00e2\u0080\u009d amount of color in the right place and apply Gestalt principles to de-clutter your data visualization. In the end, we will work on various exercises to create interactive maps with Python.\n ", "Ce cours \u00c3\u00a0 la demande acc\u00c3\u00a9l\u00c3\u00a9r\u00c3\u00a9 sur une semaine couvre les fonctionnalit\u00c3\u00a9s Big Data et Machine Learning (apprentissage automatique) de Google Cloud Platform (GCP). Il pr\u00c3\u00a9sente rapidement Google Cloud Platform et d\u00c3\u00a9crit en d\u00c3\u00a9tail les fonctionnalit\u00c3\u00a9s de traitement des donn\u00c3\u00a9es.\n\n\u00c3\u0080 la fin de ce cours, les participants seront en mesure de r\u00c3\u00a9aliser les t\u00c3\u00a2ches suivantes :\n\u00e2\u0080\u00a2 Identifier l'objectif et la valeur des principaux produits Big Data et machine learning de Google Cloud Platform\n\u00e2\u0080\u00a2 Utiliser CloudSQL et Cloud Dataproc pour migrer les charges de travail MySQL et Hadoop/Pig/Spark/Hive existantes vers Google Cloud Platform\n\u00e2\u0080\u00a2 Utiliser BigQuery et Cloud Datalab pour r\u00c3\u00a9aliser une analyse de donn\u00c3\u00a9es interactive\n\u00e2\u0080\u00a2 Choisir entre Cloud SQL, BigTable et Datastore\n\u00e2\u0080\u00a2 Entra\u00c3\u00aener et utiliser un r\u00c3\u00a9seau de neurones \u00c3\u00a0 l'aide de TensorFlow\n\u00e2\u0080\u00a2 Choisir entre diff\u00c3\u00a9rents produits de traitement des donn\u00c3\u00a9es sur Google Cloud Platform\n\nPour pouvoir s'inscrire \u00c3\u00a0 ce cours, les participants doivent avoir environ un (1) an d'exp\u00c3\u00a9rience dans l'un ou plusieurs des domaines suivants :\n\u00e2\u0080\u00a2 Langage de requ\u00c3\u00aate commun, tel que SQL\n\u00e2\u0080\u00a2 Activit\u00c3\u00a9s d'extraction, de transformation et de chargement\n\u00e2\u0080\u00a2 Mod\u00c3\u00a9lisation de donn\u00c3\u00a9es\n\u00e2\u0080\u00a2 Machine learning et/ou statistiques\n\u00e2\u0080\u00a2 Programmation dans Python\n\nRemarques sur le compte Google :\n\u00e2\u0080\u00a2 Pour b\u00c3\u00a9n\u00c3\u00a9ficier d'une version d'essai gratuite de Google Cloud Platform, un compte Google/Gmail et une carte de cr\u00c3\u00a9dit ou un compte bancaire sont requis (les services Google ne sont actuellement pas disponibles en Chine).\n\u00e2\u0080\u00a2 Si vous \u00c3\u00aates un client Google Cloud Platform avec une adresse de facturation situ\u00c3\u00a9e dans l'Union europ\u00c3\u00a9enne (UE) ou en Russie, consultez le document VAT Overview \u00c3\u00a0 l'adresse : https://cloud.google.com/billing/docs/resources/vat-overview\n\u00e2\u0080\u00a2 Des questions fr\u00c3\u00a9quentes sur la version d'essai gratuite de Google Cloud Platform sont disponibles \u00c3\u00a0 l'adresse : https://cloud.google.com/free-trial/\n\nBuscando la versi\u00c3\u00b3n en espa\u00c3\u00b1ol de este curso? Visita https://www.coursera.org/learn/gcp-big-data-ml-fundamentals-es/\n\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u0097\u00a5\u00e6\u009c\u00ac\u00e8\u00aa\u009e\u00e7\u0089\u0088\u00e3\u0082\u0092\u00e3\u0081\u008a\u00e6\u008e\u00a2\u00e3\u0081\u0097\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0081\u008b\u00ef\u00bc\u009fhttps://www.coursera.org/learn/gcp-big-data-ml-fundamentals-jp/ Pr\u00c3\u00a9sentation de la sp\u00c3\u00a9cialisation Data and Machine Learning on Google Cloud Platform Pr\u00c3\u00a9sentation de Google Cloud Platform et de ses produits Big Data Notions de base sur les ressources de calcul et de stockage de GCP Analyse de donn\u00c3\u00a9es dans le cloud Module 5 - Scaling de l'analyse de donn\u00c3\u00a9es et Machine Learning Architecture de traitement des donn\u00c3\u00a9es Ingestion, transformation et chargement \u00c3\u00a9volutifs Synth\u00c3\u00a8se de GCP, Big Data et ML  Ce module pr\u00c3\u00a9sente Google Cloud Platform et les fonctionnalit\u00c3\u00a9s de gestion des donn\u00c3\u00a9es de la plate-forme. Dans ce module, nous pr\u00c3\u00a9sentons les notions de base sur les ressources de calcul et stockage de Google Cloud Platform et expliquons comment elles fonctionnent pour offrir ingestion des donn\u00c3\u00a9es, stockage et analyse f\u00c3\u00a9d\u00c3\u00a9r\u00c3\u00a9e. Dans ce module, nous pr\u00c3\u00a9sentons les cas d'utilisation courants du Big Data qui seront g\u00c3\u00a9r\u00c3\u00a9s pour vous par Google. Voici les cas les plus r\u00c3\u00a9pandus aujourd'hui dans le secteur et pour lesquels nous proposons une migration facile dans le cloud. Ce module aborde les technologies les plus transformationnelles de Google Cloud Platform sans rapport avec celles utilis\u00c3\u00a9es par les participants (\"l'avenir\"). Dans ce module, vous d\u00c3\u00a9couvrirez les architectures de traitement des donn\u00c3\u00a9es de Google Cloud Platform : Traitement asynchrone avec TaskQueues Architectures de messagerie avec Pub/Sub Cr\u00c3\u00a9er des pipelines avec Dataflow ", "This course will assist you with recreating work that a previous coworker completed, revisiting a project you abandoned some time ago, or simply reproducing a document with a consistent format and workflow. Incomplete information about how the work was done, where the files are, and which is the most recent version can give rise to many complications. This course  focuses on the proper documentation creation process, allowing you and your colleagues to easily reproduce the components of your workflow. Throughout this course, you'll receive helpful demonstrations of RStudio and the R Markdown language and engage in active learning opportunities to help you build a professional online portfolio. Introduction to Reproducible Research and Dynamic Documentation R Markdown: Syntax, Document, and Presentation Formats R Markdown Templates: Processing and Customizing Leveraging Custom Templates from Leading Scientific Journals Working in Teams and Disseminating Templates and Reports This module provides an introduction to the concepts surrounding reproducibility and the Open Science movement, RStudio and GitHub, and foundational cases and authors in the field.  This module explores the R Markdown syntax to format and customize the layout of presentations or reports and will also look at inserting and creating objects such as tables, images, or video within documents. This module goes further with R Markdown to help turn documents, reports, and presentations into templates for easier automation, reproducibility, and customization. This module delves into custom templates available for websites, books, and scientific publishers, such as Elsevier and the IEEE, with the chance to create your first R Package. This module focuses on helpful tips for sharing and using the templates you create, as well as methods for organizing content. We'll also look at a few web-publishing services.", "Important note: The second assignment in this course covers the topic of Graph Analysis in the Cloud, in which you will use Elastic MapReduce and the Pig language to perform graph analysis over a moderately large dataset, about 600GB. In order to complete this assignment, you will need to make use of Amazon Web Services (AWS). Amazon has generously offered to provide up to $50 in free AWS credit to each learner in this course to allow you to complete the assignment. Further details regarding the process of receiving this credit are available in the welcome message for the course, as well as in the assignment itself. Please note that Amazon, University of Washington, and Coursera cannot reimburse you for any charges if you exhaust your credit.\n\nWhile we believe that this assignment contributes an excellent learning experience in this course, we understand that some learners may be unable or unwilling to use AWS. We are unable to issue Course Certificates for learners who do not complete the assignment that requires use of AWS. As such, you should not pay for a Course Certificate in Communicating Data Results if you are unable or unwilling to use AWS, as you will not be able to successfully complete the course without doing so.\n\nMaking predictions is not enough!  Effective data scientists know how to explain and interpret their results, and communicate findings accurately to stakeholders to inform business decisions.  Visualization is the field of research in computer science that studies effective communication of quantitative results by linking perception, cognition, and algorithms to exploit the enormous bandwidth of the human visual cortex.  In this course you will learn to recognize, design, and use effective visualizations.\n\nJust because you can make a prediction and convince others to act on it doesn\u00e2\u0080\u0099t mean you should.  In this course you will explore the ethical considerations around big data and how these considerations are beginning to influence policy and practice.   You will learn the foundational limitations of using technology to protect privacy and the codes of conduct emerging to guide the behavior of data scientists.  You will also learn the importance of reproducibility in data science and how the commercial cloud can help support reproducible research even for experiments involving massive datasets, complex computational infrastructures, or both.\n\nLearning Goals: After completing this course, you will be able to:\n1. Design and critique visualizations\n2. Explain the state-of-the-art in privacy, ethics, governance around big data and data science\n3. Use cloud computing to analyze large datasets in a reproducible way. Visualization Privacy and Ethics Reproducibility and Cloud Computing Statistical inferences from large, heterogeneous, and noisy datasets are useless if you can't communicate them to your colleagues, your customers, your management and other stakeholders.  Learn the fundamental concepts behind information visualization, an increasingly critical field of research and increasingly important skillset for data scientists.  This module is taught by Cecilia Aragon, faculty in the Human Centered Design and Engineering Department. Big Data has become closely linked to issues of privacy and ethics: As the limits on what we *can* do with data continue to evaporate, the question of what we *should* do with data becomes paramount.  Motivated in the context of case studies, you will learn the core principles of codes of conduct for data science and statistical analysis.  You will learn the limits of current theory on protecting privacy while still permitting useful statistical analysis.  Science is facing a credibility crisis due to unreliable reproducibility, and as research becomes increasingly computational, the problem seems to be paradoxically getting worse.  But reproducibility is not just for academics: Data scientists who cannot share, explain, and defend their methods for others to build on are dangerous.  In this module, you will explore the importance of reproducible research and how cloud computing is offering new mechanisms for sharing code, data, environments, and even costs that are critical for practical reproducibility.", "Este curso intensivo sob demanda tem dura\u00c3\u00a7\u00c3\u00a3o de uma semana e foi elaborado com base nos cursos Google Cloud Platform Big Data e Machine Learning Fundamentals. Com videoaulas, demonstra\u00c3\u00a7\u00c3\u00b5es e laborat\u00c3\u00b3rios pr\u00c3\u00a1ticos, voc\u00c3\u00aa aprender\u00c3\u00a1 a criar canais de dados de streaming usando o Google Cloud Pub/Sub e o Dataflow para a tomada de decis\u00c3\u00b5es em tempo real. Voc\u00c3\u00aa tamb\u00c3\u00a9m aprender\u00c3\u00a1 a criar pain\u00c3\u00a9is com conte\u00c3\u00bado personalizado para v\u00c3\u00a1rios p\u00c3\u00bablicos.\n\nPr\u00c3\u00a9-requisitos:\n\u00e2\u0080\u00a2 conclus\u00c3\u00a3o dos cursos Google Cloud Platform Big Data e Machine Learning Fundamentals (ou experi\u00c3\u00aancia equivalente)\n\u00e2\u0080\u00a2 conhecimentos b\u00c3\u00a1sicos de Java\n\nObjetivos:\n\u00e2\u0080\u00a2 compreender casos de an\u00c3\u00a1lise de streaming em tempo real\n\u00e2\u0080\u00a2 usar o servi\u00c3\u00a7o de mensagens ass\u00c3\u00adncronas do Google Cloud Pub/Sub para gerenciar eventos de dados\n\u00e2\u0080\u00a2 escrever canais de streaming e fazer transforma\u00c3\u00a7\u00c3\u00b5es quando necess\u00c3\u00a1rio\n\u00e2\u0080\u00a2 conhecer as duas vias de um canal de streaming: produ\u00c3\u00a7\u00c3\u00a3o e consumo\n\u00e2\u0080\u00a2 usar o Dataflow, o BigQuery e o Cloud Pub/Sub juntos para an\u00c3\u00a1lise e streaming em tempo real M\u00c3\u00b3dulo 1: Arquitetura de canais de an\u00c3\u00a1lise de streaming M\u00c3\u00b3dulo 2: Ingest\u00c3\u00a3o de volumes vari\u00c3\u00a1veis M\u00c3\u00b3dulo 3: Implementa\u00c3\u00a7\u00c3\u00a3o de canais de streaming M\u00c3\u00b3dulo 4: Pain\u00c3\u00a9is e an\u00c3\u00a1lises de streaming M\u00c3\u00b3dulo 5: Como lidar com os requisitos de capacidade e lat\u00c3\u00aancia     ", "The number of composite indices that are constructed and used internationally is growing very fast; but whilst the complexity of quantitative techniques has increased dramatically, the education and training in this area has been dragging and lagging behind. As a consequence, these simple numbers, expected to synthesize quite complex issues, are often presented to the public and used in the political debate without proper emphasis on their intrinsic limitations and correct interpretations. \n\nIn this course on global statistics, offered by the University of Geneva jointly with the ETH Z\u00c3\u00bcrich KOF, you will learn the general approach of constructing composite indices and some of resulting problems. We will discuss the technical properties, the internal structure (like aggregation, weighting, stability of time series), the primary data used and the variable selection methods.  These concepts will be illustrated using a sample of the most popular composite indices. We will try to address not only statistical questions but also focus on the distinction between policy-, media- and paradigm-driven indicators. Welcome module Some introductory issues The steps of constructing a composite index Globalization and Youth labour market indices (ETH Zurich/KOF) Export Potential Assessment (ITC) Liner shipping connectivity indices (UNCTAD) and Human development index (UNDP) Welcome to the first module of this course. In this welcome module, you will be introduced with the Professors that will take part in this course on composites indices. We explain the rationale for composite indices (CIs) and show how they can be of interest. This course is open to NGO members, politicians, journalists, students and all persons interested in understanding, creating and/or interpreting CIs. By the end of this first module, you will have an overview of the content of the course week by week.  This module contains four lessons. The first lesson is an introduction to CIs. It defines what a CI is, introduces their mathematical notation and reviews some core historical aspects of their development,   the need and use of CIs. The second lesson focuses on the demand for CIs while the third lesson develops a qualitative framework for the construction of CIs. More specifically, the intrinsic quality of CIs is discussed by reviewing their pros and cons. Finally, the last lesson of this introductory module sketches the steps involved in the construction of a CI. \nLearning outcomes: by the end of this module you will have a clear idea what a CI is (definition, ingredients, history, objective), know why it is needed and where it is used (needs and demand), be familiar with the quality requirements and have a first idea steps involved in the construction of a CI.\n This module is organized along four lessons. The objective of this module is to familiarize you with the key steps to undertake when constructing a CI. The first lesson will develop a theoretical framework to support CIs\u00e2\u0080\u0099 construction. Notably, it will cover topics such as variables selection and data issues. The second lesson will introduce a unifying approach to construct CI by discussing aspects related to transformation functions and the elasticity of substitution. The entire third lesson will be devoted to an essential aspect in the construction of a CI:  the choice of weights. Finally, the module will conclude by addressing questions arising after the construction of a CI. For instance, lesson four will discuss how to assess the robustness of the resulting CI. By the end of this module you will be familiar with all the most important technical (or say statistical) steps involved in constructing CIs.  In this module, you will discover two popular indices developed by ETH Zurich: the Young Labour Market Index and the KOF Globalization index.\nIn the first lesson of this week, you will learn more about the Youth Labour Market Index (YLMI). The KOF YLMI captures various aspects of the youth labour market situation of countries across the world. You will learn which indicators are included in the KOF YLMI and how these are aggregated into a single index. Furthermore, you will get to know an online tool that invites you to analyse the youth labour market situation yourself.\nIn the second lesson of this module, you will learn about the KOF Globalization Index which is a widely used composite indicator that measures the degree of globalization for every country in the world since 1970. It distinguishes between three dimensions of globalization: Economic, social and political globalization. In the following module, you will learn why it is important to measure globalization and what the different stages in constructing the KOF Globalization Index are. A critical discussion of the Index sums up the module.\n This module focuses on trade indices developed by the International Trade Centre, the Export Potential Index (EDI) and the Product Diversification index (PDI).\nFrictions often create a gap between what a country could export and what it does export to markets around the world. Trade advisers could better address these frictions and help firms realize greater exports if they knew exactly which products and markets offer best chances. During this week, you will learn about the Export Potential Assessment (EPI and PDI), which indicates products, sectors and markets for trade development activities for over 200 countries and 4,000 products. Based upon an assessment of the exporting country\u00e2\u0080\u0099s supply capacity, the target market\u00e2\u0080\u0099s demand and tariff conditions as well as the bilateral links between the exporting country and the target market, it provides a ranking of untapped opportunities.\n During this week you will be exploring two indices. The first index, the Liner Shipping (Bilateral) Connectivity Index (LSCI/LSBCI) computed each year by UNCTAD since 2004. It provides an overall indicator of a country maritime connectivity related to liner shipping. Throughout this lesson, we give some insights on why the LSCI and LSBCI were developed. We also cover the methodology to build both indices. We then discuss some stylized facts.\nThe second index presented this week is the Human Development Index (HDI) developed by UNDP. During this lesson, you will be slightly introduced with the history of the HDI. We explain the steps of constructing the HDI, i.e. choosing the three dimensions (health, education and living conditions) composing the HDI and their respective indicators, normalizing the indicators and aggregating the indicators and dimensional sub-indices using different methods. Then, we use a practical example to calculate the HDI for one country. At the end, we discuss some limitations of the HDI and give some elements for future improvement. \n", "In this course you will learn how to use survey weights to estimate descriptive statistics, like means and totals, and more complicated quantities like model parameters for linear and logistic regressions.  Software capabilities will be covered with R\u00c2\u00ae receiving particular emphasis.  The course will also cover the basics of record linkage and statistical matching\u00e2\u0080\u0094both of which are becoming more important as ways of combining data from different sources.  Combining of datasets raises ethical issues which the course reviews.  Informed consent may have to be obtained from persons to allow their data to be linked. You will learn about differences in the legal requirements in different countries. Basic Estimation Models Record Linkage Ethics After completing Modules 1 and 2 of this course you will understand how to estimate descriptive statistics, overall and for subgroups, when you deal with survey data.  We will review software for estimation (R, Stata, SAS) with examples for how to estimate things like means, proportions, and totals.  You will also learn how to estimate parameters in linear, logistic, and other models and learn software options with emphasis on R. Module 3 and 4 discuss how you can add additional data to your analysis. This requires knowing about record linkage techniques, and what it takes to get permission to link data. Module 2 covers how to estimate linear and logistic model parameters using survey data. After completing this module, you will understand how the methods used differ from the ones for non-survey data. We also cover the features of survey data sets that need to be accounted for when estimating standard errors of estimated model parameters. Module starts with the current debate on using more (linked) administrative records in the U.S. Federal Statistical System, and a general motivation for linking records. Several examples will be given on why it is useful to link data. Challenges of record linkage will be discussed. A brief overview over key linkage techniques is included as well. This module will discuss key issues in obtaining consent to record linkage. Failure to consent can lead to bias estimates. Current research examples will be given as well as practical suggestions on how to obtain linkage consent.\n", "The capstone course, Design and Build a Data Warehouse for Business Intelligence Implementation, features a real-world case study that integrates your learning across all courses in the specialization. In response to business requirements presented in a case study, you\u00e2\u0080\u0099ll design and build a small data warehouse, create data integration workflows to refresh the warehouse, write SQL statements to support analytical and summary query requirements, and use the MicroStrategy business intelligence platform to create dashboards and visualizations.\n\nIn the first part of the capstone course, you\u00e2\u0080\u0099ll be introduced to a medium-sized firm, learning about their data warehouse and business intelligence requirements and existing data sources. You\u00e2\u0080\u0099ll first architect a warehouse schema and dimensional model for a small data warehouse. You\u00e2\u0080\u0099ll then create data integration workflows using Pentaho Data Integration to refresh your data warehouse. Next, you\u00e2\u0080\u0099ll write SQL statements for analytical query requirements and create materialized views to support summary data management. Finally, you will use MicroStrategy OLAP capabilities to gain insights into your data warehouse. In the completed project, you\u00e2\u0080\u0099ll have built a small data warehouse containing a schema design, data integration workflows, analytical queries, materialized views, dashboards and visualizations that you\u00e2\u0080\u0099ll be proud to show to your current and prospective employers. Course Overview Data Warehouse Design Data Integration Analytical Queries and Summary Data Management  Data Visualization and Dashboard Design Requirements   Wrap Up and Project Submission Module 1 introduces the objectives and topics in the course and provides background on the case and software requirements. The capstone course is organized around a realistic case study based on the business situation faced by CPI Card Group in 2015.  Module 2 presents the requirements of the first part of the case study involving data warehouse design. To provide a context for the case study, you can listen to an executive interview with a CPI Card Group executive. Module 3 presents requirements for the second part of the case study involving data integration. To provide a context for the case study, you can listen to executive interviews with executives from CPI Card Group, First Bank, and Pinnacol Assurance. Module 4 presents requirements for the third part of the case study involving analytical queries and summary data management.  Module 5 presents the data visualization and dashboard design requirements for the fourth part of the case study.  This is an extension of Module 5. The peer assessment from module 5 is moved to module 6 to give you more time completing the assignments in prior modules as well as for you to do your peer assessment in this module.", "In this capstone project we\u00e2\u0080\u0099ll combine  all of the skills from all four specialization courses to do something really fun: analyze social networks!  \n\nThe opportunities for learning are practically endless in a social network.  Who are the \u00e2\u0080\u009cinfluential\u00e2\u0080\u009d members of the network?  What are the sub-communities in the network?   Who is connected to whom, and by how many links?   These are just some of the questions you can explore in this project.\n\nWe will provide you with a real-world data set and some infrastructure for getting started, as well as some warm up tasks and basic project requirements, but then it\u00e2\u0080\u0099ll be up to you where you want to take the project.  If you\u00e2\u0080\u0099re running short on ideas, we\u00e2\u0080\u0099ll have several suggested directions that can help get your creativity and imagination going.  Finally, to integrate the skills you acquired in course 4 (and to show off your project!) you will be asked to create a video showcase of your final product. Introduction and Warm up Project Definition and Scope Capstone Implementation: Mini-project Capstone Implementation: Full project checkpoint Capstone Implementation: Full project final deadline Capstone oral report Welcome to our capstone project!  In the last four courses in this specialization you've learned many core data structures and algorithms, and applied them to three different real-world projects. In this capstone project you'll be doing a project very much like the projects from these other courses, only it will be almost entirely directed by you!  In this first week you'll get warmed up by playing around with the data that will form the backbone of this project: social network data.  Then you'll get back into writing code by implementing a couple of graph algorithms to answer questions about this data.   Now that you're warmed up, it's time to get started planning for the bulk of your capstone project.  This week you will identify several questions you'd like to answer about the social network data.  For each of these questions, you'll research and evaluate data structures and algorithms that would be useful in implementing a solution.  Defining the scope of your project and anticipating bottlenecks and tricky spots is tough but extremely valuable.  You'll use asymptotic analysis to guide and refine your design. Now that you've identified the two problems you want to solve, this week you'll work to solve the easier of the two. This week you are predominately on your own to work independently.  To solve the problem, you'll likely create small datasets for testing, research existing solutions to related problems, implement a solution, test your solution, and analyze the algorithmic runtime of the solution.  You can optionally write-up a report of your work for peer-review feedback.  This week, you will work on your own on the larger problem you aim to solve.  You'll have two weeks (this and the next) to solve the larger problem and submit a report for peer feedback.  For this week, you should aim to create small test datasets, research exist solutions, and analyze the runtime of your potential solutions.  You should also research datasets which might be particularly interesting for your problem. Now you get to finalize your project!  This week, you will finish your solution to the larger problem and submit a report for peer feedback.  This is also an opportunity for reflection about what went well and what went poorly in the process of completing the project.  It is also an opportunity to reflect on how far your technical skills have advanced since the beginning of this specialization. In this week, you get to present your project to the learner community!  This will combine all the skills you've learned in the specialization: algorithm analysis, object oriented programming, design and use of data structures, and presenting your work with confidence.  We look forward to seeing what you've created!", "The goal of this course is to understand the foundations of Big Data and the data that is being generated in the health domain and how the use of technology would help to integrate and exploit all those data to extract meaningful information that can be later used in different sectors of the health domain from physicians to management, from patients to care givers, etc. The course will offer to the student a high-level perspective of the importance of the medical context within the European context, the types of data that are managed in the health (clinical) context, the challenges to be addressed in the mining of unstructured medical data (text and image) as well as the opportunities from the analytical point of view with an introduction to the basics of data analytics field. Introduction Challenges in unstructured data in health domain NLP in medical domain Medical Image Analysis Data Analysis of structured information     ", "\u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\n\u00d9\u0087\u00d9\u0084 \u00d8\u00a3\u00d9\u0086\u00d8\u00aa \u00d9\u0085\u00d9\u0087\u00d8\u00aa\u00d9\u0085 \u00d8\u00a8\u00d8\u00b2\u00d9\u008a\u00d8\u00a7\u00d8\u00af\u00d8\u00a9 \u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d8\u00aa\u00d9\u0083 \u00d8\u00a8\u00d8\u00a3\u00d8\u00a8\u00d8\u00b1\u00d8\u00b2 \u00d8\u00b3\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\u00d8\u009f \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d9\u0086 \u00d9\u0081\u00d9\u008a \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u0086 \u00d8\u00a8\u00d9\u0081\u00d9\u0087\u00d9\u0085 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d8\u00a7\u00d8\u00a8 \u00d8\u00b8\u00d9\u0087\u00d9\u0088\u00d8\u00b1 \u00d8\u00b9\u00d8\u00b5\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9. \u00d9\u0081\u00d9\u0087\u00d9\u008a \u00d9\u0085\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5\u00d8\u00a9 \u00d9\u0084\u00d9\u0085\u00d9\u0086 \u00d9\u008a\u00d8\u00b1\u00d9\u008a\u00d8\u00af\u00d9\u0088\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d9\u0085 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b5\u00d8\u00b7\u00d9\u0084\u00d8\u00ad\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0081\u00d8\u00a7\u00d9\u0087\u00d9\u008a\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d8\u00a7\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00a3\u00d9\u0086\u00d8\u00b8\u00d9\u0085\u00d8\u00aa\u00d9\u0087\u00d8\u00a7. \u00d8\u00a5\u00d9\u0086\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d9\u0085\u00d9\u0086 \u00d9\u008a\u00d8\u00b1\u00d9\u008a\u00d8\u00af\u00d9\u0088\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00af\u00d8\u00a1 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0081\u00d9\u0083\u00d9\u008a\u00d8\u00b1 \u00d8\u00a8\u00d8\u00b4\u00d8\u00a3\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d9\u0081\u00d9\u008a\u00d8\u00af\u00d9\u0087\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a8\u00d9\u0087\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u0087\u00d9\u0085 \u00d8\u00a3\u00d9\u0088 \u00d9\u0085\u00d8\u00b3\u00d9\u008a\u00d8\u00b1\u00d8\u00aa\u00d9\u0087\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0086\u00d9\u008a\u00d8\u00a9. \u00d8\u00ad\u00d9\u008a\u00d8\u00ab \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6 \u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a3\u00d8\u00ad\u00d8\u00af \u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00a3\u00d8\u00b7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a7\u00d8\u00a6\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a3\u00d9\u0084\u00d8\u00a7 \u00d9\u0088\u00d9\u0087\u00d9\u0088 Hadoop\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d8\u00b2\u00d8\u00a7\u00d8\u00af \u00d9\u0085\u00d9\u0086 \u00d8\u00b3\u00d9\u0087\u00d9\u0088\u00d9\u0084\u00d8\u00a9 \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d8\u00a5\u00d9\u0085\u00d9\u0083\u00d8\u00a7\u00d9\u0086\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b5\u00d9\u0088\u00d9\u0084 \u00d8\u00a5\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7\u00d8\u008c \u00d9\u0081\u00d9\u0082\u00d8\u00af \u00d8\u00b2\u00d8\u00a7\u00d8\u00af \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d8\u00ad\u00d8\u00aa\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00b7\u00d9\u0088\u00d9\u008a\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0086\u00d8\u00a7!\n\n\u00d9\u0088\u00d9\u0081\u00d9\u008a \u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0085\u00d8\u00a7 \u00d9\u008a\u00d9\u0084\u00d9\u008a:\n\n*  \u00d9\u0088\u00d8\u00b5\u00d9\u0081 \u00d8\u00a3\u00d8\u00a8\u00d8\u00b1\u00d8\u00b2 \u00d8\u00b3\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a8\u00d9\u0085\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00b0\u00d9\u0084\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0085\u00d8\u00ab\u00d9\u0084\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d8\u00b1\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00a7\u00d9\u0082\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00aa\u00d8\u00b6\u00d9\u0085\u00d9\u0086 \u00d8\u00ab\u00d9\u0084\u00d8\u00a7\u00d8\u00ab\u00d8\u00a9 \u00d9\u0085\u00d8\u00b5\u00d8\u00a7\u00d8\u00af\u00d8\u00b1 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d9\u0087\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0081\u00d8\u00b1\u00d8\u00a7\u00d8\u00af \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a4\u00d8\u00b3\u00d8\u00b3\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00b4\u00d8\u00b9\u00d8\u00a7\u00d8\u00b1.\n\n* \u00d8\u00b4\u00d8\u00b1\u00d8\u00ad \u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00a8\u00d8\u00af\u00d8\u00a3 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b1\u00d9\u0081 V \u00d9\u0085\u00d8\u00ab\u00d9\u0084 (volume (\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00ac\u00d9\u0085)\u00d8\u008c \u00d9\u0088velocity (\u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d8\u00b1\u00d8\u00b9\u00d8\u00a9)\u00d8\u008c \u00d9\u0088variety (\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d9\u0088\u00d8\u00b9)\u00d8\u008c \u00d9\u0088veracity (\u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d8\u00ad\u00d8\u00a9)\u00d8\u008c \u00d9\u0088valence (\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0083\u00d8\u00a7\u00d9\u0081\u00d8\u00a4)\u00d8\u008c \u00d9\u0088value (\u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9)) \u00d9\u0088\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00b0\u00d8\u00a7 \u00d8\u00aa\u00d8\u00a4\u00d8\u00ab\u00d8\u00b1 \u00d9\u0083\u00d9\u0084 \u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 \u00d8\u00aa\u00d9\u0084\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5 \u00d9\u0081\u00d9\u008a \u00d8\u00ac\u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d9\u0085\u00d8\u00aa\u00d8\u00a7\u00d8\u00a8\u00d8\u00b9\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00aa\u00d8\u00ae\u00d8\u00b2\u00d9\u008a\u00d9\u0086\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00a8\u00d9\u0084\u00d8\u00a7\u00d8\u00ba \u00d8\u00b9\u00d9\u0086\u00d9\u0087\u00d8\u00a7\n\n* \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d9\u0081\u00d8\u00a7\u00d8\u00af\u00d8\u00a9 \u00d8\u00a8\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 5 \u00d8\u00ae\u00d8\u00b7\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0087\u00d9\u008a\u00d9\u0083\u00d9\u0084\u00d8\u00a9 \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084\u00d9\u0083. \n\n* \u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d9\u0086\u00d8\u00af\u00d8\u00b1\u00d8\u00ac \u00d8\u00aa\u00d8\u00ad\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u0084\u00d8\u00a7 \u00d8\u00aa\u00d9\u0086\u00d8\u00af\u00d8\u00b1\u00d8\u00ac \u00d8\u00aa\u00d8\u00ad\u00d8\u00aa\u00d9\u0087\u00d8\u00a7\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a5\u00d8\u00b9\u00d8\u00a7\u00d8\u00af\u00d8\u00a9 \u00d8\u00aa\u00d8\u00b4\u00d9\u0083\u00d9\u008a\u00d9\u0084 \u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa.\n\n* \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u008a\u00d9\u0085 \u00d8\u00aa\u00d9\u0081\u00d8\u00b3\u00d9\u008a\u00d8\u00b1 \u00d9\u0084\u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0087\u00d9\u0086\u00d8\u00af\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0085\u00d8\u00a7\u00d8\u00b0\u00d8\u00ac \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00a7\u00d8\u00a8\u00d9\u0084 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00b3\u00d9\u008a\u00d8\u00b9 \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9.\n\n* \u00d8\u00aa\u00d9\u0084\u00d8\u00ae\u00d9\u008a\u00d8\u00b5 \u00d9\u0085\u00d9\u008a\u00d8\u00b2\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d8\u00af\u00d8\u00b3 Hadoop \u00d9\u0088\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00a8\u00d9\u0085\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00b0\u00d9\u0084\u00d9\u0083 \u00d9\u0085\u00d9\u0088\u00d8\u00b1\u00d8\u00af YARN \u00d9\u0088\u00d9\u0086\u00d8\u00b8\u00d8\u00a7\u00d9\u0085 \u00d8\u00a5\u00d8\u00af\u00d8\u00a7\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081\u00d8\u008c \u00d9\u0088\u00d9\u0086\u00d8\u00b8\u00d8\u00a7\u00d9\u0085 \u00d9\u0085\u00d9\u0084\u00d9\u0081\u00d8\u00a7\u00d8\u00aa HDFS\u00d8\u008c \u00d9\u0088\u00d9\u0086\u00d9\u0085\u00d9\u0088\u00d8\u00b0\u00d8\u00ac \u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d8\u00a9 MapReduce.\n\n* \u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d9\u0088\u00d8\u00aa\u00d8\u00b4\u00d8\u00ba\u00d9\u008a\u00d9\u0084\u00d9\u0087\u00d8\u00a7 \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a5\u00d8\u00b7\u00d8\u00a7\u00d8\u00b1 \u00d8\u00b9\u00d9\u0085\u00d9\u0084 Hadoop!\n\n\u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0088\u00d8\u00ac\u00d9\u0087\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d9\u0086 \u00d9\u0081\u00d9\u008a \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa.  \u00d9\u0084\u00d8\u00a7 \u00d9\u008a\u00d9\u0084\u00d8\u00b2\u00d9\u0085 \u00d8\u00aa\u00d9\u0088\u00d8\u00a7\u00d9\u0081\u00d8\u00b1 \u00d8\u00ae\u00d8\u00a8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00b3\u00d8\u00a8\u00d9\u0082\u00d8\u00a9\u00d8\u008c \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00ba\u00d9\u0085 \u00d9\u0085\u00d9\u0086 \u00d8\u00b6\u00d8\u00b1\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00aa\u00d9\u0088\u00d8\u00a7\u00d9\u0081\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ac\u00d9\u0087\u00d8\u00b2\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b8\u00d8\u00a7\u00d9\u0087\u00d8\u00b1\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d8\u00a5\u00d9\u0086\u00d8\u00ac\u00d8\u00a7\u00d8\u00b2 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00a7\u00d8\u00ac\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9.  \n\n\u00d9\u0085\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ac\u00d9\u0087\u00d8\u00b2\u00d8\u00a9:\n(\u00d8\u00a3) \u00d9\u0085\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d8\u00ac \u00d8\u00b1\u00d8\u00a8\u00d8\u00a7\u00d8\u00b9\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00a9 (\u00d9\u008a\u00d9\u0088\u00d8\u00b5\u00d9\u0089 \u00d8\u00a8\u00d9\u0085\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d8\u00ac \u00d9\u008a\u00d8\u00af\u00d8\u00b9\u00d9\u0085 \u00d9\u0085\u00d9\u008a\u00d8\u00b2\u00d8\u00a9 VT-x \u00d8\u00a3\u00d9\u0088 AMD-V)\u00d8\u008c 64 \u00d8\u00a8\u00d8\u00aa\u00d8\u009b (\u00d8\u00a8) \u00d8\u00b0\u00d8\u00a7\u00d9\u0083\u00d8\u00b1\u00d8\u00a9 \u00d9\u0088\u00d8\u00b5\u00d9\u0088\u00d9\u0084 \u00d8\u00b9\u00d8\u00b4\u00d9\u0088\u00d8\u00a7\u00d8\u00a6\u00d9\u008a \u00d8\u00a8\u00d8\u00ad\u00d8\u00ac\u00d9\u0085 8 \u00d8\u00ac\u00d9\u008a\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d9\u008a\u00d8\u00aa\u00d8\u009b (\u00d8\u00ac) \u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00ad\u00d8\u00a9 \u00d8\u00ae\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a8\u00d8\u00ad\u00d8\u00ac\u00d9\u0085 20 \u00d8\u00ac\u00d9\u008a\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d9\u008a\u00d8\u00aa. \n\u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00ab\u00d9\u0088\u00d8\u00b1 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ac\u00d9\u0087\u00d8\u00b2\u00d8\u00a9: (\u00d9\u0086\u00d8\u00b8\u00d8\u00a7\u00d9\u0085 Windows): \u00d8\u00a7\u00d9\u0081\u00d8\u00aa\u00d8\u00ad \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d8\u00b8\u00d8\u00a7\u00d9\u0085 \u00d8\u00b9\u00d9\u0086 \u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ba\u00d8\u00b7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00b2\u00d8\u00b1 Start (\u00d8\u00a8\u00d8\u00af\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b4\u00d8\u00ba\u00d9\u008a\u00d9\u0084)\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0086\u00d9\u0082\u00d8\u00b1 \u00d8\u00a8\u00d8\u00b2\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00a3\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u008a\u00d9\u0085\u00d9\u0086 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u008a\u00d9\u0082\u00d9\u0088\u00d9\u0086\u00d8\u00a9 Computer (\u00d8\u00ac\u00d9\u0087\u00d8\u00a7\u00d8\u00b2 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d9\u0085\u00d8\u00a8\u00d9\u008a\u00d9\u0088\u00d8\u00aa\u00d8\u00b1)\u00d8\u008c \u00d8\u00ab\u00d9\u0085 \u00d8\u00a7\u00d9\u0086\u00d9\u0082\u00d8\u00b1 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 Properties (\u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5)\u00d8\u009b (\u00d9\u0086\u00d8\u00b8\u00d8\u00a7\u00d9\u0085 Mac): \u00d8\u00a7\u00d9\u0081\u00d8\u00aa\u00d8\u00ad Overview (\u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d8\u00a9) \u00d8\u00b9\u00d9\u0086 \u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ba\u00d8\u00b7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085\u00d8\u00a9 Apple \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0082\u00d8\u00b1 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \"About This Mac.\" \u00d8\u00b3\u00d9\u008a\u00d8\u00aa\u00d9\u0088\u00d9\u0081\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0086\u00d9\u0089 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00b9\u00d8\u00b8\u00d9\u0085 \u00d8\u00a3\u00d8\u00ac\u00d9\u0087\u00d8\u00b2\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d9\u0085\u00d8\u00a8\u00d9\u008a\u00d9\u0088\u00d8\u00aa\u00d8\u00b1 \u00d8\u00b0\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d8\u00a7\u00d9\u0083\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b4\u00d9\u0088\u00d8\u00a7\u00d8\u00a6\u00d9\u008a\u00d8\u00a9 \u00d8\u00b3\u00d8\u00b9\u00d8\u00a9 8 \u00d8\u00ac\u00d9\u008a\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d9\u008a\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d9\u0085 \u00d8\u00b4\u00d8\u00b1\u00d8\u00a7\u00d8\u00a4\u00d9\u0087\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00a2\u00d8\u00ae\u00d8\u00b1 3 \u00d8\u00a3\u00d8\u00b9\u00d9\u0088\u00d8\u00a7\u00d9\u0085. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00ad\u00d8\u00aa\u00d8\u00a7\u00d8\u00ac \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00b3\u00d8\u00b1\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a7\u00d8\u00aa\u00d8\u00b5\u00d8\u00a7\u00d9\u0084 \u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d9\u0086\u00d8\u00aa\u00d8\u00b1\u00d9\u0086\u00d8\u00aa \u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d9\u0083 \u00d8\u00b3\u00d8\u00aa\u00d9\u0082\u00d9\u0088\u00d9\u0085 \u00d8\u00a8\u00d8\u00aa\u00d9\u0086\u00d8\u00b2\u00d9\u008a\u00d9\u0084 \u00d9\u0085\u00d9\u0084\u00d9\u0081\u00d8\u00a7\u00d8\u00aa \u00d9\u008a\u00d8\u00b5\u00d9\u0084 \u00d8\u00ad\u00d8\u00ac\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 4 \u00d8\u00ac\u00d9\u008a\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d9\u008a\u00d8\u00aa.\n\n\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9: \u00d8\u00aa\u00d8\u00b9\u00d8\u00aa\u00d9\u0085\u00d8\u00af \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0081\u00d8\u00aa\u00d9\u0088\u00d8\u00ad\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b5\u00d8\u00af\u00d8\u00b1\u00d8\u008c \u00d9\u0088\u00d9\u0085\u00d9\u0086\u00d9\u0087\u00d8\u00a7 Apache Hadoop. \u00d9\u0088\u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d8\u00aa\u00d9\u0086\u00d8\u00b2\u00d9\u008a\u00d9\u0084 \u00d8\u00ac\u00d9\u0085\u00d9\u008a\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d9\u0085\u00d8\u00ac\u00d8\u00a7\u00d9\u0086\u00d9\u008b\u00d8\u00a7.\n\u00d8\u00aa\u00d8\u00aa\u00d8\u00b6\u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00a7 \u00d9\u008a\u00d9\u0084\u00d9\u008a: Windows 7+ \u00d8\u00a3\u00d9\u0088 Mac OS X 10.10+ \u00d8\u00a3\u00d9\u0088 Ubuntu 14.04+ \u00d8\u00a3\u00d9\u0088 CentOS 6+ VirtualBox 5+. \u00d9\u0085\u00d8\u00b1\u00d8\u00ad\u00d8\u00a8\u00d9\u008b\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9: \u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d8\u00a8\u00d8\u00a8 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d8\u00a7\u00d9\u0086 \u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d8\u00a3\u00d8\u00a8\u00d8\u00b9\u00d8\u00a7\u00d8\u00af \u00d9\u0082\u00d8\u00a7\u00d8\u00a8\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00b3\u00d8\u00b9 \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa: \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d9\u0081\u00d8\u00a7\u00d8\u00af\u00d8\u00a9 \u00d8\u00a8\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a3\u00d8\u00b3\u00d8\u00b3 \u00d8\u00a3\u00d9\u0086\u00d8\u00b8\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0088\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d8\u00b8\u00d9\u0085\u00d8\u00a9: \u00d8\u00a8\u00d8\u00af\u00d8\u00a1 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Hadoop \u00d9\u0085\u00d8\u00b1\u00d8\u00ad\u00d8\u00a8\u00d9\u008b\u00d8\u00a7 \u00d8\u00a8\u00d9\u0083\u00d9\u0085 \u00d9\u0081\u00d9\u008a \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9! \u00d9\u008a\u00d8\u00b3\u00d8\u00b9\u00d8\u00af\u00d9\u0086\u00d8\u00a7 \u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d9\u0083\u00d9\u0085 \u00d8\u00a5\u00d9\u0084\u00d9\u008a\u00d9\u0086\u00d8\u00a7 \u00d9\u0088\u00d9\u0086\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00b9 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00a5\u00d9\u0084\u00d9\u008a\u00d9\u0083\u00d9\u0085! \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa - \u00d8\u00a5\u00d9\u0086\u00d9\u0087\u00d8\u00a7 \u00d9\u0085\u00d9\u0088\u00d8\u00ac\u00d9\u0088\u00d8\u00af\u00d8\u00a9 \u00d9\u0085\u00d9\u0086\u00d8\u00b0 \u00d9\u0081\u00d8\u00aa\u00d8\u00b1\u00d8\u00a9 (\u00d9\u0088\u00d9\u0084\u00d9\u0088 \u00d9\u0083\u00d8\u00a7\u00d9\u0086\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d8\u00b5\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d8\u00ad\u00d8\u00aa\u00d9\u0089).  \u00d9\u0085\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d9\u008a\u00d8\u00ac\u00d8\u00b9\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \"\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\" \u00d9\u0088\u00d9\u0085\u00d9\u0086 \u00d8\u00a3\u00d9\u008a\u00d9\u0086 \u00d8\u00aa\u00d8\u00a3\u00d8\u00aa\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\u00d8\u009f \u00d8\u00b1\u00d8\u00a8\u00d9\u0085\u00d8\u00a7 \u00d8\u00b3\u00d9\u0085\u00d8\u00b9\u00d8\u00aa \u00d8\u00b9\u00d9\u0086 \u00d9\u0085\u00d8\u00b5\u00d8\u00b7\u00d9\u0084\u00d8\u00ad \"Big Vs\". \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d9\u0086\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6 \u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0085\u00d8\u00ab\u00d9\u0084\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d8\u00b5\u00d8\u00a7\u00d9\u0081 \u00d9\u0084\u00d9\u0084\u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d9\u0085\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00aa\u00d9\u0085 \u00d9\u0085\u00d9\u0086\u00d8\u00a7\u00d9\u0082\u00d8\u00b4\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d8\u00a7\u00d8\u00af\u00d8\u00a9\u00d9\u008b. \u00d9\u0088\u00d9\u0084\u00d9\u0083\u00d9\u0086\u00d9\u0086\u00d8\u00a7 \u00d9\u0086\u00d8\u00b1\u00d9\u008a\u00d8\u00af \u00d8\u00a3\u00d9\u0086 \u00d9\u0086\u00d8\u00b7\u00d8\u00b1\u00d8\u00ad \u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d8\u00a9 \u00d8\u00b3\u00d8\u00a7\u00d8\u00af\u00d8\u00b3\u00d8\u00a9 \u00d9\u0088\u00d8\u00b3\u00d9\u0086\u00d8\u00b7\u00d9\u0084\u00d8\u00a8 \u00d9\u0085\u00d9\u0086\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d8\u00aa\u00d8\u00a7\u00d8\u00a8\u00d8\u00a9 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a6\u00d9\u0084\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00b3\u00d8\u00aa\u00d9\u0087\u00d8\u00af\u00d9\u0081 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00a3\u00d9\u0084\u00d8\u00a7 \u00d9\u0088\u00d9\u0087\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9. \u00d9\u0086\u00d8\u00ad\u00d9\u0086 \u00d9\u0086\u00d8\u00ad\u00d8\u00a8 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0088\u00d9\u0086\u00d8\u00ad\u00d8\u00a8 \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d9\u0085\u00d8\u00a8\u00d9\u008a\u00d9\u0088\u00d8\u00aa\u00d8\u00b1\u00d8\u008c \u00d9\u0088\u00d9\u0084\u00d9\u0083\u00d9\u0086 \u00d9\u0084\u00d8\u00a7 \u00d8\u00aa\u00d8\u00b3\u00d9\u008a\u00d8\u00a6\u00d9\u0088\u00d8\u00a7 \u00d9\u0081\u00d9\u0087\u00d9\u0085\u00d9\u0086\u00d8\u00a7. \u00d9\u0081\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d9\u0082\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00a3\u00d9\u0086\u00d9\u0086\u00d8\u00a7 \u00d9\u0086\u00d9\u0087\u00d8\u00aa\u00d9\u0085 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d9\u0087\u00d8\u00a7 \u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d8\u00b6\u00d9\u008a\u00d9\u0081 \u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa\u00d9\u0086\u00d8\u00a7 \u00d9\u0088\u00d8\u00ad\u00d9\u008a\u00d8\u00a7\u00d8\u00aa\u00d9\u0086\u00d8\u00a7 \u00d9\u0088\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0086\u00d8\u00a7. \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d9\u0086\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00ad\u00d8\u00af\u00d8\u00a9 \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 5 \u00d8\u00ae\u00d8\u00b7\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9 \u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa. \u00d8\u00aa\u00d8\u00aa\u00d8\u00b7\u00d9\u0084\u00d8\u00a8 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a3\u00d8\u00b7\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0088\u00d8\u00a3\u00d9\u0086\u00d8\u00b8\u00d9\u0085\u00d8\u00a9 \u00d8\u00ac\u00d8\u00af\u00d9\u008a\u00d8\u00af\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d8\u00a9. \u00d9\u0088\u00d9\u0086\u00d8\u00ad\u00d9\u0086 \u00d9\u0084\u00d8\u00a7 \u00d9\u0086\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d9\u0085\u00d8\u00b9\u00d8\u00a7\u00d8\u00b1\u00d9\u0081 \u00d8\u00a3\u00d9\u0088 \u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d8\u00b1\u00d8\u00a8 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00a8\u00d9\u0084 \u00d9\u0086\u00d8\u00b1\u00d9\u008a\u00d8\u00af \u00d8\u00a3\u00d9\u0086 \u00d9\u0086\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d9\u0084\u00d9\u0083 \u00d9\u0085\u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a3\u00d9\u0088\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00a5\u00d8\u00b7\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a8\u00d8\u00b9\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0081\u00d8\u00a7\u00d9\u0087\u00d9\u008a\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9.\n \u00d9\u0084\u00d9\u0086\u00d9\u0084\u00d9\u0082\u00d9\u0090 \u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a8\u00d8\u00b9\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0081\u00d8\u00a7\u00d8\u00b5\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac Hadoop \u00d9\u0088\u00d9\u0086\u00d9\u0085\u00d9\u0088\u00d8\u00b0\u00d8\u00ac MapReduce. \u00d8\u00ab\u00d9\u0085 \u00d8\u00b3\u00d9\u0086\u00d9\u0086\u00d8\u00aa\u00d9\u0082\u00d9\u0084 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \"\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9\" \u00d9\u0088\u00d9\u0086\u00d9\u0086\u00d9\u0081\u00d8\u00b0 \u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d8\u00a8\u00d8\u00b3\u00d9\u008a\u00d8\u00b7\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0086\u00d9\u0085\u00d9\u0088\u00d8\u00b0\u00d8\u00ac MapReduce \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00ac\u00d9\u0087\u00d8\u00a7\u00d8\u00b2 \u00d8\u00a7\u00d9\u0084\u00d8\u00b8\u00d8\u00a7\u00d9\u0087\u00d8\u00b1\u00d9\u008a Cloudera VM. \u00d9\u0086\u00d8\u00b1\u00d8\u00ac\u00d9\u0088 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d9\u0086\u00d8\u00aa\u00d8\u00a8\u00d8\u00a7\u00d9\u0087 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u0086\u00d9\u0086\u00d8\u00a7 \u00d8\u00b3\u00d9\u0088\u00d9\u0081 \u00d9\u0086\u00d9\u0088\u00d8\u00ac\u00d9\u0087\u00d9\u0083 \u00d9\u0081\u00d9\u008a \"\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\" \u00d8\u00a8\u00d8\u00b4\u00d8\u00a3\u00d9\u0086 \u00d8\u00a5\u00d9\u0086\u00d8\u00b4\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00b7\u00d8\u00b7\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d9\u0085 \u00d9\u0086\u00d9\u0085\u00d9\u0088\u00d8\u00b0\u00d8\u00ac MapReduce \u00d9\u0083\u00d8\u00a5\u00d8\u00ad\u00d8\u00af\u00d9\u0089 \u00d9\u0085\u00d8\u00b1\u00d8\u00a7\u00d8\u00ac\u00d8\u00b9\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0082\u00d8\u00b1\u00d8\u00a7\u00d9\u0086.", "Welcome to Data Analytics Foundations for Accountancy II!  I'm excited to have you in the class and look forward to your contributions to the learning community.\n\nTo begin, I recommend taking a few minutes to explore the course site. Review the material we\u00e2\u0080\u0099ll cover each week, and preview the assignments you\u00e2\u0080\u0099ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.\n\nIf you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.\n\nGood luck as you get started, and I hope you enjoy the course! Course Orientation Module 1: Introduction to Machine Learning Module 2: Fundamental Algorithms Module 3: Practical Concepts in Machine Learning Module 4: Overfitting & Regularization Module 5: Fundamental Probabilistic Algorithms Module 6: Feature Engineering Module 7: Introduction to Clustering Module 8: Introduction to Anomaly Detection You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. This module provides the basis for the rest of the course by introducing the basic concepts behind machine learning, and, specifically, how to perform machine learning by using Python and the scikit learn machine learning module. First, you will learn how machine learning and artificial intelligence are disrupting businesses. Next, you will learn about the basic types of machine learning and how to leverage these algorithms in a Python script. Third, you will learn how linear regression can be considered a machine learning problem with parameters that must be determined computationally by minimizing a cost function. Finally, you will learn about neighbor-based algorithms, including the k-nearest neighbor algorithm, which can be used for both classification and regression tasks. This module introduces several of the most important machine learning algorithms: logistic regression, decision trees, and support vector machine. Of these three algorithms, the first, logistic regression, is a classification algorithm (despite its name). The other two, however, can be used for either classification or regression tasks. Thus, this module will dive deeper into the concept of machine classification, where algorithms learn from existing, labeled data to classify new, unseen data into specific categories; and, the concept of machine regression, where algorithms learn a model from data to make predictions for new, unseen data. While these algorithms all differ in their mathematical underpinnings, they are often used for classifying numerical, text, and image data or performing regression in a variety of domains. This module will also review different techniques for quantifying the performance of a classification and regression algorithms and how to deal with imbalanced training data. This module introduces several important and practical concepts in machine learning. First, you will learn about the challenges inherent in applying data analytics (and machine learning in particular) to real world data sets. This also introduces several methodologies that you may encounter in the future that dictate how to approach, tackle, and deploy data analytic solutions. Next, you will learn about a powerful technique to combine the predictions from many weak learners to make a better prediction via a process known as ensemble learning. Specifically, this module will introduce two of the most popular ensemble learning techniques: bagging and boosting and demonstrate how to employ them in a Python data analytics script. Finally, the concept of a machine learning pipeline is introduced, which encapsulates the process of creating, deploying, and reusing machine learning models.  This module introduces the concept of regularization, problems it can cause in machine learning analyses, and techniques to overcome it. First, the basic concept of overfitting is presented along with ways to identify its occurrence. Next, the technique of cross-validation is introduced, which can mitigate the likelihood that overfitting can occur. Next, the use of cross-validation to identify the optimal parameters for a machine learning algorithm trained on a given data set is presented. Finally, the concept of regularization, where an additional penalty term is applied when determining the best machine learning model parameters, is introduced and demonstrated for different regression and classification algorithms. This module starts by discussing practical machine learning workflows that are deployed in production environments, which emphasizes the big picture view of machine learning. Next this module introduces two additional fundamental algorithms: naive Bayes and Gaussian Processes. These algorithms both have foundations in probability theory but operate under very different assumptions. Naive Bayes is generally used for classification tasks, while Gaussian Processes are generally used for regression tasks. This module also discusses practical issues in constructing machine learning workflows. This module introduces an important concept in machine learning, the selection of the actual features that will be used by a machine learning algorithm. Along with data cleaning, this step in the data analytics process is extremely important, yet it is often overlooked as a method for improving the overall performance of an analysis. This module beings with a discussion of ethics in machine learning, in large part because the selection of features can have (sometimes) non-obvious impacts on the final performance of an algorithm. This can be important when machine learning is applied to data in a regulated industry or when the improper application of an algorithm might lead to discrimination. The rest of this module introduces different techniques for either selecting the best features in a data set, or the construction of new features from the existing set of features. This module introduces clustering, where data points are assigned to larger groups of points based on some specific property, such as spatial distance or the local density of points. While humans often find clusters visually with ease in given data sets, computationally the problem is more challenging. This module starts by exploring the basic ideas behind this unsupervised learning technique, as well as different areas in which clustering can be used by businesses. Next, one of the most popular clustering techniques, K-means, is introduced. Next the density-based DB-SCAN technique is introduced. This module concludes by introducing the mixture models technique for probabilistically assigning points to clusters. This module introduces the concept of an anomaly, or outlier, and different techniques for identifying these unusual data points. First, the general concept of an anomaly is discussed and demonstrated in the business community via the detection of fraud, which in general should be an anomaly when compared to normal customers or operations. Next, statistical techniques for identifying outliers are introduced, which often involve simple descriptive statistics that can highlight data that are sufficiently far from the norm for a given data set. Finally, machine learning techniques are reviewed that can either classify outliers or identify points in low density (or outside normal clusters) areas as potential outliers.", "En este curso acelerado a pedido de una semana, los participantes recibir\u00c3\u00a1n una introducci\u00c3\u00b3n pr\u00c3\u00a1ctica sobre c\u00c3\u00b3mo dise\u00c3\u00b1ar y compilar modelos de aprendizaje autom\u00c3\u00a1tico en Google Cloud Platform. Mediante una serie de presentaciones, demostraciones y labs pr\u00c3\u00a1cticos, los participantes conocer\u00c3\u00a1n conceptos de aprendizaje autom\u00c3\u00a1tico (AA) y TensorFlow, y adquirir\u00c3\u00a1n habilidades pr\u00c3\u00a1cticas para desarrollar, evaluar y producir modelos de AA.\n \n OBJETIVOS\n \n En este curso, los participantes adquirir\u00c3\u00a1n las siguientes habilidades:\n \n \u00e2\u0097\u008f Identificar casos pr\u00c3\u00a1cticos de aprendizaje autom\u00c3\u00a1tico\n \n \u00e2\u0097\u008f Compilar un modelo de AA con TensorFlow\n \n \u00e2\u0097\u008f Compilar modelos de AA implementables y escalables con Cloud ML\n \n \u00e2\u0097\u008f Conocer la importancia del procesamiento previo y la combinaci\u00c3\u00b3n de atributos\n \n \u00e2\u0097\u008f Incorporar conceptos avanzados de AA a sus modelos\n \n \u00e2\u0097\u008f Llevar modelos entrenados de AA a producci\u00c3\u00b3n\n \n \n REQUISITOS PREVIOS\n \n Para aprovechar al m\u00c3\u00a1ximo este curso, los participantes deben cumplir con los siguientes requisitos previos:\n \n \u00e2\u0097\u008f Haber completado el curso \"Google Cloud Fundamentals - Big Data and Machine Learning\" O contar con experiencia equivalente\n \n \u00e2\u0097\u008f Tener un conocimiento b\u00c3\u00a1sico del lenguaje de consulta com\u00c3\u00ban, como SQL\n \n \u00e2\u0097\u008f Tener experiencia con las actividades de extracci\u00c3\u00b3n, transformaci\u00c3\u00b3n, carga y modelado de datos\n \n \u00e2\u0097\u008f Haber desarrollado aplicaciones mediante un lenguaje de programaci\u00c3\u00b3n com\u00c3\u00ban, como Python\n \n \u00e2\u0097\u008f Estar familiarizados con el aprendizaje autom\u00c3\u00a1tico o las estad\u00c3\u00adsticas\n \n Notas sobre la Cuenta de Google:\n \u00e2\u0080\u00a2 Por el momento, los servicios de Google no est\u00c3\u00a1n disponibles en China. Bienvenido a Serverless Machine Learning on Google Cloud Platform M\u00c3\u00b3dulo 1: C\u00c3\u00b3mo comenzar a usar el aprendizaje autom\u00c3\u00a1tico M\u00c3\u00b3dulo 2: C\u00c3\u00b3mo crear modelos de AA con TensorFlow M\u00c3\u00b3dulo 3: C\u00c3\u00b3mo escalar modelos de AA con Cloud ML Engine M\u00c3\u00b3dulo 4: Ingenier\u00c3\u00ada de atributos     ", "Learners will create a roadmap to achieve their own personal goals related to the digital manufacturing and design (DM&D) profession, which will help them leverage relevant opportunities. The culminating project provides a tangible element to include in their professional portfolios that showcases their knowledge of Industry 4.0.\n\nThis project is part of the Digital Manufacturing and Design Technology specialization that explores the many facets of manufacturing\u00e2\u0080\u0099s \u00e2\u0080\u009cFourth Revolution,\u00e2\u0080\u009d  aka Industry 4.0. To learn more about the specialization and its courses, please watch the overview video by copying and pasting the following link into your web browser: https://youtu.be/wETK1O9c-CA Project Definition Self-Assessment Network Exploration Roadmap Execution Final Project Submission and Peer Review Evaluation The purpose of this module is to introduce learners to the factors and trends motivating the transition from the current state of manufacturing to a Digital Manufacturing and Design (DMD) model. Learners will demonstrate critical thinking to define a personalized roadmap project relating to the transformation such that they can impact themselves and the future of DMD. Details of individual lessons in this module are provided below. The purpose of this module is for learners to identify their current situation, perform a self-assessment, and create a personalized future goal within the Digital Manufacturing and Design paradigm. Details of individual lessons in this module are provided below. This module focuses on defining and acquiring resources to help learners grow their network to assist in attaining the future goal identified within the Digital Manufacturing and Design domain. Details of individual lessons in this module are provided below. In this module, learners will created a realistic roadmap execution plan to transition from a current state to a future desired state in the Digital Manufacturing and Design paradigm, using concepts gleaned throughout the course and elsewhere.  In this final module, learners will continue to complete final edits and submit personalized artifacts and attestations to support a transition from a current state to a future state in Digital Manufacturing and Design as a peer review. Learners will review and provide feedback of the work of classmates. ", "O que \u00c3\u00a9 aprendizado de m\u00c3\u00a1quina e que tipos de problema ele pode resolver? O Google pensa no aprendizado de m\u00c3\u00a1quina de uma maneira um pouco diferente. Ele se concentra mais na l\u00c3\u00b3gica, em vez de apenas em dados. Discutimos por que esse modelo \u00c3\u00a9 \u00c3\u00batil quando pensamos na cria\u00c3\u00a7\u00c3\u00a3o de canais de modelos de aprendizado de m\u00c3\u00a1quina. Em seguida, falamos sobre as cinco fases da convers\u00c3\u00a3o de um poss\u00c3\u00advel caso de uso a ser realizado por aprendizado de m\u00c3\u00a1quina e vemos a import\u00c3\u00a2ncia de n\u00c3\u00a3o ignorar essas fases. Finalizamos com a identifica\u00c3\u00a7\u00c3\u00a3o das tend\u00c3\u00aancias que podem ser ampliadas pelo aprendizado de m\u00c3\u00a1quina e como reconhecer isso. Introdu\u00c3\u00a7\u00c3\u00a3o \u00c3\u00a0 especializa\u00c3\u00a7\u00c3\u00a3o O que significa ter uma estrat\u00c3\u00a9gia focada em IA Como o Google trabalha com o aprendizado de m\u00c3\u00a1quina Aprendizado de m\u00c3\u00a1quina inclusivo Blocos de notas do Python na nuvem Resumo Apresenta a especializa\u00c3\u00a7\u00c3\u00a3o e os profissionais do Google respons\u00c3\u00a1veis pelo curso. Neste m\u00c3\u00b3dulo, voc\u00c3\u00aa saber\u00c3\u00a1 o que significa dizer que o Google tem uma estrat\u00c3\u00a9gia focada em IA e como isso se traduz na pr\u00c3\u00a1tica. Este m\u00c3\u00b3dulo \u00c3\u00a9 sobre o conhecimento organizacional que o Google adquiriu ao longo dos anos. Neste m\u00c3\u00b3dulo, discutiremos por que o aprendizado de m\u00c3\u00a1quina n\u00c3\u00a3o \u00c3\u00a9 imparcial por padr\u00c3\u00a3o. Al\u00c3\u00a9m disso, falaremos sobre o que voc\u00c3\u00aa precisa levar em conta ao adicionar aprendizado de m\u00c3\u00a1quina aos seus produtos. Neste m\u00c3\u00b3dulo, abordaremos o Cloud Datalab, que \u00c3\u00a9 o ambiente de desenvolvimento usado nesta especializa\u00c3\u00a7\u00c3\u00a3o. ", "In this fourth of our five courses, I will go deeper into the training and education leadership skills that are helpful for nursing informatics leaders. I will also guide you through the process of preparing a course document or syllabus for the nursing informatics specialty both in academic settings and in practice or industry.\nFollowing are the course objectives:\n1. Describe relevant nursing informatics course development in clinical and academic settings to understand similarities and differences in informatics teaching and education across settings.\n2. Describe informatics education and training needs for diverse participants with various experience levels to enable development of appropriate training and education materials.\n3. Develop a prototype course syllabus and introductory recorded message to apply learning in a simulated setting.\n4. Describe the benefits of formal and informal mentoring for nursing informaticians to advance career opportunities and support the nursing informatics specialty. The Who What When Were and Why of Nursing Informatics Training and Education Leadership Cultures and Skills for Training and Educating Others Choosing resources for training and educating others Course Development Best Practices for Mentors and Coaches     ", "In this culminating project, you will deploy the tools and techniques that you've mastered over the course of the specialization. You'll work with a real data set to perform analyses and prepare a report of your findings. Introduction Introduction to the Dataset - Andrew Jaffe Understand the Problem Alignment QC the Alignment Get Feature Counts  Exploratory Analysis  Statistical Analysis Gene Set Analysis Describe Your Analysis  In this first week, we'll introduce the project and get you oriented to the tasks that you'll be performing over the next several weeks.  This week, we'll really dig into the dataset by providing an introduction from Andrew Jaffe, the lead scientist on the analysis. You should also be looking ahead to Task 2, which is due in Week 4; the alignment will take a long time to perform, so you should start early. The purpose of genomic data science is to answer fundamental questions in biology. Before starting on the data analysis process, the first step is always to understand the scientific question you are trying to answer. Don't forget to stay on top of the alignment task due in Week 4; it will take a long time to accomplish and shouldn't be put off. Once you have understood the problem, the next step is to obtain the raw data so that you can perform your analysis.  Now you have aligned the data, the next step is to do some quality control to make sure that the data are in good shape.  Now that you have performed alignment and quality control, the next step is to calculate the abundance of every gene in every sample. After summarizing your genomic data the next step is to load the data into R for analysis with Bioconductor. The next step is to perform a statistical analysis to detect genes that are differentially expressed.  In task 6, we have identified genes differentially expressed between fetal and adult brain. Now we will examine these results in a wider context.  The next step is to document your work. One of the major issues in genomic data science is that there are so many steps in the process. If these steps are not documented well the result can be major problems. ", "This capstone project course for the Recommender Systems Specialization brings together everything you've learned about recommender systems algorithms and evaluation into a comprehensive recommender analysis and design project.  You will be given a case study to complete where you have to select and justify the design of a recommender system through analysis of recommender goals and algorithm performance.  \n\nLearners in the honors track will focus on experimental evaluation of the algorithms against medium sized datasets.  The standard track will include a mix of provided results and spreadsheet exploration.\n\nBoth groups will produce a capstone report documenting the analysis, the selected solution, and the justification for that solution. Capstone Project ", "Qu'est-ce que le machine learning et quels types de probl\u00c3\u00a8mes permet-il de r\u00c3\u00a9soudre ? Google adopte une approche particuli\u00c3\u00a8re du machine learning qui s'appuie non seulement sur les donn\u00c3\u00a9es, mais \u00c3\u00a9galement sur la logique. Nous expliquerons l'int\u00c3\u00a9r\u00c3\u00aat que pr\u00c3\u00a9sente cette conception pour la cr\u00c3\u00a9ation d'un pipeline de mod\u00c3\u00a8les de ML. Ensuite, nous examinerons les cinq phases permettant de convertir un cas d'utilisation devant \u00c3\u00aatre trait\u00c3\u00a9 \u00c3\u00a0 l'aide du machine learning et \u00c3\u00a9tudierons pourquoi chaque \u00c3\u00a9tape est importante. Enfin, nous identifierons les biais que le machine learning est susceptible d'amplifier et apprendrons \u00c3\u00a0 les rep\u00c3\u00a9rer. Introduction de la sp\u00c3\u00a9cialisation Le r\u00c3\u00b4le central de l'intelligence artificielle Le machine learning chez Google Le machine learning inclusif Blocs-notes Python dans le cloud R\u00c3\u00a9capitulatif Pr\u00c3\u00a9sentation de la sp\u00c3\u00a9cialisation et des experts Google qui s'occupent de la formation Dans ce module, vous allez d\u00c3\u00a9couvrir comment Google donne la priorit\u00c3\u00a9 \u00c3\u00a0 l'intelligence artificielle dans sa strat\u00c3\u00a9gie d'entreprise et comment ce concept se traduit en pratique. Ce module pr\u00c3\u00a9sente le savoir-faire organisationnel acquis par Google au fil des ann\u00c3\u00a9es. Dans ce module, nous expliquons pourquoi les syst\u00c3\u00a8mes de machine learning ne sont pas inclusifs par d\u00c3\u00a9faut, et nous indiquons les \u00c3\u00a9l\u00c3\u00a9ments que vous devez garder \u00c3\u00a0 l'esprit lorsque vous int\u00c3\u00a9grez le ML \u00c3\u00a0 vos produits. Ce module concerne Cloud Datalab, l'environnement de d\u00c3\u00a9veloppement que vous allez utiliser dans le cadre de cette sp\u00c3\u00a9cialisation. ", "The purpose of this course is to summarize new directions in Chinese history and social science produced by the creation and analysis of big historical datasets based on newly opened Chinese archival holdings, and to organize this knowledge in a framework that encourages learning about China in comparative perspective.\n\nOur course demonstrates how a new scholarship of discovery is redefining what is singular about modern China and modern Chinese history. Current understandings of human history and social theory are based largely on Western experience or on non-Western experience seen through a Western lens. This course offers alternative perspectives derived from Chinese experience over the last three centuries. We present specific case studies of this new scholarship of discovery divided into two stand-alone parts, which means that students can take any part without prior or subsequent attendance of the other part.\n\nPart 1 (https://www.coursera.org/learn/understanding-china-history-part-1) focuses on comparative inequality and opportunity and addresses two related questions \u00e2\u0080\u0098Who rises to the top?\u00e2\u0080\u0099 and \u00e2\u0080\u0098Who gets what?\u00e2\u0080\u0099. \n\nPart 2 (this course) turns to an arguably even more important question \u00e2\u0080\u0098Who are we?\u00e2\u0080\u0099 as seen through the framework of comparative population behavior - mortality, marriage, and reproduction \u00e2\u0080\u0093 and their interaction with economic conditions and human values. We do so because mortality and reproduction are fundamental and universal, because they differ historically just as radically between China and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of human behavior and values.\n\nCourse Overview video: https://youtu.be/dzUPRyJ4ETk Orientation and Module 1: Who Are We and Who Survives Module 2: Who Reproduces and Who Marries Module 3: Who Cares and Course Conclusion Final Exam and Farewell Before you start with the content for Module 1, please review the Assignments and Grading page and introduce yourself to other learners who will be studying this course with you.  In this module, James and his post-graduate student Hao DONG will co-deliver the lectures. Now is time to test your understanding on the entire course. Take the final exam and complete the post-course survey. Your valuable feedback will certainly help us improve future iterations of the course.", "\u00e3\u0081\u0093\u00e3\u0081\u00ae 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e9\u0080\u009f\u00e7\u00bf\u0092\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u0087\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0083\u0089 \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e8\u00a8\u00ad\u00e8\u00a8\u0088\u00e3\u0081\u00a8\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e3\u0081\u0097\u00e3\u0081\u00aa\u00e3\u0081\u008c\u00e3\u0082\u0089\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00ac\u009b\u00e7\u00be\u00a9\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00a2\u00e3\u0080\u0081\u00e3\u0083\u008f\u00e3\u0083\u00b3\u00e3\u0082\u00ba\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0082\u0092\u00e9\u0080\u009a\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00ef\u00bc\u0088ML\u00ef\u00bc\u0089\u00e3\u0081\u00a8 TensorFlow \u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e5\u00bf\u00b5\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0080\u0081ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e3\u0080\u0081\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0080\u0081\u00e8\u00a3\u00bd\u00e5\u0093\u0081\u00e5\u008c\u0096\u00e3\u0081\u00ae\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e7\u00bf\u0092\u00e5\u00be\u0097\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e7\u009b\u00ae\u00e6\u00a8\u0099\n \n \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00ac\u00a1\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00ab\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e2\u0097\u008f \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e3\u0083\u00a6\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b\n \n \u00e2\u0097\u008f TensorFlow \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6 ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u0099\u00e3\u0082\u008b\n \n \u00e2\u0097\u008f Cloud ML \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0083\u0087\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00a4\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e3\u0081\u00aa ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u0099\u00e3\u0082\u008b\n \n \u00e2\u0097\u008f \u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e8\u00a4\u0087\u00e6\u0095\u00b0\u00e3\u0081\u00ae\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e7\u00b5\u0084\u00e3\u0081\u00bf\u00e5\u0090\u0088\u00e3\u0082\u008f\u00e3\u0081\u009b\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ae\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e6\u0080\u00a7\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b\n \n \u00e2\u0097\u008f ML \u00e3\u0081\u00ae\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e6\u00a6\u0082\u00e5\u00bf\u00b5\u00e3\u0082\u0092 ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ab\u00e7\u00b5\u0084\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0082\u0080\n \n \u00e2\u0097\u008f \u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e6\u00b8\u0088\u00e3\u0081\u00bf\u00e3\u0081\u00ae ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e8\u00a3\u00bd\u00e5\u0093\u0081\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\n \n \n \u00e5\u008f\u0097\u00e8\u00ac\u009b\u00e8\u00a6\u0081\u00e4\u00bb\u00b6\n \n \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e5\u00a4\u00a7\u00e9\u0099\u0090\u00e3\u0081\u00ab\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00ab\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00ac\u00a1\u00e3\u0081\u00ae\u00e8\u00a6\u0081\u00e4\u00bb\u00b6\u00e3\u0082\u0092\u00e6\u00ba\u0080\u00e3\u0081\u009f\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e2\u0097\u008f Google Cloud Fundamentals - Big Data and Machine Learning \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e4\u00bf\u00ae\u00e4\u00ba\u0086\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0081\u00af\u00e5\u0090\u008c\u00e7\u00ad\u0089\u00e3\u0081\u00ae\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\n \n \u00e2\u0097\u008f SQL \u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e8\u00a8\u0080\u00e8\u00aa\u009e\u00e3\u0081\u00ae\u00e5\u009f\u00ba\u00e6\u009c\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00ab\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\n \n \u00e2\u0097\u008f \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0080\u0081\u00e6\u008a\u00bd\u00e5\u0087\u00ba\u00e3\u0080\u0081\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0080\u0081\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0081\u00ae\u00e3\u0082\u00a2\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u0093\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0081\u00ae\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\n \n \u00e2\u0097\u008f Python \u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e8\u00a8\u0080\u00e8\u00aa\u009e\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0082\u00a2\u00e3\u0083\u0097\u00e3\u0083\u00aa\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00ae\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\n \n \u00e2\u0097\u008f \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a8\u00e7\u00b5\u00b1\u00e8\u00a8\u0088\u00e3\u0081\u00ab\u00e7\u00b2\u00be\u00e9\u0080\u009a\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\n \n Google \u00e3\u0082\u00a2\u00e3\u0082\u00ab\u00e3\u0082\u00a6\u00e3\u0083\u00b3\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e6\u00b3\u00a8\u00e6\u0084\u008f:\n \u00e2\u0080\u00a2 Google \u00e3\u0081\u00ae\u00e3\u0082\u00b5\u00e3\u0083\u00bc\u00e3\u0083\u0093\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e7\u008f\u00be\u00e5\u009c\u00a8\u00e4\u00b8\u00ad\u00e5\u009b\u00bd\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0081\u0094\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e3\u0081\u0084\u00e3\u0081\u009f\u00e3\u0081\u00a0\u00e3\u0081\u0091\u00e3\u0081\u00be\u00e3\u0081\u009b\u00e3\u0082\u0093\u00e3\u0080\u0082 Serverless Machine Learning on Google Cloud Platform \u00e3\u0081\u00b8\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u0093\u00e3\u0081\u009d \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 1: \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e9\u0096\u008b\u00e5\u00a7\u008b \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 2: Tensorflow \u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u00a7\u008b\u00e7\u00af\u0089 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 3: Cloud ML Engine \u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 4: \u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0082\u00b8\u00e3\u0083\u008b\u00e3\u0082\u00a2\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0     ", "\u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0084\u00d9\u0082\u00d9\u0089 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0081\u00d9\u0083\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a7\u00d8\u00b5\u00d8\u00a9 \u00d8\u00a8\u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa. \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d9\u0081\u00d8\u00b3\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u0088 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa. \u00d9\u0087\u00d9\u0086\u00d8\u00a7\u00d9\u0083 \u00d8\u00b9\u00d9\u0086\u00d8\u00b5\u00d8\u00b1\u00d8\u00a7\u00d9\u0086 \u00d9\u0084\u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9. \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084 \u00d9\u0087\u00d9\u0088 \u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0081\u00d9\u0083\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d8\u00a7\u00d9\u0085\u00d9\u0086\u00d8\u00a9 \u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00aa\u00d8\u00ad\u00d9\u0088\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d9\u0082\u00d8\u00a7\u00d8\u00a8\u00d9\u0084\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082. \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00ab\u00d8\u00a7\u00d9\u0086\u00d9\u008a \u00d9\u0087\u00d9\u0088 \u00d9\u0085\u00d9\u0082\u00d8\u00af\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00b3\u00d9\u008a\u00d8\u00aa\u00d9\u0085 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac \u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d9\u0083\u00d9\u0085 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u008f\u00d8\u00b3\u00d9\u008e\u00d8\u00ae \u00d9\u0088\u00d9\u0084\u00d8\u00ba\u00d8\u00a9 Markdown \u00d9\u0088Git \u00d9\u0088GitHub \u00d9\u0088R \u00d9\u0088RStudio. \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ab\u00d8\u00a7\u00d9\u0086\u00d9\u008a: \u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa \u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ab\u00d8\u00a7\u00d9\u0084\u00d8\u00ab: \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a7\u00d8\u00a8\u00d8\u00b9: \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u008a\u00d9\u0085 \u00d9\u0085\u00d8\u00b4\u00d8\u00b1\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085\u00d9\u0087 \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u0087\u00d8\u00af\u00d8\u00a7\u00d9\u0081 \u00d9\u0088\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d8\u00b5\u00d8\u00af \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d9\u0084 \u00d8\u00b9\u00d9\u0086\u00d8\u00b5\u00d8\u00b1 \u00d9\u0085\u00d9\u0086 \u00d8\u00b9\u00d9\u0086\u00d8\u00a7\u00d8\u00b5\u00d8\u00b1\u00d9\u0087. \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0084\u00d9\u0082\u00d9\u0089 \u00d8\u00a3\u00d9\u008a\u00d8\u00b6\u00d9\u008b\u00d8\u00a7 \u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ac\u00d8\u00a7\u00d9\u0084 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00b6\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d8\u00a5\u00d8\u00b1\u00d8\u00b4\u00d8\u00a7\u00d8\u00af\u00d8\u00a7\u00d8\u00aa \u00d8\u00ad\u00d9\u0088\u00d9\u0084 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d8\u00ac R. \u00d9\u0088\u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d9\u0087\u00d9\u0088 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d9\u0083\u00d8\u00ab\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ad\u00d8\u00a7\u00d8\u00b6\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9. \u00d9\u008a\u00d8\u00aa\u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0087\u00d8\u00af\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a7\u00d8\u00b3\u00d9\u008a \u00d9\u0081\u00d9\u008a \u00d8\u00a5\u00d8\u00b9\u00d8\u00af\u00d8\u00a7\u00d8\u00af\u00d9\u0083 \u00d9\u0084\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d9\u0085\u00d8\u00ac R \u00d9\u0088Rstudio \u00d9\u0088Github \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ae\u00d8\u00b1\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00b3\u00d9\u0086\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00aa\u00d9\u0085\u00d8\u00b1 \u00d9\u0083\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u0090\u00d9\u0085 \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa. \u00d8\u00aa\u00d8\u00b1\u00d9\u0083\u00d8\u00b2 \u00d9\u0085\u00d8\u00ad\u00d8\u00a7\u00d8\u00b6\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00ab\u00d8\u00a7\u00d9\u0084\u00d8\u00ab \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d8\u00a7\u00d9\u0085\u00d9\u0086\u00d8\u00a9 \u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00aa\u00d8\u00b5\u00d9\u0085\u00d9\u008a\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d8\u00ad\u00d9\u0088\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d8\u00a9. \u00d8\u00a5\u00d8\u00b0\u00d8\u00a7 \u00d9\u0083\u00d9\u0086\u00d8\u00aa \u00d8\u00aa\u00d9\u0088\u00d8\u00a7\u00d8\u00ac\u00d9\u0087 \u00d9\u0085\u00d8\u00b4\u00d9\u0083\u00d9\u0084\u00d8\u00a9 \u00d8\u00a3\u00d9\u0088 \u00d8\u00aa\u00d8\u00b1\u00d8\u00ba\u00d8\u00a8 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d9\u0083\u00d8\u00b4\u00d8\u00a7\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00a8\u00d8\u00b9\u00d9\u0085\u00d9\u0082 \u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1\u00d8\u008c \u00d9\u0081\u00d9\u008a\u00d9\u008f\u00d8\u00b1\u00d8\u00ac\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00ad\u00d8\u00ab \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0086\u00d8\u00aa\u00d8\u00af\u00d9\u008a\u00d8\u00a7\u00d8\u00aa. \u00d9\u0081\u00d9\u0087\u00d9\u008a \u00d9\u0085\u00d8\u00b5\u00d8\u00af\u00d8\u00b1 \u00d9\u0085\u00d9\u0085\u00d8\u00aa\u00d8\u00a7\u00d8\u00b2! \u00d9\u0088\u00d8\u00a5\u00d8\u00b0\u00d8\u00a7 \u00d9\u0083\u00d9\u0086\u00d8\u00aa \u00d8\u00a3\u00d8\u00ad\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00a8\u00d8\u00b1\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a\u00d9\u0086 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a8\u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d9\u0084 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d8\u00b9\u00d9\u0084\u00d8\u008c \u00d9\u0081\u00d9\u008a\u00d9\u008f\u00d8\u00b1\u00d8\u00ac\u00d9\u0089 \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d9\u008a\u00d8\u00b5 \u00d8\u00a8\u00d8\u00b9\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d9\u0082\u00d8\u00aa \u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af\u00d8\u00a9 \u00d8\u00b2\u00d9\u0085\u00d9\u0084\u00d8\u00a7\u00d8\u00a6\u00d9\u0083 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 \u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00ac\u00d8\u00a7\u00d8\u00a8\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a3\u00d8\u00b3\u00d8\u00a6\u00d9\u0084\u00d8\u00aa\u00d9\u0087\u00d9\u0085 \u00d8\u00a3\u00d9\u008a\u00d8\u00b6\u00d9\u008b\u00d8\u00a7. \u00d9\u0088\u00d9\u008a\u00d8\u00b9\u00d8\u00aa\u00d8\u00a8\u00d8\u00b1 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d9\u0085\u00d9\u0086 \u00d8\u00a3\u00d9\u0081\u00d8\u00b6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d8\u00b1\u00d9\u0082 \u00d9\u0084\u00d9\u0085\u00d9\u0085\u00d8\u00a7\u00d8\u00b1\u00d8\u00b3\u00d8\u00a9 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa\u00d9\u0083 \u00d9\u0088\u00d8\u00b4\u00d8\u00b1\u00d8\u00ad\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d9\u0084\u00d8\u00a2\u00d8\u00ae\u00d8\u00b1\u00d9\u008a\u00d9\u0086. \u00d9\u0088\u00d9\u0087\u00d8\u00a7\u00d8\u00aa\u00d8\u00a7\u00d9\u0086 \u00d8\u00a7\u00d8\u00ab\u00d9\u0086\u00d8\u00aa\u00d8\u00a7\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00b5\u00d8\u00a7\u00d8\u00a6\u00d8\u00b5 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d9\u0087\u00d8\u00b1\u00d9\u008a\u00d9\u0086. \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a7\u00d8\u00a8\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d9\u0086\u00d8\u00b1\u00d9\u0083\u00d8\u00b2 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d8\u00b4\u00d8\u00b1\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9. \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d9\u0087\u00d9\u008a \u00d9\u0081\u00d8\u00b1\u00d8\u00b5\u00d8\u00aa\u00d9\u0083 \u00d9\u0084\u00d8\u00aa\u00d8\u00ab\u00d8\u00a8\u00d9\u008a\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d9\u0088\u00d8\u00a5\u00d8\u00b9\u00d8\u00af\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00b3\u00d8\u00aa\u00d8\u00ad\u00d8\u00aa\u00d8\u00a7\u00d8\u00ac\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d8\u00a8\u00d9\u0082\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d9\u0088\u00d9\u0084\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0081\u00d9\u008a \u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa.", "\u00e3\u0081\u0093\u00e3\u0081\u00ae 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e9\u0080\u009f\u00e7\u00bf\u0092\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081Data Engineering on Google Cloud Platform \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae\u00e4\u00bb\u00a5\u00e5\u0089\u008d\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e5\u009f\u00ba\u00e3\u0081\u00ab\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e5\u008b\u0095\u00e7\u0094\u00bb\u00e8\u00ac\u009b\u00e7\u00be\u00a9\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00a2\u00e3\u0080\u0081\u00e3\u0083\u008f\u00e3\u0083\u00b3\u00e3\u0082\u00ba\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0082\u0092\u00e9\u0080\u009a\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00a7 Hadoop\u00e3\u0080\u0081Spark\u00e3\u0080\u0081Pig\u00e3\u0080\u0081Hive \u00e3\u0081\u00ae\u00e5\u0090\u0084\u00e3\u0082\u00b8\u00e3\u0083\u00a7\u00e3\u0083\u0096\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0094\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0080\u0081\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0080\u0081\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0094\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00a6\u00e3\u0083\u0089 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0081\u00ae\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0082\u00aa\u00e3\u0083\u0097\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00ab\u00e3\u0082\u00a2\u00e3\u0082\u00af\u00e3\u0082\u00bb\u00e3\u0082\u00b9\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081Google \u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0082\u0092\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0081\u00ab\u00e7\u00b5\u00b1\u00e5\u0090\u0088\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e3\u0083\u008f\u00e3\u0083\u00b3\u00e3\u0082\u00ba\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0082\u00a6\u00e3\u0082\u00a7\u00e3\u0083\u0096 \u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0082\u00bd\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a8 CLI \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e3\u0081\u00a3\u00e3\u0081\u00a6 Dataproc \u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0080\u0081\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6 Spark \u00e3\u0081\u00a8 Pig \u00e3\u0081\u00ae\u00e3\u0082\u00b8\u00e3\u0083\u00a7\u00e3\u0083\u0096\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ac\u00a1\u00e3\u0081\u00ab\u00e3\u0080\u0081BigQuery \u00e3\u0081\u008a\u00e3\u0082\u0088\u00e3\u0081\u00b3\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0081\u00a8\u00e7\u00b5\u00b1\u00e5\u0090\u0088\u00e3\u0081\u0099\u00e3\u0082\u008b iPython \u00e3\u0083\u008e\u00e3\u0083\u00bc\u00e3\u0083\u0088\u00e3\u0083\u0096\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0097\u00e3\u0080\u0081Spark \u00e3\u0082\u0092\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u00be\u008c\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092 API \u00e3\u0082\u0092\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ab\u00e7\u00b5\u00b1\u00e5\u0090\u0088\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e8\u00a6\u0081\u00e4\u00bb\u00b6\n \u00e2\u0080\u00a2 Google Cloud Platform Big Data & Machine Learning Fundamentals \u00e3\u0082\u0092\u00e4\u00bf\u00ae\u00e4\u00ba\u0086\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00ef\u00bc\u0088\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0081\u00af\u00e5\u0090\u008c\u00e7\u00ad\u0089\u00e3\u0081\u00ae\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00ef\u00bc\u0089\n \u00e2\u0080\u00a2 Python \u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 1: Cloud Dataproc \u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 2: Dataproc \u00e3\u0082\u00b8\u00e3\u0083\u00a7\u00e3\u0083\u0096\u00e3\u0081\u00ae\u00e5\u00ae\u009f\u00e8\u00a1\u008c \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 3: GCP \u00e3\u0081\u00ae\u00e6\u00b4\u00bb\u00e7\u0094\u00a8 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 4: \u00e9\u009d\u009e\u00e6\u00a7\u008b\u00e9\u0080\u00a0\u00e5\u008c\u0096\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u0088\u0086\u00e6\u009e\u0090    ", "Apr\u00c3\u00a8s avoir pr\u00c3\u00a9sent\u00c3\u00a9 un historique du machine learning, nous \u00c3\u00a9tudierons pourquoi les r\u00c3\u00a9seaux de neurones sont aujourd'hui parfaitement adapt\u00c3\u00a9s \u00c3\u00a0 diverses probl\u00c3\u00a9matiques. Nous apprendrons ensuite \u00c3\u00a0 d\u00c3\u00a9finir un probl\u00c3\u00a8me d'apprentissage supervis\u00c3\u00a9 et \u00c3\u00a0 trouver une solution adapt\u00c3\u00a9e \u00c3\u00a0 l'aide d'une descente de gradient. Ce processus implique la cr\u00c3\u00a9ation d'ensembles de donn\u00c3\u00a9es permettant la g\u00c3\u00a9n\u00c3\u00a9ralisation. Nous examinerons comment proc\u00c3\u00a9der \u00c3\u00a0 cette op\u00c3\u00a9ration de fa\u00c3\u00a7on reproductible de sorte que l'exp\u00c3\u00a9rimentation soit possible.\n\nObjectifs du cours :\nD\u00c3\u00a9terminer pourquoi le deep learning est d\u00c3\u00a9sormais si courant\nOptimiser et \u00c3\u00a9valuer des mod\u00c3\u00a8les en utilisant des fonctions de perte et des statistiques de performances\nCorriger les probl\u00c3\u00a8mes courants li\u00c3\u00a9s au machine learning\nCr\u00c3\u00a9er des ensembles de donn\u00c3\u00a9es de formation, d'\u00c3\u00a9valuation et de test reproductibles et \u00c3\u00a9volutifs Introduction Le machine learning en pratique Optimisation G\u00c3\u00a9n\u00c3\u00a9ralisation et \u00c3\u00a9chantillonnage R\u00c3\u00a9sum\u00c3\u00a9 Dans ce cours, nous vous enseignerons les connaissances fondamentales en mati\u00c3\u00a8re de ML pour que vous puissiez comprendre la terminologie que nous utiliserons au cours de cette sp\u00c3\u00a9cialisation. Gr\u00c3\u00a2ce aux sp\u00c3\u00a9cialistes du machine learning de Google, vous d\u00c3\u00a9couvrirez \u00c3\u00a9galement des astuces pratiques, ainsi que les \u00c3\u00a9cueils \u00c3\u00a0 \u00c3\u00a9viter. \u00c3\u0080 la fin du cours, vous disposerez du code et des connaissances n\u00c3\u00a9cessaires pour lancer vos propres mod\u00c3\u00a8les de ML. Dans ce module, nous vous pr\u00c3\u00a9sentons certains des principaux types de machine learning et aborderons son histoire, des d\u00c3\u00a9buts jusqu'\u00c3\u00a0 l'apog\u00c3\u00a9e. Vous pourrez ainsi rapidement vous familiariser avec le ML. Dans ce module, nous vous guidons sur la voie qui vous permettra d'optimiser vos mod\u00c3\u00a8les de ML. Penchons-nous maintenant sur une question un peu particuli\u00c3\u00a8re : dans quelles conditions est-il pr\u00c3\u00a9f\u00c3\u00a9rable de ne pas choisir le mod\u00c3\u00a8le ML le plus pr\u00c3\u00a9cis ? Comme nous en avons d\u00c3\u00a9j\u00c3\u00a0 parl\u00c3\u00a9 lors du module pr\u00c3\u00a9c\u00c3\u00a9dent sur l'optimisation, ce n'est pas parce que le mod\u00c3\u00a8le appliqu\u00c3\u00a9 \u00c3\u00a0 un ensemble de donn\u00c3\u00a9es d'apprentissage pr\u00c3\u00a9sente un taux de perte \u00c3\u00a9gal \u00c3\u00a0 z\u00c3\u00a9ro qu'il sera performant pour de nouvelles donn\u00c3\u00a9es r\u00c3\u00a9elles. ", "This course seeks to turn learners into informed consumers of social science research. It introduces concepts, standards, and principles of social science research to the interested non-expert. Learners who complete the course will be able to assess evidence and critically evaluate claims about important social phenomena. It reviews the origins and development of social science, describes the process of discovery in contemporary social science research, and explains how contemporary social science differs from apparently related fields. It describes the goals, basic paradigms, and methodologies of the major social science disciplines. It offers an overview of the major questions that are the focus of much contemporary social science research, overall and for China. Special emphasis is given to explaining the challenges that social scientists face in drawing conclusions about cause and effect from their studies, and offers an overview of the approaches that are used to overcome these challenges. Explanation is non-technical and does not involve mathematics. Statistics and quantitative methods are not covered. \n\nExplore the big questions in social science and learn how you can be a critical, informed consumer of social science research. \n\nCourse Overview video: https://youtu.be/QuMOAlwhpvU\n\nAfter you complete Part 1, enroll in Part 2 to learn how to be a PRODUCER of Social science research. \nPart 2:  https://www.coursera.org/learn/social-science-research-chinese-society What is social science? The Big Questions Social Science Research on China The Social Science Disciplines Study Designs Challenges Cause and Effect Final Exam Welcome to Social Science Approaches to the Study of Chinese Society Part 1! Part 1 focuses on being a CONSUMER of social science research.  Take some time to review the Course Overview video and the assignments for this course. In Week 1, we will explore What is Social Science.  By the end of this week, you will be able to understand the differences between social science from humanities, natural and life sciences, outline the origins of social science and have a grasp of key definitions and terms. In Week 2, we will focus on The Big Questions. By the end of this week, you should have some sense of the range of questions that are the focus of much social science research. The next week, Week 3, will expand on China specific research. In Week 3, we will focus on Social Science Research on China. By the end of this week, you should have some sense of major topics in current social science research on China. In Week 4, we will focus on The Social Science Disciplines. By the end of this week, you should have a better understanding of the emphases of each of the major social science disciplines, and the differences between the disciplines. In Week 5, we will focus on Study Designs. By the end of this week, you should understand the differences between the most common types of study, and have some sense of the settings in which each is most relevant. In Week 6, we will focus on Challenges. By the end of this week, you should be able to have a better understanding of key challenges to interpreting results from social science research, and be able to reflect on potential problems with study design. In Week 7, we will focus on Cause and Effect. By the end of this week, you should understand the basic approaches that social scientists follow in trying to establish that an observed relationship reflects cause and effect. You've reached the final exam week!  Complete the final exam and post-course survey. Your feedback can help us improve future iterations of the course.  \n\nGood luck on the exam and hope to see you in Social Science Approaches Part 2. https://www.coursera.org/learn/social-science-research-chinese-society", "The analytics process is a collection of interrelated activities that lead to better decisions and to a higher business performance. The capstone of this specialization is designed with the goal of allowing you to experience this process. The capstone project will take you from data to analysis and models, and ultimately to presentation of insights. \n\nIn this capstone project, you will analyze the data on financial loans to help with the investment decisions of an investment company. You will go through all typical steps of a data analytics project, including data understanding and cleanup, data analysis, and presentation of analytical results. \nFor the first week, the goal is to understand the data and prepare the data for analysis. As we discussed  in this specialization, data preprocessing and cleanup is often the first step in data analytics projects. Needless to say, this step is crucial for the success of this project.  \n\nIn the second week, you will perform some predictive analytics tasks, including classifying loans and predicting losses from defaulted loans. You will try a variety of tools and techniques  this week, as the predictive accuracy of different tools can vary quite a bit. It is rarely the case that the default model produced by ASP is the best model possible. Therefore, it is important for you to tune the different models in order to improve the performance.\n\nBeginning in the third week, we turn our attention to prescriptive analytics, where you will provide some concrete suggestions on how to allocate investment funds using analytics tools, including clustering and simulation based optimization. You will see that allocating funds wisely is crucial for the financial return of the investment portfolio.\n\nIn the last week, you are expected to present your analytics results to your clients. Since you will obtain many results in your project, it is important for you to judiciously choose what to include in your presentation. You are also expected to follow the principles we covered in the courses in preparing your presentation. Module 1 - Understand the data and prepare your data for analysis Module 2 - Perform predictive analytics tasks Module 3 - Provide suggestions on how to allocate investment funds using prescriptive analytics tools Module 4  -  Present your analytics results to your clients This week your goal is to understand the data and prepare the data for analysis. As we discussed in this specialization, data preprocessing and cleanup is often the first step in data analytics projects. Needless to say, this step is crucial for the success of this project.  We've selected a few videos from Courses 2 and 4 for you to review before completing this week's assignments. Dealing With Missing Values and Dealing with Outliers videos will remind you how to perform preliminary data cleanups.  The last part of the assignments ask you to construct data visualizations. You may find the ideas discussed in What is Good Data Visualization? and Graphical Excellence useful.  This week you will perform some predictive analytics tasks, including classifying loans and predicting losses from defaulted loans. You will try a variety of tools and techniques this week, as the predictive accuracy of different tools can vary quite a bit. It is rarely the case that the default model produced by ASP is the best model possible. Therefore, it is important for you to tune the different models in order to improve the performance.This week\u00e2\u0080\u0099s assignments require you to build predictive models for both classification and regression tasks. <p> Before working on the assignments, you may review a few videos to remind yourself several important concepts, such as cross validation. These concepts are discussed in the videos  Cross Validation and Confusion Matrix and Assessing Predictive Accuracy Using Cross-Validation. You may also find a refresher on XLMiner useful. The videos Building Logistic Regression Models using XLMiner  and How to Build a Model using XLMiner discuss how to build logistic regression and linear regression models. Depending on your needs, you may also go back to the videos that discuss how to build trees and neural networks. </p> This week we turn our attention to prescriptive analytics, where you will provide some concrete suggestions on how to allocate investment funds using analytics tools, including clustering and simulation-based optimization. You will see that allocating funds wisely is crucial for the financial return of the investment portfolio. \n <p>The relevant videos for this week are from Course 3: Week 1: Cluster analysis with XLMiner, Week 2: Adding uncertainty to spreadsheet model, Week 2: Defining output variables and analyzing results. </p> You have done a lot so far! In this last week, you will present to your analytics results to your clients. Since you have many results in your project, it is important for you to judiciously choose what to include in your presentation. Several videos in Course 4 offer some guidelines on communicating analytics results. This assignment will give you an opportunity to apply the skills you learned there. \nGood luck!", "The Capstone project is an individual assignment.\nParticipants decide the theme they want to explore and define the issue they want to solve. Their \u00e2\u0080\u009cplaying field\u00e2\u0080\u009d should provide data from various sectors (such as farming and nutrition, culture, economy and employment, Education & Research, International & Europe, Housing, Sustainable, Development & Energies, Health & Social, Society, Territories & Transport). Participants are encouraged to mix the different fields and leverage the existing information with other (properly sourced) open data sets.\n\nDeliverable 1 is the preliminary preparation and problem qualification step. The objectives is to define the what, why & how. What issue do we want to solve? Why does it promise value for public authorities, companies, citizens? How do we want to explore the provided data? \n\nFor Deliverable 2, the participant needs to present the intermediary outputs and adjustments to the analysis framework. The objectives is to confirm the how and the relevancy of the first results. \n\nFinally, with Deliverable 3, the participant needs to present the final outputs and the value case. The objective is to confirm the why. Why will it create value for public authorities, companies, and citizens.\n\nAssessment and grading: the participants will present their results to their peers on a regular basis. An evaluation framework will be provided for the participants to assess the quality of each other\u00e2\u0080\u0099s deliverables. Introduction and step 1 : Define the analysis framework Required assignement 1: Define the analysis framework Required feed back on Delivery 1:Define the analysis framework and preparation of deliverable 2 Practice for Deliverable 2 Required assignement 2: Present the intermediary outputs and adjustments to the analysis framework Required feedback for delivery 2 and preparation of delivery 3 Required Delivery 3: Present the final outputs and value case Required feedback on Assignment 3: Present the final outputs and value case Module 1 in the Business Analytics capstone project provides you with a clear idea of how to successfully complete the ESSEC Business Analytics MOOC. It is dedicated to ensuring that you understand the objectives of the capstone project and lets you consult the datasets to be used for the project as well as examples of what the expected deliverable should look like and contain. Before beginning the project, you are advised to review two previous modules dealing with how to effectively structure and present your findings, and how to approach and explore datasets (\"Foundation of Business Analytics\", the wrap up of \"Case Studies in Business Analytics with Accenture\"). This module  also gives you the opportunity to try out the preparation of deliverable 1 and receive non-graded feedback from your peers, thereby giving you an essential insight into how the other deliverables and peer review steps will work.  The module 2 sets the assignment of preparing deliverable 1 \u00e2\u0080\u0093 your analysis framework \u00e2\u0080\u0093 for assessment by your peers.  In Module 3 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 2 Module 4 enables you to submit your draft proposal for deliverable 2 to your peers for review and feedback. \n The module 5 sets the assignment of preparing deliverable 2 \u00e2\u0080\u0093 Present the intermediary outputs and adjustments to the analysis framework \u00e2\u0080\u0093 for assessment by your peers.  In Module 6 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 3 The module 7 sets the assignment of preparing deliverable 3 \u00e2\u0080\u0093 Present the final outputs and value case \u00e2\u0080\u0093 for assessment by your peers.  In Module 8 you will be involved in reviewing the deliverables of a minimum of 3 other students ( if you can manage more than 3, then all the better!) as well as preparing deliverable 3", "Was ist maschinelles Lernen und welche Probleme lassen sich damit l\u00c3\u00b6sen? F\u00c3\u00bcr Google geht es beim maschinellen Lernen (ML) mehr um Logik als nur um Daten. In diesem Kurs erfahren Sie, warum dieser Ansatz beim Erstellen einer Pipeline aus ML-Modellen n\u00c3\u00bctzlich ist. Au\u00c3\u009ferdem erl\u00c3\u00a4utern wir die f\u00c3\u00bcnf Phasen zur Umsetzung eines f\u00c3\u00bcr ML geeigneten Anwendungsfalls und warum keine dieser Phasen \u00c3\u00bcbersprungen werden darf. Zum Abschluss besprechen wir die Verzerrung, die durch ML entstehen kann, und erkl\u00c3\u00a4ren, wie man sie erkennt. Einf\u00c3\u00bchrung in die Spezialisierung Was bedeutet \"k\u00c3\u00bcnstliche Intelligenz\"? Maschinelles Lernen bei Google Inklusives maschinelles Lernen Python-Notebooks in der Cloud \u00c3\u009cbersicht Hier wird eine Einf\u00c3\u00bchrung in die Spezialisierung geboten und es werden die Google-Experten vorgestellt, die dieses Thema unterrichten. Sie erfahren, was gemeint ist, wenn wir sagen, dass Google auf k\u00c3\u00bcnstliche Intelligenz setzt und wie dies in der Praxis aussieht. In diesem Modul geht es um das organisatorische Know-how, das sich Google \u00c3\u00bcber die Jahre angeeignet hat. In diesem Modul erfahren Sie, warum Systeme f\u00c3\u00bcr maschinelles Lernen standardm\u00c3\u00a4\u00c3\u009fig nicht fair sind und was Sie beachten sollten, wenn Sie in Ihren Produkten maschinelles Lernen nutzen. In diesem Modul geht es um Cloud Datalab, die Entwicklungsumgebung, die Sie f\u00c3\u00bcr dieses spezielle Thema verwenden. ", "Even decades into the Information Age, accounting practices yet fail to recognize the financial value of information. Moreover, traditional asset management practices fail to recognize information as an asset to be managed with earnest discipline. This has led to a business culture of complacence, and the inability for most organizations to fully leverage available information assets. \n\nThis second course in the two-part Infonomics series explores how and why to adapt well-honed asset management principles and practices to information, and how to apply accepted and new valuation models to gauge information\u00e2\u0080\u0099s potential and realized economic benefits. In addition, the course will enlighten students on the critical but confounding issues of information ownership, property rights, and sovereignty. The course will wrap up with an overview of emergent roles for the information-savvy organization of the 21st century. Course Orientation Module 1 Managing Information as an Asset Module 2: Measuring and Accounting for Information Module 3 Privacy, Rights, Ownership, and Sovereignty Module 4 Roles and Organization Structure     ", "In diesem einw\u00c3\u00b6chigen On-Demand-Intensivkurs erhalten die Teilnehmer eine Einf\u00c3\u00bchrung in die Funktionen der Google Cloud Platform (GCP) f\u00c3\u00bcr Big Data und maschinelles Lernen. Dabei wird ein kurzer \u00c3\u009cberblick \u00c3\u00bcber die Google Cloud Platform geboten, w\u00c3\u00a4hrend die Funktionen f\u00c3\u00bcr die Datenverarbeitung eingehender behandelt werden.\n\nNach Abschluss dieses Kurses sind die Teilnehmer in der Lage:\n\u00e2\u0080\u00a2 den Zweck und den Nutzen der wichtigsten Produkte f\u00c3\u00bcr Big Data und maschinelles Lernen in der Google Cloud Platform zu beschreiben,\n\u00e2\u0080\u00a2 vorhandene MySQL- und Hadoop-/Pig-/Spark-/Hive-Arbeitslasten mit Cloud SQL und Cloud Dataproc zur Google Cloud Platform zu migrieren,\n\u00e2\u0080\u00a2 mit BigQuery und Cloud Datalab interaktive Datenanalysen auszuf\u00c3\u00bchren,\n\u00e2\u0080\u00a2 eine Auswahl zwischen Cloud SQL, BigTable und Datastore zu treffen,\n\u00e2\u0080\u00a2 mit TensorFlow ein neuronales Netzwerk zu trainieren und zu verwenden,\n\u00e2\u0080\u00a2 eine Auswahl zwischen verschiedenen Datenverarbeitungsprodukten in der Google Cloud Platform zu treffen.\n\nWenn Sie sich zu diesem Kurs anmelden m\u00c3\u00b6chten, sollten Sie ungef\u00c3\u00a4hr ein (1) Jahr Erfahrung in einem oder mehreren der folgenden Bereiche haben:\n\u00e2\u0080\u00a2 G\u00c3\u00a4ngige Abfragesprache wie SQL\n\u00e2\u0080\u00a2 Extraktions-, Transformations-, Ladeaktivit\u00c3\u00a4ten\n\u00e2\u0080\u00a2 Datenmodellierung\n\u00e2\u0080\u00a2 Maschinelles Lernen und/oder Statistik\n\u00e2\u0080\u00a2 Programmierung in Python\n\nHinweise zum Google-Konto:\n\u00e2\u0080\u00a2 Sie ben\u00c3\u00b6tigen ein Google-/Gmail-Konto und eine Kreditkarte oder ein Bankkonto, um sich f\u00c3\u00bcr den kostenlosen Test der Google Cloud Platform zu registrieren (in China stehen die Dienste von Google derzeit nicht zur Verf\u00c3\u00bcgung).\n\u00e2\u0080\u00a2 Wenn Sie Google Cloud Platform-Kunde mit Rechnungsadresse in der Europ\u00c3\u00a4ischen Union (EU) oder Russland sind, lesen Sie sich die Dokumentation zur Mehrwertsteuer unter https://cloud.google.com/billing/docs/resources/vat-overview durch.\n\u00e2\u0080\u00a2 Weitere h\u00c3\u00a4ufig gestellte Fragen zum kostenlosen Test der Google Cloud Platform finden Sie unter https://cloud.google.com/free-trial/.\n\nBuscando la versi\u00c3\u00b3n en espa\u00c3\u00b1ol de este curso? Visita https://www.coursera.org/learn/gcp-big-data-ml-fundamentals-es/\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u0097\u00a5\u00e6\u009c\u00ac\u00e8\u00aa\u009e\u00e7\u0089\u0088\u00e3\u0082\u0092\u00e3\u0081\u008a\u00e6\u008e\u00a2\u00e3\u0081\u0097\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0081\u008b\u00ef\u00bc\u009fhttps://www.coursera.org/learn/gcp-big-data-ml-fundamentals-jp/ Einf\u00c3\u00bchrung in die Spezialisierung f\u00c3\u00bcr Big Data und maschinelles Lernen mit der Google Cloud Platform Einf\u00c3\u00bchrung in die Google Cloud Platform und ihre Big Data-Produkte Grundlagen der GCP: Compute und Storage Datenanalyse in der Cloud Modul 5: Datenanalysen und maschinelles Lernen skalieren Datenverarbeitungsarchitekturen: Skalierbares Aufnehmen, Transformieren und Laden Zusammenfassung von GCP, Big Data und ML  In diesem Modul erhalten Sie eine Einf\u00c3\u00bchrung in die Google Cloud Platform und ihre Datenverarbeitungsfunktionen. In diesem Modul erhalten Sie eine Einf\u00c3\u00bchrung in Compute und Storage der Google Cloud Platform und lernen deren Funktionsweise bei Datenaufnahme, Speicherung und Verbundanalyse kennen. In diesem Modul stellen wir Ihnen die g\u00c3\u00a4ngigen Big Data-Anwendungsf\u00c3\u00a4lle vor, die von Google f\u00c3\u00bcr Sie verwaltet werden. Diese Vorg\u00c3\u00a4nge sind heute branchenweite Standards und wir sorgen dabei f\u00c3\u00bcr eine einfache Migration zur Cloud. In diesem Modul geht es um die transformationsorientierten Technologien in der Google Cloud Platform, die m\u00c3\u00b6glicherweise keine direkten Parallelen zu den Technologien aufweisen, die die Teilnehmer verwenden (\"Weitere Informationen\"). In diesem Modul stellen wir Ihnen die Datenverarbeitungsarchitekturen in der Google Cloud Platform vor: Asynchrone Verarbeitung mit Aufgabenwarteschlangen. Nachrichtenorientierte Architekturen mit Pub/Sub. Pipelines mit Dataflow erstellen. ", "\u00e3\u0081\u0093\u00e3\u0081\u00ae 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e9\u009b\u0086\u00e4\u00b8\u00ad\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u0087\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0083\u0089 \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081Google Cloud Platform Big Data and Machine Learning Fundamentals \u00e3\u0082\u0092\u00e3\u0083\u0099\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0083\u0093\u00e3\u0083\u0087\u00e3\u0082\u00aa\u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e8\u00ac\u009b\u00e7\u00be\u00a9\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00a2\u00e3\u0080\u0081\u00e3\u0083\u008f\u00e3\u0083\u00b3\u00e3\u0082\u00ba\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0082\u0092\u00e9\u0080\u009a\u00e3\u0081\u0098\u00e3\u0081\u00a6\u00e3\u0080\u0081Google Cloud Pub/Sub \u00e3\u0081\u00a8 Dataflow \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u0092\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0083\u00aa\u00e3\u0082\u00a2\u00e3\u0083\u00ab\u00e3\u0082\u00bf\u00e3\u0082\u00a4\u00e3\u0083\u00a0\u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e6\u0084\u008f\u00e6\u0080\u009d\u00e6\u00b1\u00ba\u00e5\u00ae\u009a\u00e3\u0082\u0092\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e3\u0081\u00ab\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0083\u009b\u00e3\u0083\u00ab\u00e3\u0083\u0080\u00e3\u0083\u00bc\u00e3\u0081\u00ab\u00e5\u0090\u0088\u00e3\u0082\u008f\u00e3\u0081\u009b\u00e3\u0081\u009f\u00e5\u0087\u00ba\u00e5\u008a\u009b\u00e3\u0082\u0092\u00e8\u00a1\u00a8\u00e7\u00a4\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u0080\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u009c\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0082\u0092\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\n\u00e5\u0089\u008d\u00e6\u008f\u0090\u00e6\u009d\u00a1\u00e4\u00bb\u00b6:\n\u00e2\u0080\u00a2 Google Cloud Platform Big Data and Machine Learning Fundamentals \u00e3\u0082\u0092\u00e4\u00bf\u00ae\u00e4\u00ba\u0086\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00ef\u00bc\u0088\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e5\u0090\u008c\u00e7\u00ad\u0089\u00e3\u0081\u00ae\u00e7\u00b5\u008c\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00ef\u00bc\u0089\n\u00e2\u0080\u00a2 Java \u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e7\u00a8\u008b\u00e5\u00ba\u00a6\u00e3\u0081\u00ae\u00e7\u009f\u00a5\u00e8\u00ad\u0098\n\n\u00e7\u009b\u00ae\u00e6\u00a8\u0099:\n\u00e2\u0080\u00a2 \u00e3\u0083\u00aa\u00e3\u0082\u00a2\u00e3\u0083\u00ab\u00e3\u0082\u00bf\u00e3\u0082\u00a4\u00e3\u0083\u00a0 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ae\u00e3\u0083\u00a6\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 Google Cloud Pub/Sub \u00e3\u0081\u00ae\u00e9\u009d\u009e\u00e5\u0090\u008c\u00e6\u009c\u009f\u00e3\u0083\u00a1\u00e3\u0083\u0083\u00e3\u0082\u00bb\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0082\u00b5\u00e3\u0083\u00bc\u00e3\u0083\u0093\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00a4\u00e3\u0083\u0099\u00e3\u0083\u00b3\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u0092\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u00ab\u00e5\u00bf\u009c\u00e3\u0081\u0098\u00e3\u0081\u00a6\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0081\u00ae\u00e4\u00b8\u00a1\u00e9\u009d\u00a2\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u00a8\u00e6\u00b6\u0088\u00e8\u00b2\u00bb\u00e3\u0081\u00ab\u00e7\u00b2\u00be\u00e9\u0080\u009a\u00e3\u0081\u0099\u00e3\u0082\u008b\n\u00e2\u0080\u00a2 \u00e3\u0083\u00aa\u00e3\u0082\u00a2\u00e3\u0083\u00ab\u00e3\u0082\u00bf\u00e3\u0082\u00a4\u00e3\u0083\u00a0 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ae\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab\u00e3\u0080\u0081Dataflow\u00e3\u0080\u0081BigQuery\u00e3\u0080\u0081Cloud Pub/Sub \u00e3\u0082\u0092\u00e7\u009b\u00b8\u00e4\u00ba\u0092\u00e9\u0081\u008b\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 1: \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0081\u00ae\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 2: \u00e5\u008f\u00af\u00e5\u00a4\u0089\u00e3\u0083\u009c\u00e3\u0083\u00aa\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00a0\u00e3\u0081\u00ae\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e8\u00be\u00bc\u00e3\u0081\u00bf \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 3: \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0081\u00ae\u00e5\u00ae\u009f\u00e8\u00a3\u0085 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 4: \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u009f\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00a8\u00e3\u0083\u0080\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u009c\u00e3\u0083\u00bc\u00e3\u0083\u0089 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 5: \u00e3\u0082\u00b9\u00e3\u0083\u00ab\u00e3\u0083\u00bc\u00e3\u0083\u0097\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00a8\u00e3\u0083\u00ac\u00e3\u0082\u00a4\u00e3\u0083\u0086\u00e3\u0083\u00b3\u00e3\u0082\u00b7\u00e3\u0081\u00ae\u00e8\u00a6\u0081\u00e4\u00bb\u00b6\u00e3\u0081\u00ae\u00e6\u0089\u00b1\u00e3\u0081\u0084     ", "Este curso acelerado de una semana est\u00c3\u00a1 basado en cursos anteriores de la especializaci\u00c3\u00b3n \"Data Engineering on Google Cloud Platform\". Mediante una serie de clases por video, demostraciones y labs pr\u00c3\u00a1cticos, aprender\u00c3\u00a1 a crear y administrar cl\u00c3\u00basteres de procesamiento para ejecutar trabajos de Hadoop, Spark, Pig o Hive en Google Cloud Platform.Adem\u00c3\u00a1s, aprender\u00c3\u00a1 a acceder a varias opciones de almacenamiento en la nube desde sus cl\u00c3\u00basteres de procesamiento y a integrar las capacidades del aprendizaje autom\u00c3\u00a1tico de Google en sus programas de estad\u00c3\u00adsticas.\n \n En los labs pr\u00c3\u00a1cticos, crear\u00c3\u00a1 y administrar\u00c3\u00a1 cl\u00c3\u00basteres de Dataproc con la consola web y la CLI. Luego, usar\u00c3\u00a1 los cl\u00c3\u00basteres para ejecutar trabajos de Spark y Pig. A continuaci\u00c3\u00b3n, crear\u00c3\u00a1 notebooks de IPython que se integran con BigQuery y el almacenamiento, y utilizar\u00c3\u00a1 Spark. Por \u00c3\u00baltimo, integrar\u00c3\u00a1 las API de aprendizaje autom\u00c3\u00a1tico en el an\u00c3\u00a1lisis de sus datos.\n \n Requisitos previos\n \u00e2\u0080\u00a2 Google Cloud Platform Big Data & Machine Learning Fundamentals (o contar con experiencia equivalente)\n \u00e2\u0080\u00a2 Conocimientos de Python M\u00c3\u00b3dulo\u00c2\u00a01: Introducci\u00c3\u00b3n a Cloud\u00c2\u00a0Dataproc M\u00c3\u00b3dulo\u00c2\u00a02: Ejecuci\u00c3\u00b3n de trabajos de Dataproc M\u00c3\u00b3dulo\u00c2\u00a03: Aproveche GCP M\u00c3\u00b3dulo\u00c2\u00a04: An\u00c3\u00a1lisis de datos no estructurados    ", "Note: You should complete all the other courses in this Specialization before beginning this course.\n\nThis six-week long Project course of the Data Mining Specialization will allow you to apply the learned algorithms and techniques for data mining from the previous courses in the Specialization, including Pattern Discovery, Clustering, Text Retrieval, Text Mining, and Visualization, to solve interesting real-world data mining challenges. Specifically, you will work on a restaurant review data set from Yelp and use all the knowledge and skills you\u00e2\u0080\u0099ve learned from the previous courses to mine this data set to discover interesting and useful knowledge. The design of the Project emphasizes: 1) simulating the workflow of a data miner in a real job setting; 2) integrating different mining techniques covered in multiple individual courses; 3) experimenting with different ways to solve a problem to deepen your understanding of techniques; and 4) allowing you to propose and explore your own ideas creatively. \n\nThe goal of the Project is to analyze and mine a large Yelp review data set to discover useful knowledge to help people make decisions in dining. The project will include the following outputs: \n\n1. Opinion visualization: explore and visualize the review content to understand what people have said in those reviews.\n2. Cuisine map construction: mine the data set to understand the landscape of different types of cuisines and their similarities.\n3. Discovery of popular dishes for a cuisine: mine the data set to discover the common/popular dishes of a particular cuisine.\n4. Recommendation of restaurants to help people decide where to dine: mine the data set to rank restaurants for a specific dish and predict the hygiene condition of a restaurant.\n\nFrom the perspective of users, a cuisine map can help them understand what cuisines are there and see the big picture of all kinds of cuisines and their relations. Once they decide what cuisine to try, they would be interested in knowing what the popular dishes of that cuisine are and decide what dishes to have. Finally, they will need to choose a restaurant. Thus, recommending restaurants based on a particular dish would be useful. Moreover, predicting the hygiene condition of a restaurant would also be helpful. \n\nBy working on these tasks, you will gain experience with a typical workflow in data mining that includes data preprocessing, data exploration, data analysis, improvement of analysis methods, and presentation of results. You will have an opportunity to combine multiple algorithms from different courses to complete a relatively complicated mining task and experiment with different ways to solve a problem to understand the best way to solve it. We will suggest specific approaches, but you are highly encouraged to explore your own ideas since open exploration is, by design, a goal of the Project. \n\nYou are required to submit a brief report for each of the tasks for peer grading. A final consolidated report is also required, which will be peer-graded. Orientation Task 1 - Exploration of a Data Set Task 2 - Cuisine Clustering and Map Construction Task 3 - Dish Recognition Task 4 & 5 - Popular Dishes and Restaurant Recommendation Task 6 Final Report In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment.      ", "R Programming Capstone Obtain and Clean the Data Building Geoms Building a Leaflet Map Documentation and Packaging Deployment Final Assessment The overall goal of the capstone project is to integrate the skills you have developed over the courses in this Specialization and to build a software package that can be used to work with the NOAA Significant Earthquakes dataset. Show us when earthquakes occurred in different countries, their magnitude, and their toll on human life. Show and annotate the earthquake epicenters. Documentation is one of the most important and most commonly overlooked steps when writing software, but you're not going to let that happen in your project. The moment of truth. It's time to push your package to GitHub. It's time to submit your deployed package for evaluation and to evaluate the work of a few of your classmates.", "\u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a3\u00d9\u0081\u00d8\u00b6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0085\u00d8\u00a7\u00d8\u00b1\u00d8\u00b3\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00aa\u00d8\u00ac\u00d8\u00b9\u00d9\u0084 \u00d8\u00a3\u00d9\u008a \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d9\u0084\u00d9\u0087\u00d8\u00a7 \u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a3\u00d9\u0083\u00d8\u00a8\u00d8\u00b1 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00a7\u00d9\u0081\u00d8\u00b3 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a8\u00d8\u00ad. \u00d8\u00b3\u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d9\u0087\u00d9\u0085 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d9\u0088\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d8\u00af\u00d9\u008a\u00d8\u00a9.\n \n\u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0084\u00d8\u00af\u00d9\u008a\u00d9\u0083 \u00d8\u00b5\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d8\u00b6\u00d8\u00ad\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d9\u008a\u00d9\u0088\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d8\u00b6\u00d8\u00b7\u00d9\u0084\u00d8\u00b9 \u00d8\u00a8\u00d9\u0087\u00d8\u00a7 \u00d9\u0083\u00d9\u0084 \u00d9\u0085\u00d9\u0086 \u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u008a \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081 \u00d8\u00a3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00a8\u00d8\u00b7 \u00d8\u00a3\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d8\u00b1\u00d8\u00aa\u00d9\u0081\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d9\u0084\u00d8\u00a8 \u00d8\u00b9\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d8\u00ac\u00d8\u00a7\u00d8\u00ad \u00d9\u0081\u00d9\u008a\u00d9\u0087\u00d8\u00a7.\n \n\u00d9\u0088\u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00b3\u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0086\u00d8\u00a9 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b1\u00d8\u00ac\u00d8\u00b9\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d9\u0088\u00d9\u0081\u00d8\u00b1\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9\u00d8\u009b \u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085 \u00d8\u00a3\u00d9\u008a \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1\u00d9\u008b \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d8\u00a8\u00d9\u0086\u00d9\u0091\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d8\u00ab\u00d9\u0082\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a8\u00d9\u0081\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d9\u0091\u00d8\u00a9. \u00d8\u00aa\u00d9\u008f\u00d8\u00ad\u00d8\u00af\u00d8\u00ab \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 Amazon\u00d8\u008c \u00d9\u0088Uber\u00d8\u008c \u00d9\u0088Airbnb \u00d8\u00aa\u00d8\u00ad\u00d9\u0088\u00d9\u0091\u00d9\u0084\u00d9\u008b\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u0086\u00d8\u00a7\u00d8\u00b9\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d9\u0086 \u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00a8\u00d8\u00af\u00d8\u00a7\u00d8\u00b9\u00d9\u008a \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9.. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u0083 \u00d9\u0084\u00d9\u0090\u00d9\u0085\u00d9\u008e \u00d8\u00aa\u00d9\u0083\u00d9\u0088\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0082\u00d9\u0084\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00ba\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0086\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa\u00d8\u009b \u00d9\u0084\u00d9\u0083\u00d9\u008a \u00d8\u00aa\u00d8\u00aa\u00d9\u0081\u00d9\u0088\u00d9\u0082 \u00d9\u0081\u00d9\u008a \u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00a7\u00d9\u0081\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d8\u00a9. \u00d9\u0086\u00d8\u00a8\u00d8\u00b0\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u008a\u00d9\u0085 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d9\u0088\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d8\u00b1\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084  \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d9\u0085\u00d9\u0082 \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u008a\u00d9\u0082 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d8\u00a9 \u00d8\u00ad\u00d8\u00a7\u00d9\u0084\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d9\u0083\u00d9\u0088\u00d8\u00b1\u00d8\u00b3\u00d9\u008a\u00d8\u00b1\u00d8\u00a7 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7: \u00d9\u0085\u00d9\u0086 Excel \u00d8\u00a5\u00d9\u0084\u00d9\u0089 MySQL: \u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d8\u00ad\u00d9\u0088\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0086\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00ad\u00d9\u0088\u00d9\u0084 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d9\u0081\u00d8\u00a7\u00d8\u00b9\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d9\u0081\u00d9\u008a\u00d8\u00b1 \u00d9\u0082\u00d9\u008a\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084. \u00d9\u0088\u00d9\u008a\u00d8\u00ad\u00d8\u00aa\u00d9\u0088\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a3\u00d8\u00b1\u00d8\u00a8\u00d8\u00b9 \u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d9\u0085\u00d8\u00b4\u00d8\u00b1\u00d9\u0088\u00d8\u00b9 \u00d8\u00ae\u00d8\u00aa\u00d8\u00a7\u00d9\u0085\u00d9\u008a \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00ad\u00d9\u008a\u00d8\u00ab \u00d8\u00b3\u00d8\u00aa\u00d8\u00b7\u00d8\u00a8\u00d9\u0082 \u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa\u00d9\u0083 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00ad\u00d9\u0082\u00d9\u008a\u00d9\u0082\u00d9\u008a\u00d8\u00a9. \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d8\u00aa\u00d9\u0082\u00d9\u0088\u00d9\u0085 \u00d8\u00a8\u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081 \u00d9\u0085\u00d8\u00b9\u00d9\u0082\u00d8\u00af\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00b1\u00d9\u0085\u00d8\u00ac\u00d9\u008a\u00d8\u00a9 \u00d9\u0082\u00d9\u0088\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0085\u00d8\u00ab\u00d9\u0084 Microsoft Excel\u00d8\u008c \u00d9\u0088Tableau\u00d8\u008c \u00d9\u0088MySQL. \u00d9\u0084\u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b2\u00d9\u008a\u00d8\u00af\u00d8\u008c \u00d8\u00b4\u00d8\u00a7\u00d9\u0087\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d9\u0088 \u00d9\u0088\u00d8\u00b1\u00d8\u00a7\u00d8\u00ac\u00d8\u00b9 \u00d9\u0088\u00d8\u00ab\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d8\u00b8\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5\u00d8\u008c \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u0088\u00d9\u0081\u00d8\u00b1\u00d9\u0086\u00d8\u00a7\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d9\u0083. \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0088\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5: \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d9\u0084\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00b9\u00d8\u00aa\u00d9\u0085\u00d8\u00af \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a3\u00d9\u0081\u00d8\u00b6\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0085\u00d8\u00a7\u00d8\u00b1\u00d8\u00b3\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00aa\u00d8\u00ac\u00d8\u00b9\u00d9\u0084 \u00d8\u00a3\u00d9\u008a \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00a7\u00d9\u0081\u00d8\u00b3 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a8\u00d8\u00ad \u00d8\u00a3\u00d9\u0083\u00d8\u00a8\u00d8\u00b1\u00d8\u008c \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00a3\u00d9\u0087\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d8\u00af\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u0083 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d9\u008a\u00d9\u0088\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d8\u00b6\u00d8\u00b7\u00d9\u0084\u00d8\u00b9 \u00d8\u00a8\u00d9\u0087\u00d8\u00a7 \u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u0088 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u0088 \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa\u00d8\u009b \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00a8\u00d8\u00b7 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00b7\u00d9\u0084\u00d8\u00a8 \u00d8\u00b9\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7. \u00d9\u0088\u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d8\u00a8\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b1\u00d8\u00ac\u00d8\u00b9\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 20 \u00d8\u00b9\u00d9\u0086\u00d8\u00b5\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085 \u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9\u00d9\u008a\u00d9\u0086\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085 \u00d8\u00a3\u00d9\u008a \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d8\u00a8\u00d9\u0086\u00d8\u00a7\u00d8\u00a1\u00d9\u008b \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d8\u00ad\u00d8\u00aa\u00d9\u0088\u00d8\u00a7\u00d8\u00a6\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00ab\u00d9\u0082\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9 \u00d8\u00a8\u00d8\u00b7\u00d8\u00b1\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d9\u0081\u00d8\u00b9\u00d8\u00a7\u00d9\u0084\u00d8\u00a9. \u00d8\u00aa\u00d9\u008f\u00d8\u00ad\u00d8\u00af\u00d8\u00ab \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 Amazon\u00d8\u008c \u00d9\u0088Uber\u00d8\u008c \u00d9\u0088Airbnb \u00d8\u00aa\u00d8\u00ad\u00d9\u0088\u00d9\u0084\u00d8\u00a7\u00d9\u008b \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00b5\u00d9\u0086\u00d8\u00a7\u00d8\u00b9\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0083\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d9\u0086 \u00d8\u00ae\u00d9\u0084\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d8\u00a7\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00a8\u00d8\u00af\u00d8\u00a7\u00d8\u00b9\u00d9\u008a \u00d9\u0084\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u0083 \u00d9\u0084\u00d9\u0090\u00d9\u0085\u00d9\u008e \u00d8\u00aa\u00d8\u00b9\u00d8\u00af \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0082\u00d9\u0084\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00ba\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d8\u00aa\u00d8\u00b3\u00d8\u00aa\u00d8\u00ae\u00d8\u00af\u00d9\u0085 \u00d8\u00aa\u00d9\u0082\u00d9\u0086\u00d9\u008a\u00d8\u00a7\u00d8\u00aa \u00d8\u00aa\u00d8\u00ad\u00d9\u0084\u00d9\u008a\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0084\u00d8\u00aa\u00d8\u00aa\u00d9\u0081\u00d9\u0088\u00d9\u0082 \u00d9\u0081\u00d9\u008a \u00d9\u0082\u00d8\u00af\u00d8\u00b1\u00d8\u00aa\u00d9\u0087\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0086\u00d8\u00a7\u00d9\u0081\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d8\u00a9. \u00d9\u0088\u00d9\u0082\u00d8\u00a8\u00d9\u0084 \u00d9\u0083\u00d9\u0084 \u00d8\u00b4\u00d9\u008a\u00d8\u00a1\u00d8\u008c \u00d9\u008a\u00d9\u008f\u00d8\u00b1\u00d8\u00ac\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d8\u00af\u00d8\u00a1 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d9\u0088 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b9\u00d9\u0086\u00d9\u0088\u00d9\u0086 \u00d8\u00a8\u00d9\u0080 \"\u00d9\u0086\u00d8\u00a8\u00d8\u00b0\u00d8\u00a9 \u00d8\u00b9\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5\". \u00d8\u00a2\u00d9\u0085\u00d9\u0084 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d8\u00b3\u00d8\u00aa\u00d9\u0085\u00d8\u00aa\u00d8\u00b9 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0088\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9! \u00d9\u0085\u00d8\u00b1\u00d8\u00ad\u00d8\u00a8\u00d9\u008b\u00d8\u00a7! \u00d8\u00b3\u00d9\u0086\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0081 \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d9\u0088\u00d9\u0087\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d8\u00ad\u00d8\u00b5\u00d8\u00a7\u00d8\u00a1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00b3\u00d8\u00a7\u00d8\u00b9\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u008a \u00d9\u0081\u00d9\u0087\u00d9\u0085 \u00d9\u0083\u00d9\u008a\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0081\u00d9\u008a \u00d8\u00ab\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a7\u00d8\u00b2\u00d8\u00af\u00d9\u0087\u00d8\u00a7\u00d8\u00b1. \u00d9\u008a\u00d9\u0088\u00d8\u00ac\u00d8\u00af \u00d8\u00af\u00d8\u00a7\u00d8\u00ae\u00d9\u0084 \u00d9\u0083\u00d9\u0084 \u00d9\u0085\u00d8\u00ac\u00d9\u0085\u00d9\u0088\u00d8\u00b9\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d9\u0085\u00d9\u0082\u00d9\u008a\u00d8\u00a7\u00d8\u00b3 \u00d8\u00ad\u00d9\u008a\u00d9\u0088\u00d9\u008a \u00d9\u0086\u00d8\u00ad\u00d8\u00a7\u00d9\u0088\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d9\u0091\u00d9\u0081 \u00d8\u00b9\u00d9\u0084\u00d9\u008a\u00d9\u0087! \u00d8\u00a8\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00a7\u00d9\u0084\u00d9\u008a: \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2 \u00d8\u00a8\u00d9\u008a\u00d9\u0086 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d9\u0088\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00a7\u00d8\u00af\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d9\u0085\u00d8\u00ab\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d9\u0081\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0082\u00d8\u00af\u00d9\u008a\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a8\u00d8\u00ad\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b3\u00d9\u0088\u00d9\u008a\u00d9\u0082 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ac\u00d8\u00b2\u00d8\u00a6\u00d8\u00a9 \u00d8\u00b9\u00d8\u00a8\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d9\u0086\u00d8\u00aa\u00d8\u00b1\u00d9\u0086\u00d8\u00aa\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2 \u00d8\u00a8\u00d9\u008a\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a5\u00d9\u008a\u00d8\u00b1\u00d8\u00a7\u00d8\u00af\u00d8\u00a7\u00d8\u00aa\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a8\u00d8\u00ad\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00b7\u00d8\u00b1\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2 \u00d8\u00a8\u00d9\u008a\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u008a\u00d9\u0086\u00d8\u00a7\u00d9\u0085\u00d9\u008a\u00d9\u0083\u00d9\u008a\u00d8\u00a9. \u00d9\u0088\u00d8\u00aa\u00d8\u00b4\u00d8\u00aa\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d9\u0088\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d9\u0081\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d9\u0086\u00d9\u0082\u00d8\u00af\u00d9\u008a \u00d9\u0088\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b1\u00d8\u00a8\u00d8\u00a7\u00d8\u00ad \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00b3\u00d8\u00a7\u00d8\u00a6\u00d8\u00b1 \u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d8\u00a9 \u00d8\u00ad\u00d8\u00a7\u00d9\u0084\u00d8\u00a9 Egger\u00e2\u0080\u0099s Roast Coffee\u00d8\u008c \u00d8\u00a8\u00d9\u0088\u00d8\u00b5\u00d9\u0081\u00d9\u0087\u00d8\u00a7 \u00d9\u0088\u00d8\u00ab\u00d9\u008a\u00d9\u0082\u00d8\u00a9 \u00d8\u00aa\u00d9\u0083\u00d9\u0085\u00d9\u008a\u00d9\u0084\u00d9\u008a\u00d8\u00a9. \u00d9\u0084\u00d8\u00b0\u00d8\u00a7 \u00d8\u00aa\u00d8\u00a3\u00d9\u0083\u00d8\u00af \u00d8\u00a3\u00d9\u0086\u00d9\u0083 \u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d8\u00ac\u00d8\u00b9\u00d9\u0087\u00d8\u00a7 \u00d8\u00a8\u00d8\u00b9\u00d9\u0086\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d9\u0088\u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d8\u00ac\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b3\u00d8\u00b1\u00d8\u00af \u00d9\u0084\u00d9\u0084\u00d8\u00ad\u00d8\u00b5\u00d9\u0088\u00d9\u0084 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d8\u00a6\u00d9\u008a\u00d8\u00b3\u00d9\u008a\u00d8\u00a9.  \u00d9\u0085\u00d8\u00b1\u00d8\u00ad\u00d8\u00a8\u00d9\u008b\u00d8\u00a7!  \u00d9\u0081\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d9\u0086\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u008a \u00d8\u00a8\u00d8\u00a8\u00d8\u00b9\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d8\u00b8\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d9\u0088\u00d9\u0087\u00d9\u0085 \u00d8\u00aa\u00d9\u0084\u00d8\u00a7\u00d9\u0085\u00d8\u00b0\u00d8\u00aa\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d8\u00a7\u00d8\u00a8\u00d9\u0082\u00d9\u0088\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a\u00d9\u0086 \u00d9\u008a\u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u0088\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a2\u00d9\u0086 \u00d9\u0081\u00d9\u008a \u00d9\u0088\u00d8\u00b8\u00d8\u00a7\u00d8\u00a6\u00d9\u0081 \u00d9\u0085\u00d8\u00a8\u00d9\u0087\u00d8\u00ac\u00d8\u00a9 \u00d9\u0088\u00d9\u0085\u00d8\u00ab\u00d9\u008a\u00d8\u00b1\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00a7\u00d9\u0087\u00d8\u00aa\u00d9\u0085\u00d8\u00a7\u00d9\u0085 \u00d9\u0084\u00d9\u0084\u00d8\u00ba\u00d8\u00a7\u00d9\u008a\u00d8\u00a9\u00d8\u008c \u00d9\u0085\u00d8\u00ab\u00d9\u0084: \u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d8\u00a3\u00d9\u0088 \u00d9\u0085\u00d8\u00ad\u00d9\u0084\u00d9\u0084\u00d9\u008a \u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d8\u008c \u00d8\u00a3\u00d9\u0088 \u00d8\u00b9\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d8\u00a1 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa. \u00d8\u00b3\u00d9\u0086\u00d9\u0083\u00d8\u00aa\u00d8\u00b4\u00d9\u0081 \u00d9\u0085\u00d8\u00a7 \u00d9\u008a\u00d9\u0082\u00d9\u0088\u00d9\u0085\u00d9\u0088\u00d9\u0086 \u00d8\u00a8\u00d9\u0087\u00d8\u008c \u00d9\u0088\u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0082 \u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00b1\u00d9\u0087\u00d9\u0085 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d8\u00a9 \u00d9\u0084\u00d8\u00aa\u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d9\u0087\u00d9\u0085! \u00d9\u0088\u00d8\u00a3\u00d9\u0085\u00d9\u0084\u00d9\u0086\u00d8\u00a7 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b9\u00d9\u0084\u00d9\u0088\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d9\u0081\u00d9\u0087\u00d9\u0085\u00d9\u008b\u00d8\u00a7 \u00d8\u00a3\u00d9\u0081\u00d8\u00b6\u00d9\u0084 \u00d9\u0084\u00d9\u0086\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0082\u00d8\u00a9 \u00d8\u00a8\u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d8\u00a5\u00d9\u0084\u00d9\u008a\u00d9\u0087\u00d8\u00a7 \u00d8\u00b9\u00d9\u0086\u00d8\u00af\u00d9\u0085\u00d8\u00a7 \u00d8\u00aa\u00d9\u0086\u00d8\u00aa\u00d9\u0087\u00d9\u008a \u00d9\u0085\u00d9\u0086 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ae\u00d8\u00b5\u00d8\u00b5\u00d8\u008c \u00d9\u0088\u00d8\u00a7\u00d8\u00b3\u00d8\u00aa\u00d9\u008a\u00d8\u00b9\u00d8\u00a7\u00d8\u00a8\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d9\u0086\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00b3\u00d8\u00aa\u00d8\u00ac\u00d8\u00af\u00d9\u0087\u00d8\u00a7 \u00d8\u00a3\u00d9\u0083\u00d8\u00ab\u00d8\u00b1 \u00d8\u00ac\u00d8\u00a7\u00d8\u00b0\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u0084\u00d8\u00b5\u00d8\u00a7\u00d9\u0084\u00d8\u00ad\u00d9\u0087\u00d8\u00a7. \u00d8\u00a8\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00a7\u00d9\u0084\u00d9\u008a:  \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u008a\u00d8\u00b2 \u00d8\u00a8\u00d9\u008a\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00af\u00d9\u0088\u00d8\u00a7\u00d8\u00b1 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d9\u0085 \u00d8\u00af\u00d8\u00a7\u00d8\u00ae\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d9\u008a\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0085\u00d9\u0084 \u00d9\u0083\u00d9\u0084 \u00d8\u00af\u00d9\u0088\u00d8\u00b1 \u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa\u00d8\u008c \u00d9\u0088\u00d9\u0088\u00d8\u00b5\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d8\u00a7\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b7\u00d9\u0084\u00d9\u0088\u00d8\u00a8\u00d8\u00a9 \u00d9\u0084\u00d8\u00aa\u00d8\u00a3\u00d8\u00af\u00d9\u008a\u00d8\u00a9 \u00d9\u0083\u00d9\u0084 \u00d8\u00af\u00d9\u0088\u00d8\u00b1 \u00d9\u0088\u00d8\u00b8\u00d9\u008a\u00d9\u0081\u00d9\u008a. \u00d8\u00b3\u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d9\u0091\u00d8\u00b2 \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d8\u00aa\u00d8\u00b1\u00d8\u00aa\u00d8\u00a8\u00d8\u00b7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00ab\u00d9\u0082\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00b5\u00d9\u0086\u00d9\u0081 \u00d8\u00a3\u00d9\u008a \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d8\u00ad\u00d8\u00b3\u00d8\u00a8 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b1\u00d8\u00ac\u00d8\u00b9\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 20 \u00d8\u00b9\u00d9\u0086\u00d8\u00b5\u00d8\u00b1\u00d9\u008b\u00d8\u00a7. \u00d9\u0088\u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d9\u0084\u00d9\u0085 \u00d8\u00a3\u00d9\u008a\u00d8\u00b6\u00d9\u008b\u00d8\u00a7 \u00d8\u00a3\u00d9\u0086 \u00d8\u00aa\u00d9\u0085\u00d9\u008a\u00d8\u00b2 \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d8\u00aa\u00d8\u00b1\u00d8\u00aa\u00d8\u00a8\u00d8\u00b7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00ae\u00d8\u00aa\u00d9\u0084\u00d9\u0081\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a8\u00d8\u00ab\u00d9\u0082\u00d8\u00a7\u00d9\u0081\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a8\u00d9\u008a\u00d8\u00a7\u00d9\u0086\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b6\u00d8\u00ae\u00d9\u0085\u00d8\u00a9. \u00d9\u0088\u00d8\u00aa\u00d8\u00b4\u00d8\u00aa\u00d9\u0085\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0088\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085\u00d8\u00a9 \u00d9\u0085\u00d8\u00b1\u00d8\u00ac\u00d8\u00b9\u00d9\u008a\u00d8\u00a9 \u00d9\u0085\u00d9\u0083\u00d9\u0088\u00d9\u0086\u00d8\u00a9 \u00d9\u0085\u00d9\u0086 20 \u00d8\u00b9\u00d9\u0086\u00d8\u00b5\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0084\u00d8\u00aa\u00d8\u00b5\u00d9\u0086\u00d9\u008a\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa. \u00d9\u0083\u00d9\u0085\u00d8\u00a7 \u00d9\u008a\u00d8\u00b4\u00d9\u0085\u00d9\u0084 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00a7\u00d9\u0082\u00d8\u00aa\u00d8\u00b1\u00d8\u00a7\u00d8\u00b9\u00d8\u00a7\u00d8\u00aa \u00d8\u00af\u00d8\u00a7\u00d8\u00ae\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d9\u0088 \u00d8\u00a8\u00d8\u00ad\u00d9\u008a\u00d8\u00ab \u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086\u00d9\u0083 \u00d9\u0085\u00d8\u00b9\u00d8\u00b1\u00d9\u0081\u00d8\u00a9 \u00d9\u0083\u00d9\u008a\u00d9\u0081 \u00d9\u008a\u00d9\u008f\u00d8\u00b5\u00d9\u0086\u00d9\u0081 \u00d8\u00a7\u00d9\u0084\u00d8\u00a2\u00d8\u00ae\u00d8\u00b1\u00d9\u0088\u00d9\u0086 \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa\u00d9\u0087\u00d9\u0085. \u00d9\u0085\u00d8\u00b1\u00d8\u00ad\u00d8\u00a8\u00d9\u008b\u00d8\u00a7! \u00d8\u00b3\u00d9\u0086\u00d8\u00aa\u00d8\u00b9\u00d9\u0085\u00d9\u0082 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00ba\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d8\u00b3\u00d9\u0088\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d9\u008a\u00d8\u008c \u00d9\u0088\u00d9\u0087\u00d9\u008a \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d9\u008a\u00d8\u00ad\u00d8\u00aa\u00d8\u00a7\u00d8\u00ac \u00d9\u0083\u00d9\u0084 \u00d9\u0086\u00d9\u0088\u00d8\u00b9 \u00d9\u0085\u00d9\u0086 \u00d8\u00a3\u00d9\u0086\u00d9\u0088\u00d8\u00a7\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a5\u00d9\u0084\u00d9\u0089 \u00d9\u0081\u00d9\u0087\u00d9\u0085\u00d9\u0087\u00d8\u00a7 \u00d9\u0084\u00d9\u0083\u00d9\u008a \u00d9\u008a\u00d8\u00aa\u00d8\u00b9\u00d8\u00a7\u00d9\u0081\u00d9\u0089. \u00d8\u00b3\u00d9\u0086\u00d8\u00aa\u00d8\u00b9\u00d9\u0085\u00d9\u0082 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b3\u00d9\u0088\u00d9\u008a\u00d9\u0082 \"\u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0088\u00d8\u00af\u00d9\u008a\" \u00d9\u0084\u00d9\u0084\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9\u00d8\u009b \u00d8\u00ad\u00d9\u008a\u00d8\u00ab \u00d8\u00aa\u00d8\u00aa\u00d8\u00b9\u00d8\u00b1\u00d8\u00b6 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00b1\u00d9\u0082\u00d9\u0085\u00d9\u008a\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00aa\u00d9\u0087\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d8\u00a8\u00d8\u00b3\u00d8\u00b1\u00d9\u0082\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b3\u00d9\u0088\u00d9\u0082 \u00d9\u0085\u00d9\u0086 \u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00ac\u00d8\u00a7\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u0082\u00d9\u0084\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d8\u00a9. \u00d9\u0088\u00d8\u00a8\u00d9\u0086\u00d9\u0087\u00d8\u00a7\u00d9\u008a\u00d8\u00a9 \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9\u00d8\u008c \u00d8\u00b3\u00d8\u00aa\u00d8\u00aa\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d9\u0085\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00a7\u00d9\u0084\u00d9\u008a: \u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d8\u00ac\u00d9\u0085\u00d9\u008a\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b4\u00d8\u00a7\u00d8\u00b1\u00d9\u0083\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00b3\u00d9\u0088\u00d9\u008a\u00d9\u0082 \u00d8\u00a7\u00d9\u0084\u00d9\u0082\u00d8\u00a7\u00d8\u00a6\u00d9\u0085 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d8\u00a7\u00d9\u0084\u00d8\u00b4\u00d8\u00a8\u00d9\u0083\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d9\u008a\u00d8\u00af \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b9\u00d9\u0085\u00d8\u00a7\u00d9\u0084 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d9\u0084\u00d9\u0084\u00d8\u00b4\u00d8\u00b1\u00d9\u0083\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d9\u008a \u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00ae\u00d8\u00af\u00d9\u0085\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00a7\u00d9\u0084\u00d9\u008a\u00d8\u00a9. \u00d8\u00b3\u00d8\u00aa\u00d8\u00ac\u00d8\u00af \u00d8\u00b1\u00d9\u0088\u00d8\u00a7\u00d8\u00a8\u00d8\u00b7 \u00d9\u0088\u00d9\u008a\u00d8\u00a8 \u00d8\u00a5\u00d8\u00b6\u00d8\u00a7\u00d9\u0081\u00d9\u008a\u00d8\u00a9 \u00d8\u00aa\u00d9\u0088\u00d8\u00b3\u00d8\u00b9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0088\u00d8\u00a7\u00d8\u00af \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d8\u00b1\u00d8\u00a7\u00d8\u00b3\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d8\u00b4\u00d9\u0085\u00d9\u0088\u00d9\u0084\u00d8\u00a9 \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00ad\u00d8\u00a7\u00d8\u00b6\u00d8\u00b1\u00d8\u00a7\u00d8\u00aa \u00d8\u00a7\u00d9\u0084\u00d9\u0081\u00d9\u008a\u00d8\u00af\u00d9\u008a\u00d9\u0088 \u00d9\u0084\u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9. \u00d9\u008a\u00d8\u00ad\u00d8\u00aa\u00d9\u0088\u00d9\u008a \u00d9\u0087\u00d8\u00b0\u00d8\u00a7 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00b3\u00d8\u00a8\u00d9\u0088\u00d8\u00b9 \u00d8\u00b9\u00d9\u0084\u00d9\u0089 \u00d9\u0085\u00d9\u0087\u00d9\u0085\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00af\u00d9\u0088\u00d8\u00b1\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00aa\u00d8\u00af\u00d8\u00b1\u00d9\u008a\u00d8\u00a8\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d8\u00ae\u00d9\u008a\u00d8\u00b1\u00d8\u00a9\u00d8\u008c \u00d9\u0088\u00d8\u00aa\u00d9\u0082\u00d9\u008a\u00d9\u008a\u00d9\u0085 \u00d8\u00a7\u00d9\u0084\u00d8\u00a3\u00d9\u0082\u00d8\u00b1\u00d8\u00a7\u00d9\u0086 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d8\u00b3\u00d8\u00aa\u00d8\u00ad\u00d8\u00af\u00d8\u00af \u00d9\u0081\u00d9\u008a\u00d9\u0087 \u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d8\u00a3\u00d8\u00ab\u00d8\u00a7\u00d8\u00b1\u00d8\u00aa \u00d8\u00a7\u00d9\u0087\u00d8\u00aa\u00d9\u0085\u00d8\u00a7\u00d9\u0085\u00d9\u0083 \u00d9\u0081\u00d9\u008a \u00d9\u0085\u00d8\u00ab\u00d8\u00a7\u00d9\u0084 \u00d9\u0084\u00d8\u00ad\u00d8\u00a7\u00d9\u0084\u00d8\u00a9 \u00d8\u00aa\u00d8\u00b5\u00d9\u0081 \u00d9\u0087\u00d8\u00b0\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0082\u00d8\u00a7\u00d9\u008a\u00d9\u008a\u00d8\u00b3 \u00d9\u0088\u00d8\u00aa\u00d9\u0082\u00d8\u00af\u00d9\u0085 \u00d8\u00aa\u00d8\u00ba\u00d9\u008a\u00d8\u00b1\u00d9\u008b\u00d8\u00a7 \u00d9\u0081\u00d9\u008a \u00d8\u00b9\u00d9\u0085\u00d9\u0084\u00d9\u008a\u00d8\u00a9 \u00d8\u00a7\u00d9\u0084\u00d8\u00b9\u00d9\u0085\u00d9\u0084 \u00d9\u008a\u00d9\u0085\u00d9\u0083\u00d9\u0086 \u00d8\u00a3\u00d9\u0086 \u00d9\u008a\u00d8\u00af\u00d8\u00b9\u00d9\u0085\u00d9\u0087 \u00d8\u00a7\u00d9\u0084\u00d9\u0085\u00d9\u0082\u00d9\u008a\u00d8\u00a7\u00d8\u00b3 \u00d8\u00a7\u00d9\u0084\u00d8\u00b0\u00d9\u008a \u00d8\u00aa\u00d9\u0085 \u00d8\u00a7\u00d8\u00ae\u00d8\u00aa\u00d9\u008a\u00d8\u00a7\u00d8\u00b1\u00d9\u0087.", "The Capstone project will allow you to continue to apply and refine the data analytic techniques learned from the previous courses in the Specialization to address an important issue in society. You will use real world data to complete a project with our industry and academic partners. For example, you can work with our industry partner, DRIVENDATA, to help them solve some of the world's biggest social challenges! DRIVENDATA at www.drivendata.org, is committed to bringing cutting-edge practices in data science and crowdsourcing to some of the world's biggest social challenges and the organizations taking them on. \n\nOr, you can work with our other industry partner, The Connection (www.theconnectioninc.org) to help them better understand recidivism risk for people on parole seeking substance use treatment. For more than 40 years, The Connection has been one of Connecticut\u00e2\u0080\u0099s leading private, nonprofit human service and community development agencies. Each month, thousands of people are assisted by The Connection\u00e2\u0080\u0099s diverse behavioral health, family support and community justice programs. The Connection\u00e2\u0080\u0099s Institute for Innovative Practice was created in 2010 to bridge the gap between researchers and practitioners in the behavioral health and criminal justice fields with the goal of developing maximally effective, evidence-based treatment programs. \n\nA major component of the Capstone project is for you to be able to choose the information from your analyses that best conveys results and implications, and to tell a compelling story with this information. By the end of the course, you will have a professional quality report of your findings that can be shown to colleagues and potential employers to demonstrate the skills you learned by completing the Specialization. Module 1. Identify Your Data and Research Question Module 2. Data Management Module 3. Exploratory Data Analysis Complete Your Final Report In this Module, your goal is to review the lectures and readings in the Overview of the Capstone Project, and 1) decide which data set you will use to complete your capstone project. In addition 2) identify your research question, 3) propose a title for your final report, and 4) complete Milestone Assignment 1 as described in the assignment. By the end of this Module you will have drafted a final report Title and Introduction to the Research Question. Your Introduction to the Research Question should include a statement of your research question, your motivation or rationale for testing the research question, and some potential implications of answering your research question. In this Module, your goals are to 1) complete the majority of your data management so that you are ready to begin your preliminary statistical analyses; and 2) complete Milestone Assignment 2 as described in the assignment. By the end of this Module you will have drafted a final report Methods section. Your Methods section should include a description of your sample, measures, and the analyses you plan to use to test your research question. In this Module, your goals are to 1) explore your data more extensively through descriptive and basic statistical analyses and data visualization; and 2) complete Milestone Assignment 3 as described in the Assignment. By the end of this module, you will have begun to draft your final report Results section, including some figures. In this Module, you 1) will complete your analyses; 2) finish writing your final report, and 3) submit your completed Final Report as the fourth and final assignment. A complete description of what is required for your final report and a detailed grading rubric can be found with the assignment; a sample final report is provided with the materials in the first module. ", "\u00c2\u00bfDesea saber c\u00c3\u00b3mo mejorar la precisi\u00c3\u00b3n de sus modelos de aprendizaje autom\u00c3\u00a1tico? \u00c2\u00bfCu\u00c3\u00a1l es la forma de saber qu\u00c3\u00a9 columnas de datos se prestan para las funciones m\u00c3\u00a1s \u00c3\u00batiles? Bienvenido a Feature Engineering en Google Cloud Platform, el curso en el que hablaremos de c\u00c3\u00b3mo reconocer buenas funciones, y c\u00c3\u00b3mo puede preprocesarlas y transformarlas para usarlas de forma \u00c3\u00b3ptima en sus modelos de aprendizaje autom\u00c3\u00a1tico.\n\nEn este curso, practicar\u00c3\u00a1 c\u00c3\u00b3mo elegir funciones y preprocesarlas en Google Cloud Platform mediante labs interactivos. Nuestros instructores lo guiar\u00c3\u00a1n por las soluciones de c\u00c3\u00b3digo, que tambi\u00c3\u00a9n se har\u00c3\u00a1n p\u00c3\u00bablicas para que las consulte mientras trabaja en sus propios proyectos de AA en el futuro. Introducci\u00c3\u00b3n Conversi\u00c3\u00b3n de datos sin procesar en funciones Preprocesamiento y creaci\u00c3\u00b3n de funciones Combinaciones de funciones TF\u00c2\u00a0Transform Resumen \u00c2\u00bfDesea saber c\u00c3\u00b3mo mejorar la precisi\u00c3\u00b3n de sus modelos de AA? \u00c2\u00bfCu\u00c3\u00a1l es la forma de saber qu\u00c3\u00a9 columnas de datos se prestan para las funciones m\u00c3\u00a1s \u00c3\u00batiles? Bienvenido a Feature Engineering, el curso en el que hablaremos de c\u00c3\u00b3mo reconocer buenas funciones, y c\u00c3\u00b3mo puede preprocesarlas y transformarlas para usarlas de forma \u00c3\u00b3ptima en sus modelos. La ingenier\u00c3\u00ada de funciones es a menudo la etapa m\u00c3\u00a1s larga y m\u00c3\u00a1s dif\u00c3\u00adcil en el desarrollo de un proyecto de AA. En el proceso de ingenier\u00c3\u00ada de funciones, comienza con los datos sin procesar y usa su propio conocimiento del dominio para crear funciones que hagan funcionar sus algoritmos de aprendizaje autom\u00c3\u00a1tico. En este m\u00c3\u00b3dulo, exploraremos qu\u00c3\u00a9 hace que una funci\u00c3\u00b3n sea buena y c\u00c3\u00b3mo representarla en su modelo de AA. En esta secci\u00c3\u00b3n del m\u00c3\u00b3dulo, se tratar\u00c3\u00a1n el preprocesamiento y la creaci\u00c3\u00b3n de funciones, que son t\u00c3\u00a9cnicas de procesamiento de datos que pueden ayudarlo a preparar un conjunto de funciones para un sistema de aprendizaje autom\u00c3\u00a1tico.\n En el aprendizaje autom\u00c3\u00a1tico tradicional, las combinaciones de funciones no juegan un papel muy importante, pero con los m\u00c3\u00a9todos actuales de AA, estas combinaciones son una herramienta muy valiosa. En este m\u00c3\u00b3dulo, aprender\u00c3\u00a1 a reconocer los tipos de problemas en los que las combinaciones de funciones representan una excelente manera de ayudar al aprendizaje autom\u00c3\u00a1tico. TensorFlow Transform (tf.Transform) es una biblioteca para preprocesar datos con TensorFlow. Es \u00c3\u00batil para el preprocesamiento que requiere un pasaje completo de los datos, como la normalizaci\u00c3\u00b3n de un valor de entrada seg\u00c3\u00ban la media y la desviaci\u00c3\u00b3n est\u00c3\u00a1ndar, la conversi\u00c3\u00b3n a entero de un vocabulario mediante un examen de todos los ejemplos de entradas para los valores, la organizaci\u00c3\u00b3n en dep\u00c3\u00b3sitos de las entradas seg\u00c3\u00ban la distribuci\u00c3\u00b3n de datos observada. En este m\u00c3\u00b3dulo, exploraremos los casos pr\u00c3\u00a1cticos para tf.Transform. Aqu\u00c3\u00ad resumimos los puntos m\u00c3\u00a1s importantes aprendidos en cada m\u00c3\u00b3dulo sobre la ingenier\u00c3\u00ada de funciones: C\u00c3\u00b3mo seleccionar buenas funciones, c\u00c3\u00b3mo preprocesar a gran escala, c\u00c3\u00b3mo usar combinaciones de funciones y c\u00c3\u00b3mo practicar con TensorFlow.", "Este curso de una semana, acelerado y a pedido se basa en Google Cloud Platform Big Data and Machine Learning Fundamentals. Mediante una combinaci\u00c3\u00b3n de clases por video, demonstraciones y labs pr\u00c3\u00a1cticos, aprender\u00c3\u00a1 a crear canalizaciones de datos de transmisi\u00c3\u00b3n usando Pub/Sub y Dataflow de Google Cloud a fin de permitir la toma de decisiones en tiempo real. Tambi\u00c3\u00a9n aprender\u00c3\u00a1 a crear paneles para presentar resultados personalizados para diversas audiencias de partes interesadas.\n\nRequisitos previos:\n\u00e2\u0080\u00a2 Google Cloud Platform Big Data and Machine Learning Fundamentals (o experiencia equivalente)\n\u00e2\u0080\u00a2 Cierto conocimiento sobre Java\n\nObjetivos:\n\u00e2\u0080\u00a2 Comprender los casos pr\u00c3\u00a1cticos de estad\u00c3\u00adsticas de transmisi\u00c3\u00b3n en tiempo real\n\u00e2\u0080\u00a2 Usar el servicio de mensajer\u00c3\u00ada as\u00c3\u00adncrona de PubSub de Google Cloud para administrar eventos de datos\n\u00e2\u0080\u00a2 Escribir canalizaciones de transmisi\u00c3\u00b3n y ejecutar transformaciones cuando sea necesario\n\u00e2\u0080\u00a2 Familiarizarse con ambos extremos de una canalizaci\u00c3\u00b3n de transmisi\u00c3\u00b3n: producci\u00c3\u00b3n y consumo\n\u00e2\u0080\u00a2 Interoperar Dataflow, BigQuery y Cloud Pub/Sub para lograr una transmisi\u00c3\u00b3n y un an\u00c3\u00a1lisis en tiempo real M\u00c3\u00b3dulo 1: Arquitectura de las canalizaciones de estad\u00c3\u00adsticas de transmisi\u00c3\u00b3n M\u00c3\u00b3dulo 2: C\u00c3\u00b3mo transferir vol\u00c3\u00bamenes variables M\u00c3\u00b3dulo 3: C\u00c3\u00b3mo implementar canalizaciones de transmisi\u00c3\u00b3n M\u00c3\u00b3dulo 4: Paneles y estad\u00c3\u00adsticas de transmisi\u00c3\u00b3n M\u00c3\u00b3dulo 5: C\u00c3\u00b3mo manejar los requisitos de capacidad de procesamiento y latencia     ", "Discutiremos por que hoje as redes neurais funcionam t\u00c3\u00a3o bem para lidar com v\u00c3\u00a1rios problemas, come\u00c3\u00a7ando pela hist\u00c3\u00b3ria do aprendizado de m\u00c3\u00a1quina. Em seguida, falaremos sobre como configurar um problema de aprendizado supervisionado e encontrar uma boa solu\u00c3\u00a7\u00c3\u00a3o com gradiente descendente. Isso envolve a cria\u00c3\u00a7\u00c3\u00a3o de conjuntos de dados que permitem a generaliza\u00c3\u00a7\u00c3\u00a3o. Esses m\u00c3\u00a9todos ser\u00c3\u00a3o abordados de maneira did\u00c3\u00a1tica para auxiliar na realiza\u00c3\u00a7\u00c3\u00a3o dos testes.\n\nObjetivos do curso:\nIdentificar por que o aprendizado profundo \u00c3\u00a9 mais usado hoje em dia\nOtimizar e avaliar modelos usando fun\u00c3\u00a7\u00c3\u00b5es de perda e m\u00c3\u00a9tricas de desempenho\nReduzir problemas comuns que surgem no aprendizado de m\u00c3\u00a1quina\nCriar conjuntos de dados de treinamento, avalia\u00c3\u00a7\u00c3\u00a3o e testes que podem ser repetidos e escalon\u00c3\u00a1veis Introdu\u00c3\u00a7\u00c3\u00a3o Aprendizado de m\u00c3\u00a1quina na pr\u00c3\u00a1tica Otimiza\u00c3\u00a7\u00c3\u00a3o Generaliza\u00c3\u00a7\u00c3\u00a3o e amostragem Resumo Neste curso, voc\u00c3\u00aa obter\u00c3\u00a1 conhecimento b\u00c3\u00a1sico sobre o aprendizado de m\u00c3\u00a1quina para entender a terminologia que usamos em toda a especializa\u00c3\u00a7\u00c3\u00a3o. Voc\u00c3\u00aa tamb\u00c3\u00a9m aprender\u00c3\u00a1 dicas pr\u00c3\u00a1ticas e armadilhas que os profissionais de aprendizado de m\u00c3\u00a1quina enfrentam aqui no Google e terminar\u00c3\u00a1 o curso com o c\u00c3\u00b3digo e o conhecimento necess\u00c3\u00a1rios para criar seus pr\u00c3\u00b3prios modelos de aprendizado de m\u00c3\u00a1quina. Neste m\u00c3\u00b3dulo, apresentaremos alguns dos principais tipos de aprendizado de m\u00c3\u00a1quina e revisaremos a hist\u00c3\u00b3ria dessa tecnologia at\u00c3\u00a9 a gera\u00c3\u00a7\u00c3\u00a3o mais recente. Com isso, voc\u00c3\u00aa poder\u00c3\u00a1 acelerar seu crescimento como praticante do aprendizado de m\u00c3\u00a1quina. Neste m\u00c3\u00b3dulo, mostraremos a voc\u00c3\u00aa como otimizar seus modelos de aprendizado de m\u00c3\u00a1quina. Agora chegou a hora de responder a uma pergunta um tanto estranha: em qual situa\u00c3\u00a7\u00c3\u00a3o \u00c3\u00a9 prefer\u00c3\u00advel n\u00c3\u00a3o escolher o modelo de aprendizado de m\u00c3\u00a1quina mais preciso? Como j\u00c3\u00a1 dissemos no m\u00c3\u00b3dulo anterior sobre otimiza\u00c3\u00a7\u00c3\u00a3o, o fato de um modelo apresentar uma m\u00c3\u00a9trica de perda igual a zero para seu conjunto de dados de treinamento n\u00c3\u00a3o significa que ele ter\u00c3\u00a1 um bom desempenho com novos dados em um caso real. ", "\u00e4\u00bd\u008e\u00e3\u0083\u00ac\u00e3\u0083\u0099\u00e3\u0083\u00ab\u00e3\u0081\u00ae TensorFlow \u00e3\u0082\u0092\u00e5\u00b0\u008e\u00e5\u0085\u00a5\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e5\u0088\u0086\u00e6\u0095\u00a3\u00e5\u009e\u008b\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0082\u00bb\u00e3\u0083\u0097\u00e3\u0083\u0088\u00e3\u0081\u00a8 API \u00e3\u0082\u0092\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082TensorFlow \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0082\u0092\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u00a2\u00e3\u0082\u00a6\u00e3\u0083\u0088\u00e3\u0081\u0097\u00e3\u0080\u0081Cloud Machine Learning Engine \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e3\u0081\u00a3\u00e3\u0081\u009f\u00e9\u00ab\u0098\u00e6\u0080\u00a7\u00e8\u0083\u00bd\u00e3\u0081\u00aa\u00e4\u00ba\u0088\u00e6\u00b8\u00ac\u00e3\u0082\u0092\u00e6\u008f\u0090\u00e4\u00be\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e7\u009b\u00ae\u00e7\u009a\u0084:\nTensorFlow \u00e3\u0081\u00a7\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\nTensorFlow \u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u0096\u00e3\u0083\u00a9\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e6\u0095\u00b0\u00e5\u0080\u00a4\u00e3\u0081\u00ae\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0082\u0092\u00e8\u00a7\u00a3\u00e6\u00b1\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\nTensorFlow \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0081\u008f\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e3\u0083\u0087\u00e3\u0083\u0090\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086\ntf.estimator \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6 ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0080\u0081\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0080\u0081\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0081\u0099\u00e3\u0082\u008b\nCloud ML Engine \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6 ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e5\u00a4\u00a7\u00e8\u00a6\u008f\u00e6\u00a8\u00a1\u00e3\u0081\u00aa\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00a4\u00e3\u0080\u0081\u00e6\u009c\u00ac\u00e7\u00a8\u00bc\u00e5\u0083\u008d\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086 \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e3\u0082\u00b3\u00e3\u0082\u00a2 TensorFlow Estimator API CMLE \u00e3\u0081\u00a7 TensorFlow \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0081\u00ae\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00af TensorFlow \u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u009d\u00e3\u0081\u00ae\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af TensorFlow \u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u0088\u009d\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0083\u0093\u00e3\u0082\u00b8\u00e3\u0083\u008d\u00e3\u0082\u00b9\u00e4\u00b8\u008a\u00e3\u0081\u00ae\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0082\u0092\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u00ae\u009a\u00e5\u00bc\u008f\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0080\u00812 \u00e3\u0081\u00a4\u00e7\u009b\u00ae\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u008c\u00e5\u00ae\u009f\u00e9\u009a\u009b\u00e3\u0081\u00ab\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0080\u0082\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e6\u00ba\u0096\u00e5\u0082\u0099\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u009f\u00e3\u0081\u00ae\u00e3\u0081\u00a7\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u00bf\u00e3\u0081\u00be\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0080\u0082 \"TensorFlow \u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0082\u00a2 \u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0083\u008d\u00e3\u0083\u00b3\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e6\u00bc\u0094\u00e7\u00bf\u0092\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e9\u0081\u0085\u00e5\u00bb\u00b6\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0081\u00a8\u00e5\u0091\u00bd\u00e4\u00bb\u00a4\u00e5\u009e\u008b\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e6\u00af\u0094\u00e8\u00bc\u0083\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u0095\u00e3\u0080\u0081\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0080\u0081\u00e5\u00a4\u0089\u00e6\u0095\u00b0\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e6\u009c\u0080\u00e7\u00b5\u0082\u00e7\u009a\u0084\u00e3\u0081\u00ab TensorFlow \u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b0\u00e3\u0083\u00a9\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e3\u0083\u0087\u00e3\u0083\u0090\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\" \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Estimator API \u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081TensorFlow \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e4\u00bd\u00bf\u00e3\u0081\u0084\u00e6\u0096\u00b9\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e3\u0083\u0087\u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00a4\u00e3\u0081\u00ab\u00e5\u0090\u0091\u00e3\u0081\u0091\u00e3\u0081\u00a6 GCP \u00e3\u0081\u00ae\u00e3\u0083\u009e\u00e3\u0083\u008d\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0083\u0089 \u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0083\u0095\u00e3\u0083\u00a9\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3\u00e3\u0081\u00a7 TensorFlow \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u009f TensorFlow \u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u0094\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a6\u0081\u00e7\u0082\u00b9\u00e3\u0082\u0092\u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0082\u00b3\u00e3\u0082\u00a2 TensorFlow \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0080\u0081Estimator API\u00e3\u0080\u0081Cloud Machine Learning Engine \u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e6\u008c\u00af\u00e3\u0082\u008a\u00e8\u00bf\u0094\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082", "A partir de una historia del aprendizaje autom\u00c3\u00a1tico, analizamos por qu\u00c3\u00a9 las redes neuronales, en la actualidad, ofrecen un alto rendimiento ante una variedad de problemas. Luego, analizaremos c\u00c3\u00b3mo configurar un problema de aprendizaje supervisado y encontrar una soluci\u00c3\u00b3n adecuada mediante el descenso de gradientes. Esto incluye crear conjuntos de datos que permitan la generalizaci\u00c3\u00b3n; hablaremos sobre los m\u00c3\u00a9todos para hacerlo de una manera repetible que admita la experimentaci\u00c3\u00b3n.\n\nObjetivos del curso:\nIdentificar por qu\u00c3\u00a9 el aprendizaje profundo es popular en la actualidad\nOptimizar y evaluar los modelos mediante las funciones de p\u00c3\u00a9rdida y las m\u00c3\u00a9tricas de rendimiento\nMitigar los problemas comunes que surgen en el aprendizaje autom\u00c3\u00a1tico\nCrear conjuntos de datos de entrenamiento, evaluaci\u00c3\u00b3n y prueba, repetibles y escalables Introducci\u00c3\u00b3n AA en la pr\u00c3\u00a1ctica Optimizaci\u00c3\u00b3n Generalizaci\u00c3\u00b3n y muestreo Resumen En este curso, obtendr\u00c3\u00a1 conocimientos fundamentales sobre el AA que le brindar\u00c3\u00a1n una comprensi\u00c3\u00b3n m\u00c3\u00a1s clara de la terminolog\u00c3\u00ada que usamos durante la especializaci\u00c3\u00b3n. Tambi\u00c3\u00a9n conocer\u00c3\u00a1 sugerencias pr\u00c3\u00a1cticas y resolver\u00c3\u00a1 problemas comunes de los especialistas en AA de Google, de manera que cuando termine el curso cuente con el c\u00c3\u00b3digo y el conocimiento suficientes para poner en marcha sus propios modelos de AA. En este m\u00c3\u00b3dulo, presentaremos algunos de los principales tipos de aprendizaje autom\u00c3\u00a1tico y repasaremos la historia del AA hasta la situaci\u00c3\u00b3n actual con el fin de acelerar su crecimiento como especialista en el tema. En este m\u00c3\u00b3dulo, explicaremos c\u00c3\u00b3mo optimizar sus modelos de AA. Ahora debemos hacernos una pregunta un tanto extra\u00c3\u00b1a: \u00c2\u00bfCu\u00c3\u00a1ndo es mejor no elegir el modelo de AA m\u00c3\u00a1s preciso? Como mencionamos en el m\u00c3\u00b3dulo anterior sobre optimizaci\u00c3\u00b3n, el hecho de que un modelo tenga una m\u00c3\u00a9trica de p\u00c3\u00a9rdida de 0 con el conjunto de datos de entrenamiento no implica necesariamente que d\u00c3\u00a9 buenos resultados con datos nuevos en un caso pr\u00c3\u00a1ctico real. ", "Ce cours \u00c3\u00a0 la demande s'appuie sur la formation Google Cloud Platform Big Data and Machine Learning Fundamentals. Il s'agit d'un cours acc\u00c3\u00a9l\u00c3\u00a9r\u00c3\u00a9, que vous pourrez effectuer en une semaine. Gr\u00c3\u00a2ce \u00c3\u00a0 une s\u00c3\u00a9rie de conf\u00c3\u00a9rences vid\u00c3\u00a9o, de d\u00c3\u00a9monstrations et d'ateliers, vous apprendrez \u00c3\u00a0 cr\u00c3\u00a9er des pipelines de flux de donn\u00c3\u00a9es \u00c3\u00a0 l'aide de Google Cloud Pub/Sub et de Dataflow. Objectif : faciliter la prise de d\u00c3\u00a9cisions en temps r\u00c3\u00a9el. Vous apprendrez \u00c3\u00a9galement \u00c3\u00a0 cr\u00c3\u00a9er des tableaux de bord pour pr\u00c3\u00a9senter des r\u00c3\u00a9sultats personnalis\u00c3\u00a9s \u00c3\u00a0 diff\u00c3\u00a9rents groupes d'intervenants.\n\n\nPr\u00c3\u00a9requis :\n\u00e2\u0080\u00a2 Avoir suivi la formation Google Cloud Platform Big Data and Machine Learning Fundamentals (ou disposer d'une exp\u00c3\u00a9rience \u00c3\u00a9quivalente)\n\u00e2\u0080\u00a2 Quelques connaissances en Java\n\nObjectifs :\n\u00e2\u0080\u00a2 Savoir quand utiliser l'analyse de flux en temps r\u00c3\u00a9el\n\u00e2\u0080\u00a2 G\u00c3\u00a9rer les \u00c3\u00a9v\u00c3\u00a9nements de donn\u00c3\u00a9es avec le service de messagerie asynchrone Google Cloud PubSub\n\u00e2\u0080\u00a2 R\u00c3\u00a9diger des pipelines de flux de donn\u00c3\u00a9es et effectuer des transformations le cas \u00c3\u00a9ch\u00c3\u00a9ant\n\u00e2\u0080\u00a2 Ma\u00c3\u00aetriser les deux facettes d'un pipeline de flux de donn\u00c3\u00a9es : production et consommation\n\u00e2\u0080\u00a2 Faire interagir Dataflow, BigQuery et Cloud Pub/Sub pour obtenir des analyses et des flux en temps r\u00c3\u00a9el Module\u00c2\u00a01\u00c2\u00a0: Architecture des pipelines d'analyse des flux de donn\u00c3\u00a9es Module\u00c2\u00a02\u00c2\u00a0: Ingestion de volumes variables Module\u00c2\u00a03\u00c2\u00a0: Mise en \u00c5\u0093uvre de pipelines de flux de donn\u00c3\u00a9es Module\u00c2\u00a04\u00c2\u00a0: Analyse de flux de donn\u00c3\u00a9es et tableaux de bord Module\u00c2\u00a05\u00c2\u00a0: R\u00c3\u00a9pondre aux exigences de d\u00c3\u00a9bit et de latence     ", "Ce cours pr\u00c3\u00a9sente l'approche TensorFlow de bas niveau et dresse la liste des concepts et API n\u00c3\u00a9cessaires pour la r\u00c3\u00a9daction de mod\u00c3\u00a8les de machine learning distribu\u00c3\u00a9s. Nous verrons comment appliquer une \u00c3\u00a9volutivit\u00c3\u00a9 horizontale \u00c3\u00a0 l'entra\u00c3\u00aenement d'un mod\u00c3\u00a8le TensorFlow afin d'offrir des pr\u00c3\u00a9dictions tr\u00c3\u00a8s pertinentes avec Cloud Machine Learning Engine.\n\nObjectifs du cours :\nCr\u00c3\u00a9er des mod\u00c3\u00a8les de machine learning dans TensorFlow\nUtiliser les biblioth\u00c3\u00a8ques TensorFlow pour r\u00c3\u00a9soudre des probl\u00c3\u00a8mes num\u00c3\u00a9riques\nR\u00c3\u00a9soudre les probl\u00c3\u00a8mes et d\u00c3\u00a9boguer les erreurs de code courantes sur TensorFlow\nUtiliser tf.estimator pour cr\u00c3\u00a9er, entra\u00c3\u00aener et \u00c3\u00a9valuer un mod\u00c3\u00a8le de ML\nEntra\u00c3\u00aener et d\u00c3\u00a9ployer les mod\u00c3\u00a8les de ML avant de les envoyer en production \u00c3\u00a0 grande \u00c3\u00a9chelle avec Cloud ML Engine Introduction Core\u00c2\u00a0TensorFlow API Estimator Effectuer le scaling des mod\u00c3\u00a8les TensorFlow avec CMLE R\u00c3\u00a9capitulatif Nous utiliserons l'outil TensorFlow pour r\u00c3\u00a9diger des programmes de machine learning. Par cons\u00c3\u00a9quent, ce cours comporte une pr\u00c3\u00a9sentation de TensorFlow. Lors du premier cours, vous avez appris \u00c3\u00a0 transposer les probl\u00c3\u00a8mes d'une entreprise en probl\u00c3\u00a8mes de machine learning. Dans le deuxi\u00c3\u00a8me cours, vous avez compris le fonctionnement pratique du machine learning, et appris \u00c3\u00a0 cr\u00c3\u00a9er des ensembles de donn\u00c3\u00a9es appliqu\u00c3\u00a9s au machine learning. Maintenant que vous avez toutes les donn\u00c3\u00a9es en main, vous \u00c3\u00aates pr\u00c3\u00aat \u00c3\u00a0 commencer la r\u00c3\u00a9daction de programmes de machine learning. Nous vous pr\u00c3\u00a9senterons les principaux composants de TensorFlow, et vous pourrez vous exercer, \u00c3\u00a0 travers des ateliers pratiques, \u00c3\u00a0 cr\u00c3\u00a9er des programmes de machine learning. Vous pourrez r\u00c3\u00a9diger une \u00c3\u00a9valuation paresseuse et des programmes imp\u00c3\u00a9ratifs, puis les comparer, travailler avec des graphes, des sessions et des variables, et enfin d\u00c3\u00a9boguer les programmes TensorFlow. Au cours de ce module, nous allons vous pr\u00c3\u00a9senter dans le d\u00c3\u00a9tail l'API Estimator. Nous allons voir comment transf\u00c3\u00a9rer et entra\u00c3\u00aener votre mod\u00c3\u00a8le TensorFlow sur l'infrastructure g\u00c3\u00a9r\u00c3\u00a9e de GCP d\u00c3\u00a9di\u00c3\u00a9e \u00c3\u00a0 l'entra\u00c3\u00aenement et au d\u00c3\u00a9ploiement de mod\u00c3\u00a8les de machine learning. Voici un r\u00c3\u00a9capitulatif des sujets TensorFlow que nous avons abord\u00c3\u00a9s dans ce cours. Nous examinerons \u00c3\u00a0 nouveau le code Core\u00c2\u00a0TensorFlow et l'API Estimator, et nous finirons par effectuer le scaling de vos mod\u00c3\u00a8les de machine learning avec Cloud Machine Learning\u00c2\u00a0Engine.", "Bienvenido al curso The Art and Science of Machine Learning. En este curso, adquirir\u00c3\u00a1 las competencias b\u00c3\u00a1sicas de intuici\u00c3\u00b3n de AA, evaluaci\u00c3\u00b3n de modelos de AA y experimentaci\u00c3\u00b3n con ellos para realizar los ajustes y optimizaciones correspondientes a fin de obtener el mejor rendimiento posible.   \n\nTambi\u00c3\u00a9n aprender\u00c3\u00a1 los mecanismos necesarios para entrenar modelos. Primero, los ajustar\u00c3\u00a1 manualmente para observar los efectos en el rendimiento. Una vez que se familiarice con los valores que puede cambiar, denominados hiperpar\u00c3\u00a1metros, aprender\u00c3\u00a1 a ajustarlos autom\u00c3\u00a1ticamente con Cloud Machine Learning Engine en Google Cloud Platform. Introducci\u00c3\u00b3n El arte del AA Ajuste de hiperpar\u00c3\u00a1metros Un poco de ciencia La ciencia de las redes neuronales Incorporaciones Estimador personalizado Resumen Descripci\u00c3\u00b3n general del curso, en la que se destacan los m\u00c3\u00b3dulos y objetivos clave. En primer lugar, analizaremos los aspectos del aprendizaje autom\u00c3\u00a1tico que requieren cierto grado de intuici\u00c3\u00b3n, criterios adecuados y experimentaci\u00c3\u00b3n. Esto es lo que llamamos \u00e2\u0080\u009cel arte del AA\u00e2\u0080\u009d. Tambi\u00c3\u00a9n aprender\u00c3\u00a1 los mecanismos necesarios para entrenar modelos. Los ajustar\u00c3\u00a1 manualmente para observar los efectos en el rendimiento. En este curso, aprender\u00c3\u00a1 sobre el arte del aprendizaje autom\u00c3\u00a1tico. Analizaremos los aspectos del aprendizaje autom\u00c3\u00a1tico que requieren cierto grado de intuici\u00c3\u00b3n, criterio y experimentaci\u00c3\u00b3n para encontrar el balance adecuado y obtener resultados satisfactorios (sepa de antemano que es imposible lograr la perfecci\u00c3\u00b3n). En este m\u00c3\u00b3dulo, aprender\u00c3\u00a1 a diferenciar los par\u00c3\u00a1metros de los hiperpar\u00c3\u00a1metros. M\u00c3\u00a1s adelante, analizaremos el enfoque de b\u00c3\u00basqueda por cuadr\u00c3\u00adculas tradicional y aprenderemos a pensar m\u00c3\u00a1s all\u00c3\u00a1 mediante la implementaci\u00c3\u00b3n de algoritmos m\u00c3\u00a1s inteligentes. Por \u00c3\u00baltimo, aprender\u00c3\u00a1 c\u00c3\u00b3mo Cloud\u00c2\u00a0ML\u00c2\u00a0Engine hace que el ajuste autom\u00c3\u00a1tico de los hiperpar\u00c3\u00a1metros sea tan conveniente. En este m\u00c3\u00b3dulo, agregaremos un poco de ciencia al arte del aprendizaje autom\u00c3\u00a1tico. En primer lugar, abordaremos c\u00c3\u00b3mo regularizar los modelos para lograr dispersi\u00c3\u00b3n, a fin de que sean m\u00c3\u00a1s simples y concisos. M\u00c3\u00a1s adelante, analizaremos la regresi\u00c3\u00b3n log\u00c3\u00adstica y aprenderemos a evaluar el rendimiento. En este m\u00c3\u00b3dulo, profundizaremos en los aspectos cient\u00c3\u00adficos del aprendizaje autom\u00c3\u00a1tico, espec\u00c3\u00adficamente en las redes neuronales. En este m\u00c3\u00b3dulo, aprender\u00c3\u00a1 a utilizar incorporaciones para administrar datos dispersos, de modo que los modelos de aprendizaje autom\u00c3\u00a1tico que usan este tipo de datos consuman menos memoria y puedan entrenarse m\u00c3\u00a1s r\u00c3\u00a1pido. Las incorporaciones tambi\u00c3\u00a9n son una forma de reducir la dimensionalidad y, de esta manera, simplificar los modelos para que sean m\u00c3\u00a1s generalizables. En este m\u00c3\u00b3dulo, iremos m\u00c3\u00a1s all\u00c3\u00a1 del uso de estimadores predise\u00c3\u00b1ados y escribiremos un estimador personalizado. Al escribir un estimador personalizado, obtendr\u00c3\u00a1 un mayor control de la funci\u00c3\u00b3n del modelo. Repaso de los conceptos clave abordados en el curso Art and Science of ML.", "\u00e3\u0080\u008cArt and Science of Machine Learning\u00e3\u0080\u008d\u00e3\u0081\u00b8\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u0093\u00e3\u0081\u009d\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e5\u00be\u00ae\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u00a8\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e9\u00ab\u0098\u00e3\u0081\u0084\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e5\u00be\u0097\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab\u00e4\u00b8\u008d\u00e5\u008f\u00af\u00e6\u00ac\u00a0\u00e3\u0081\u00aa\u00e7\u009b\u00b4\u00e6\u0084\u009f\u00e3\u0081\u00a8\u00e5\u0088\u00a4\u00e6\u0096\u00ad\u00e5\u008a\u009b\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e9\u00a8\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e7\u00bf\u0092\u00e5\u00be\u0097\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082  \n\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a7\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0083\u008e\u00e3\u0083\u0096\u00e3\u0082\u0084\u00e3\u0083\u00ac\u00e3\u0083\u0090\u00e3\u0083\u00bc\u00e3\u0080\u0081\u00e3\u0081\u0099\u00e3\u0081\u00aa\u00e3\u0082\u008f\u00e3\u0081\u00a1\u00e3\u0080\u008c\u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0080\u008d\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u0088\u009d\u00e3\u0081\u00ab\u00e3\u0081\u0093\u00e3\u0081\u0086\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e6\u0089\u008b\u00e5\u008b\u0095\u00e3\u0081\u00a7\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e4\u00b8\u008e\u00e3\u0081\u0088\u00e3\u0082\u008b\u00e5\u00bd\u00b1\u00e9\u009f\u00bf\u00e3\u0082\u0092\u00e8\u00a6\u00b3\u00e5\u00af\u009f\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e4\u00bd\u00bf\u00e3\u0081\u0084\u00e6\u0096\u00b9\u00e3\u0081\u00ab\u00e6\u0085\u00a3\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u008d\u00e3\u0081\u009f\u00e3\u0082\u0089\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00a7 Cloud Machine Learning Engine \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e8\u0087\u00aa\u00e5\u008b\u0095\u00e7\u009a\u0084\u00e3\u0081\u00ab\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u008a\u0080\u00e8\u00a1\u0093 \u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e8\u00aa\u00bf\u00e6\u0095\u00b4 \u00e7\u0090\u0086\u00e8\u00ab\u0096\u00e3\u0081\u00ae\u00e5\u00b0\u008e\u00e5\u0085\u00a5 \u00e3\u0083\u008b\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u00ab \u00e3\u0083\u008d\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u00af\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0081\u00ae\u00e7\u0090\u0086\u00e8\u00ab\u0096 \u00e5\u009f\u008b\u00e3\u0082\u0081\u00e8\u00be\u00bc\u00e3\u0081\u00bf \u00e3\u0082\u00ab\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0083\u00a0 \u00e3\u0082\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e4\u00b8\u00bb\u00e3\u0081\u00aa\u00e7\u009b\u00ae\u00e7\u009a\u0084\u00e3\u0081\u00a8\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u00a7\u008b\u00e6\u0088\u0090\u00e3\u0082\u0092\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u0088\u009d\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e7\u009b\u00b4\u00e6\u0084\u009f\u00e3\u0080\u0081\u00e5\u0088\u00a4\u00e6\u0096\u00ad\u00e5\u008a\u009b\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e9\u00a8\u0093\u00e3\u0082\u0092\u00e8\u00a6\u0081\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e5\u0081\u00b4\u00e9\u009d\u00a2\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0082\u0092\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u008a\u0080\u00e8\u00a1\u0093\u00e3\u0081\u00a8\u00e5\u0091\u00bc\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a7\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0083\u008e\u00e3\u0083\u0096\u00e3\u0082\u0084\u00e3\u0083\u00ac\u00e3\u0083\u0090\u00e3\u0083\u00bc\u00e3\u0080\u0081\u00e3\u0081\u0099\u00e3\u0081\u00aa\u00e3\u0082\u008f\u00e3\u0081\u00a1\u00e3\u0080\u008c\u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0080\u008d\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0082\u0089\u00e3\u0082\u0092\u00e6\u0089\u008b\u00e5\u008b\u0095\u00e3\u0081\u00a7\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e4\u00b8\u008e\u00e3\u0081\u0088\u00e3\u0082\u008b\u00e5\u00bd\u00b1\u00e9\u009f\u00bf\u00e3\u0082\u0092\u00e8\u00a6\u00b3\u00e5\u00af\u009f\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u008a\u0080\u00e8\u00a1\u0093\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e9\u0081\u00a9\u00e5\u0088\u0087\u00e3\u0081\u00aa\u00e3\u0083\u0090\u00e3\u0083\u00a9\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0080\u0081\u00e5\u00ae\u008c\u00e7\u0092\u00a7\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00af\u00e3\u0081\u00aa\u00e3\u0081\u0084\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e7\u00a8\u008b\u00e8\u0089\u00af\u00e3\u0081\u0084\u00e7\u008a\u00b6\u00e6\u0085\u008b\u00e3\u0082\u0092\u00e8\u00a6\u008b\u00e6\u00a5\u00b5\u00e3\u0082\u0081\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e7\u009b\u00b4\u00e6\u0084\u009f\u00e3\u0080\u0081\u00e5\u0088\u00a4\u00e6\u0096\u00ad\u00e5\u008a\u009b\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e9\u00a8\u0093\u00e3\u0081\u008c\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e6\u00a7\u0098\u00e3\u0080\u0085\u00e3\u0081\u00aa\u00e5\u0081\u00b4\u00e9\u009d\u00a2\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u00be\u00e3\u0081\u009a\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00a8\u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e9\u0081\u0095\u00e3\u0081\u0084\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ac\u00a1\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e5\u00be\u0093\u00e6\u009d\u00a5\u00e3\u0081\u00ae\u00e3\u0082\u00b0\u00e3\u0083\u00aa\u00e3\u0083\u0083\u00e3\u0083\u0089\u00e3\u0082\u00b5\u00e3\u0083\u00bc\u00e3\u0083\u0081\u00e6\u0089\u008b\u00e6\u00b3\u0095\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0082\u008a\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e3\u0082\u00a2\u00e3\u0083\u00ab\u00e3\u0082\u00b4\u00e3\u0083\u00aa\u00e3\u0082\u00ba\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00b0\u00e3\u0083\u00aa\u00e3\u0083\u0083\u00e3\u0083\u0089\u00e3\u0082\u00b5\u00e3\u0083\u00bc\u00e3\u0083\u0081\u00e3\u0081\u00ae\u00e6\u009e\u00a0\u00e3\u0082\u0092\u00e8\u00b6\u0085\u00e3\u0081\u0088\u00e3\u0081\u009f\u00e8\u0080\u0083\u00e3\u0081\u0088\u00e6\u0096\u00b9\u00e3\u0082\u0092\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e5\u0085\u00a5\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u00be\u008c\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0083\u008f\u00e3\u0082\u00a4\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0083\u0091\u00e3\u0083\u00a9\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e8\u00aa\u00bf\u00e6\u0095\u00b4\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a4 Cloud Machine Learning Engine \u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u008a\u0080\u00e8\u00a1\u0093\u00e3\u0081\u00ab\u00e5\u008a\u00a0\u00e3\u0081\u0088\u00e3\u0080\u0081\u00e7\u0090\u0086\u00e8\u00ab\u0096\u00e3\u0081\u00ae\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u0082\u00e9\u0096\u008b\u00e5\u00a7\u008b\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u0088\u009d\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0082\u008a\u00e3\u0082\u00b7\u00e3\u0083\u00b3\u00e3\u0083\u0097\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e7\u00b0\u00a1\u00e6\u00bd\u0094\u00e3\u0081\u00aa\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e7\u008f\u00be\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0080\u0081\u00e3\u0082\u00b9\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e6\u0080\u00a7\u00e3\u0082\u0092\u00e9\u00ab\u0098\u00e3\u0082\u0081\u00e3\u0082\u008b\u00e6\u00ad\u00a3\u00e5\u0089\u0087\u00e5\u008c\u0096\u00e3\u0081\u00ae\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00ac\u00a1\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e5\u009b\u009e\u00e5\u00b8\u00b0\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e5\u0088\u00a4\u00e5\u00ae\u009a\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e7\u0090\u0086\u00e8\u00ab\u0096\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0082\u0092\u00e6\u00b7\u00b1\u00e3\u0082\u0081\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e3\u0081\u00ab\u00e3\u0083\u008b\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u00ab \u00e3\u0083\u008d\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u00af\u00e3\u0083\u00bc\u00e3\u0082\u00af\u00e3\u0081\u00ae\u00e7\u0090\u0086\u00e8\u00ab\u0096\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0082\u00b9\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00aa\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u00b6\u0088\u00e8\u00b2\u00bb\u00e3\u0083\u00a1\u00e3\u0083\u00a2\u00e3\u0083\u00aa\u00e9\u0087\u008f\u00e3\u0082\u0092\u00e6\u008a\u0091\u00e3\u0081\u0088\u00e3\u0081\u00a6\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e6\u0099\u0082\u00e9\u0096\u0093\u00e3\u0082\u0092\u00e7\u009f\u00ad\u00e7\u00b8\u00ae\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e5\u009f\u008b\u00e3\u0082\u0081\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00b9\u00e3\u0083\u0091\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00aa\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e5\u009f\u008b\u00e3\u0082\u0081\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00ac\u00a1\u00e5\u0085\u0083\u00e5\u0089\u008a\u00e6\u00b8\u009b\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0080\u0081\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e3\u0082\u0088\u00e3\u0082\u008a\u00e3\u0082\u00b7\u00e3\u0083\u00b3\u00e3\u0083\u0097\u00e3\u0083\u00ab\u00e3\u0081\u00ab\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e6\u00b1\u008e\u00e5\u008c\u0096\u00e6\u0080\u00a7\u00e8\u0083\u00bd\u00e3\u0082\u0092\u00e9\u00ab\u0098\u00e3\u0082\u0081\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u0097\u00a2\u00e8\u00a3\u00bd\u00e3\u0081\u00ae\u00e3\u0082\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00ae\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0081\u00aa\u00e3\u0081\u008f\u00e3\u0080\u0081\u00e3\u0082\u00ab\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0083\u00a0\u00e3\u0081\u00ae\u00e3\u0082\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0082\u00ab\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0083\u00a0 \u00e3\u0082\u00a8\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00a1\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e8\u00a8\u0098\u00e8\u00bf\u00b0\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e9\u0096\u00a2\u00e6\u0095\u00b0\u00e8\u0087\u00aa\u00e4\u00bd\u0093\u00e3\u0082\u0092\u00e3\u0082\u0088\u00e3\u0082\u008a\u00e7\u00b4\u00b0\u00e3\u0081\u008b\u00e3\u0081\u008f\u00e5\u0088\u00b6\u00e5\u00be\u00a1\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0080\u008cArt and Science of Machine Learning\u00e3\u0080\u008d\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e4\u00b8\u008a\u00e3\u0081\u0092\u00e3\u0081\u009f\u00e4\u00b8\u00bb\u00e3\u0081\u00aa\u00e5\u0086\u0085\u00e5\u00ae\u00b9\u00e3\u0082\u0092\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082", "\u00e2\u0080\u009cBy the end of this Course, you will be able to\u00e2\u0080\u00a6\u00e2\u0080\u009d \n\u00e2\u0080\u00a2\tEvaluate effective leadership styles for leadership in nursing informatics in clinical or academic contexts to improve leadership success.\n\u00e2\u0080\u00a2\tDiscover core values that support effective nursing informatics leadership in academic and clinical contexts to inform development of a personal leadership mission statement.\n\u00e2\u0080\u00a2\tDiscover competing values and polarities related to knowledge leadership and management to promote successful leadership collaboration.\n\u00e2\u0080\u00a2\tDetermine your personal informatics leadership style based on results from the Minnesota Informatics Leadership Inventory to inform successful leadership practice. \n\u00e2\u0080\u00a2\tDiscuss the value and importance of foresight leadership in nursing informatics to anticipate trends and consequences that are likely to transform the learning health care system Evaluate effective leadership styles for leadership in nursing informatics in clinical or academic contexts to improve your leadership success Discover core values that support effective nursing informatics leadership in academic and clinical contexts to inform development of a personal leadership mission statement, and development plan Discover competing values and polarities related to knowledge leadership and management to promote successful leadership collaboration Reflect on results from the Minnesota Nursing Informatics Leadership Inventory (MNLI) to inform successful leadership development and practice Discuss the value and importance of foresight leadership in nursing informatics to anticipate trends and consequences that are likely to transform the learning health care system Overview: In this module we explore the knowledge complexity archetype and its relationship to leadership. Leadership Scholar Robert Dilts suggests effective leadership involves a mixture of several types of leadership ability. Self-skills have to do with how the leader deploys themselves in situations. Relational skills have to do with the ability to understand, communicate and motivate other people. Strategic thinking skills are necessary to define a desired state with specific goals and objectives.  Finally, systemic thinking skills are used to define not only the problem space of a situation or challenge but also the desired state and how to organize the teams and people in the system to achieve that desired state. Leadership requires mastery of self, communication, relationships and mastery of the system.  In this module several leadership styles are introduced, and you are invited to reflect on the ones that you most appreciate. Leaders must navigate and negotiate different levels of perspective as they turn visions into action. The Dilts Logical Levels of Leading and Learning conceptual model is a useful leadership framework to structure your thinking about leadership challenges. The model suggests the best leaders align environments, behaviors, capabilities, values and beliefs, identity, mission and visions to create a world to which people want to belong.  Remember from the last module one of the logical levels of learning and leading was related to values and beliefs. One\u00e2\u0080\u0099s values and beliefs support one\u00e2\u0080\u0099s identity, purpose and mission and provide motivations to lead. Leadership scholar Richard Barrett has developed an evolutionary leadership model built on values and suggests becoming conscious of one\u00e2\u0080\u0099s values supports personal and professional development as people change and grow evolving from self-interest, to personal transformation and beyond to serve the common good. Values are linked with needs and motivations. As one\u00e2\u0080\u0099s needs are satisfied one evolves in terms of awareness, development, and focus.  In this module you will have an opportunity to learn about Barrett\u00e2\u0080\u0099s seven levels model and complete a personal values assessment (PVA). You will be invited to reflect on how your personal values influence your leadership style. You will have the change to contemplate what values you want to strengthen and develop.  Gaining insight into your needs and motivations will help you craft intentional leadership development plans. Using your values to create a personal leadership mission statement will help clarify your contributions to a group or organization and provide direction for professional development in the future.  Recall from earlier modules, leadership is about alignment of vision, mission, purpose, people and processes. Successful organizations and leaders find ways to manage competing values and polarities. Some people in the organization like to create, others compete, some prefer control and some prefer collaboration. Managing competing values promotes creativity and innovation as well as the positive aspects of control and competition. Preferences and perspectives associated with these competing values create polarity dynamics.  Polarities are interdependent pairs of values or alternative points of view that seem contrary, yet need each other over time to achieved desired outcomes. Uncovering and managing the missing logic in a polarity dynamic which results from a clash of competing values is an essential informatics leadership skill. Developing an awareness and skills in polarity management will contribute to your leadership success and give you tools to enhance your flexibility and requisite leadership behaviors. The law of requisite variety suggests the agent with the most flexibility in a system will control the system. Mastering competing values and polarity management will support the development of your requisite variety leadership practices.  In this module you can review and reflect on your Minnesota Nursing Informatics Leadership Inventory (MNLI) results.  You will also hear nursing informatics leaders speak to the requisite variety of leadership practices that support leadership success. How will the advice they share influence your thinking, feeling, and commitments to leadership action? With insights gained, you will have the chance to revise your personal leadership mission statement. What values guide your work? How does the way you lead influence the systems of care where you work? How do you communicate your leadership style to your peers and colleagues? How do your reflections influence in your thoughts, feelings and future plans for action? How will you develop the requisite leadership behaviors and practices to support your success?    Nursing foresight is the ability and act of forecasting what will be needed in the future in light of emergent health care trends that have consequences for population and planetary health, as well as the profession\u00e2\u0080\u0099s purpose, definition, professional scope, and standards of practice. Foresight leadership in nursing requires the development of future literacy skills. Futures literacy invites people to create and share stories about the future to inform current practice and realities. Nurses who bridge innovations across contexts must become time-conscious, future literate, and enact requisite variety leadership practices.  This requires insight about self and others\u00e2\u0080\u0099 orientations toward time, appreciation for the value of innovation and design thinking and attention to active monitoring of industry trends, forecasts and disruptions. Foresight leadership is a function of discerning logical consequences of trends and developing vision based scenarios using futures thinking tools, techniques and methods. Nursing informatics leaders must stimulate strategic conversations about espoused visions looking backwards from the future. Foresight leadership helps people and organizations anticipate and create the future rather than react to emerging futures. Nursing informatics leaders are in a position to create a legacy and position organizations for success, through intentional use of foresight leadership knowledge, principles, practices and strategies. ", "Google Cloud \u00e3\u0083\u0081\u00e3\u0083\u00bc\u00e3\u0083\u00a0\u00e3\u0081\u008c\u00e6\u008f\u0090\u00e4\u00be\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b Coursera \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0080\u0081\u00e3\u0080\u008cFrom Data to Insights with Google Cloud Platform\u00e3\u0080\u008d\u00e3\u0081\u00b8\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u0093\u00e3\u0081\u009d\u00e3\u0080\u0082\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0082\u00a2\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u00ae Evan Jones \u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e3\u0082\u00ac\u00e3\u0082\u00a4\u00e3\u0083\u0089\u00e3\u0082\u0092\u00e5\u008b\u0099\u00e3\u0082\u0081\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e6\u009c\u00ac\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e6\u009c\u0080\u00e5\u0088\u009d\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0080\u008cExploring and Preparing your Data with BigQuery\u00e3\u0080\u008d\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0082\u00a2\u00e3\u0083\u008a\u00e3\u0083\u00aa\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u008c\u00e5\u0085\u00b1\u00e9\u0080\u009a\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e7\u009b\u00b4\u00e9\u009d\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e8\u00aa\u00b2\u00e9\u00a1\u008c\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0081\u009d\u00e3\u0081\u00ae\u00e8\u00aa\u00b2\u00e9\u00a1\u008c\u00e3\u0082\u0092 Google Cloud Platform \u00e3\u0081\u00ae\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e8\u00a7\u00a3\u00e6\u00b1\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e4\u00b8\u008a\u00e3\u0081\u0092\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u009d\u00e3\u0081\u00ae\u00e9\u0081\u008e\u00e7\u00a8\u008b\u00e3\u0081\u00a7 SQL \u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00aa\u00e3\u0081\u008c\u00e3\u0082\u0089\u00e3\u0080\u0081BigQuery \u00e3\u0081\u00a8 Cloud Dataprep \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0082\u0092\u00e6\u00b7\u00b1\u00e3\u0082\u0081\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e5\u00ae\u008c\u00e4\u00ba\u0086\u00e3\u0081\u00be\u00e3\u0081\u00a7\u00e3\u0081\u00ab\u00e7\u00b4\u0084 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0080\u0081\u00e5\u0090\u0088\u00e8\u00a8\u0088 5\u00ef\u00bd\u009e7 \u00e6\u0099\u0082\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e4\u00bd\u009c\u00e6\u00a5\u00ad\u00e6\u0099\u0082\u00e9\u0096\u0093\u00e3\u0082\u0092\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e4\u00bf\u00ae\u00e4\u00ba\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081BigQuery \u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e5\u0085\u00ac\u00e9\u0096\u008b\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e5\u0086\u0085\u00e3\u0081\u00ae\u00e4\u00bd\u0095\u00e7\u0099\u00be\u00e4\u00b8\u0087\u00e3\u0082\u0082\u00e3\u0081\u00ae\u00e3\u0083\u00ac\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00ab\u00e5\u00af\u00be\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0080\u0081\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e6\u0083\u0085\u00e5\u00a0\u00b1\u00e3\u0082\u0092\u00e5\u00bc\u0095\u00e3\u0081\u008d\u00e5\u0087\u00ba\u00e3\u0081\u009b\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e5\u0093\u0081\u00e8\u00b3\u00aa\u00e3\u0082\u0092\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0081\u0097\u00e3\u0080\u0081BigQuery \u00e3\u0081\u00ab\u00e5\u0087\u00ba\u00e5\u008a\u009b\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e8\u0087\u00aa\u00e5\u008b\u0095\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0082\u00af\u00e3\u0083\u00ac\u00e3\u0083\u00b3\u00e3\u0082\u00b8\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0083\u0091\u00e3\u0082\u00a4\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u0092\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0095\u00e3\u0082\u0089\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e5\u00ae\u009f\u00e9\u009a\u009b\u00e3\u0081\u00ae Google \u00e3\u0082\u00a2\u00e3\u0083\u008a\u00e3\u0083\u00aa\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0082\u00af\u00e3\u0082\u00b9\u00e3\u0081\u00ae e \u00e3\u0082\u00b3\u00e3\u0083\u009e\u00e3\u0083\u00bc\u00e3\u0082\u00b9 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00a7\u00e3\u0080\u0081\u00e3\u0083\u009e\u00e3\u0083\u00bc\u00e3\u0082\u00b1\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e6\u0083\u0085\u00e5\u00a0\u00b1\u00e3\u0082\u0092\u00e5\u00be\u0097\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae SQL \u00e3\u0081\u00ae\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u00a8\u00e3\u0083\u0088\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00ae\u00e6\u00bc\u0094\u00e7\u00bf\u0092\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n >>> \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ab\u00e7\u0099\u00bb\u00e9\u008c\u00b2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0081\u008f\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e8\u00b3\u00aa\u00e5\u0095\u008f\u00e3\u0081\u00ab\u00e8\u00a8\u0098\u00e8\u00bc\u0089\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0081\u008a\u00e3\u0082\u008a Qwiklabs \u00e3\u0081\u00ae\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e8\u00a6\u008f\u00e7\u00b4\u0084\u00e3\u0081\u00ab\u00e5\u0090\u008c\u00e6\u0084\u008f\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00a9\u00b3\u00e7\u00b4\u00b0\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0081\u00af\u00e3\u0080\u0081https://qwiklabs.com/terms_of_service \u00e3\u0082\u0092\u00e3\u0081\u0094\u00e8\u00a6\u00a7\u00e3\u0081\u008f\u00e3\u0081\u00a0\u00e3\u0081\u0095\u00e3\u0081\u0084\u00e3\u0080\u0082<<< \u00e3\u0080\u008cFrom Data to Insights with Google Cloud Platform: Exploring and Preparing your Data\u00e3\u0080\u008d\u00e3\u0081\u00b8\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u0093\u00e3\u0081\u009d\u00e3\u0080\u0082 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 1: Google Cloud Platform \u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 2: \u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 3: SQL \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e6\u008e\u00a2\u00e7\u00b4\u00a2 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 4: Google BigQuery \u00e3\u0081\u00ae\u00e6\u0096\u0099\u00e9\u0087\u0091\u00e4\u00bd\u0093\u00e7\u00b3\u00bb \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab 5: \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e3\u0082\u00af\u00e3\u0083\u00aa\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a8\u00e5\u00a4\u0089\u00e6\u008f\u009b \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0082\u00a2\u00e3\u0083\u008a\u00e3\u0083\u00aa\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ab\u00e5\u0090\u00ab\u00e3\u0081\u00be\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0080\u0081\u00e3\u0082\u00b3\u00e3\u0083\u00b3\u00e3\u0083\u0086\u00e3\u0083\u00b3\u00e3\u0083\u0084\u00e3\u0080\u0081\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u008e\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0083\u00bc\u00e3\u0082\u0092\u00e7\u00a4\u00ba\u00e3\u0081\u0099 Google Cloud Platform \u00e3\u0081\u00ae\u00e8\u0083\u008c\u00e5\u00be\u008c\u00e3\u0081\u00ab\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e5\u009f\u00ba\u00e6\u009c\u00ac\u00e5\u008e\u009f\u00e5\u0089\u0087\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0081\u009d\u00e3\u0082\u008c\u00e3\u0082\u0092\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ab\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0080\u0081\u00e6\u00ba\u0096\u00e5\u0082\u0099\u00e3\u0080\u0081\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0081\u00ae\u00e4\u00b8\u00bb\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e3\u0083\u0093\u00e3\u0083\u0083\u00e3\u0082\u00b0\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e5\u009f\u00ba\u00e6\u009c\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa SQL\u00ef\u00bc\u0088\u00e6\u00a7\u008b\u00e9\u0080\u00a0\u00e5\u008c\u0096\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e8\u00a8\u0080\u00e8\u00aa\u009e\u00ef\u00bc\u0089\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0080\u0081BigQuery \u00e3\u0081\u00a7\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e6\u009b\u00b8\u00e3\u0081\u008d\u00e8\u00be\u00bc\u00e3\u0082\u0080\u00e6\u00bc\u0094\u00e7\u00bf\u0092\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086 BigQuery \u00e3\u0081\u00ae\u00e6\u0096\u0099\u00e9\u0087\u0091\u00e4\u00bd\u0093\u00e7\u00b3\u00bb\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e9\u00ab\u0098\u00e5\u0093\u0081\u00e8\u00b3\u00aa\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e6\u0080\u00a7\u00e3\u0082\u0092\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a4\u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0099\u00e3\u0082\u008b", "This course is intended as a first step for learners who seek to become producers of social science research. It is organized as an introduction to the design and execution of a research study. It introduces the key elements of a proposal for a research study, and explains the role of each. It reviews the major types of qualitative and quantitative data used in social science research, and then introduces some of the most important sources of existing data available freely or by application, worldwide and for China. The course offers an overview of basic principles in the design of surveys, including a brief introduction to sampling. Basic techniques for quantitative analysis are also introduced, along with a review of common challenges that arise in the interpretation of results. Professional and ethical issues that often arise in the conduct of research are also discussed.  The course concludes with an introduction to the options for further study available to the interested student, and an overview of the key steps involved in selecting postgraduate programs and applying for admission. Learners who complete the course will be able to make an informed decision about whether to pursue advanced studies, and should be adequately prepared to write an application for postgraduate study that exhibits basic understanding of key aspects of social science research paradigms and methodologies.\n\nExplore the big questions in social science and learn how you can be a producer of social science research. \n\nCourse Overview video: https://youtu.be/QuMOAlwhpvU\n\nPart 1 should be completed before taking this course:  https://www.coursera.org/learn/social-science-study-chinese-society Designing a Study Evidence Sampling Public Data for China Quantitative Analysis Research and Professional Ethics Where to go from here Final exam Welcome to Social Science Approaches to the Study of Chinese Society Part 2! Part 2 focuses on being a PRODUCER of Social Science Research.  Take some time to review the course overview, assignments for this course and say hello in the discussion forum.   Week 2 will discuss the kind sources social scientists use for research. By the end of this week, you should be able to identify some of these major sources and perhaps pinpoint some sources that can be used in your own study. By the end of Week 3, you should be able to understand why RANDOM SAMPLING is important in a survey, outline the most common approaches to sampling and discuss key considerations when choosing a sampling strategy for your study. Week 4 discusses major sources of public data available to you.  By the end of this week you should be able to describe the opportunities as well as the challenges associated with using publicly available survey data.  Week 5 will give you a taste of the basic methods for quantitative analysis.  From there you should be able to identify key issues when interpreting results and discuss implications for research. By the end of this week you should be able to describe major ethical and professional concerns in social science research. Welcome to the last week of Part 2!  By the end of this week you should be able to be aware of the options you have for further study in social science research and know the steps to move forward in the application process for advanced training. You've reached the final exam week!  Complete the final exam and the post-course survey. Your feedback can help us improve the course.  Thank you for being a part of this course and good luck for your pursuit of advanced studies in social science research!", "Este curso r\u00c3\u00a1pido de uma semana sob demanda oferece aos participantes uma introdu\u00c3\u00a7\u00c3\u00a3o pr\u00c3\u00a1tica ao design e \u00c3\u00a0 cria\u00c3\u00a7\u00c3\u00a3o de modelos de machine learning no Google Cloud Platform. Com apresenta\u00c3\u00a7\u00c3\u00b5es, demonstra\u00c3\u00a7\u00c3\u00b5es e laborat\u00c3\u00b3rios pr\u00c3\u00a1ticos, os participantes aprender\u00c3\u00a3o os conceitos de machine learning (ML) e do TensorFlow, al\u00c3\u00a9m de habilidades pr\u00c3\u00a1ticas de desenvolvimento, avalia\u00c3\u00a7\u00c3\u00a3o e produ\u00c3\u00a7\u00c3\u00a3o de modelos de ML.\n \n OBJETIVOS\n \n Os participantes desenvolver\u00c3\u00a3o as seguintes habilidades:\n \n  \u00e2\u0097\u008f Identificar casos de uso de machine learning\n \n  \u00e2\u0097\u008f Criar um modelo de ML usando o TensorFlow\n \n  \u00e2\u0097\u008f Criar modelos de ML escalon\u00c3\u00a1veis e implant\u00c3\u00a1veis usando o Cloud ML\n \n  \u00e2\u0097\u008f Entender a import\u00c3\u00a2ncia do pr\u00c3\u00a9-processamento e da combina\u00c3\u00a7\u00c3\u00a3o de atributos\n \n  \u00e2\u0097\u008f Incorporar conceitos avan\u00c3\u00a7ados de ML nos modelos criados\n \n  \u00e2\u0097\u008f Produzir modelos de ML treinados\n \n \n PR\u00c3\u0089-REQUISITOS\n \n Para aproveitar ao m\u00c3\u00a1ximo este curso, os participantes precisam ter:\n \n  \u00e2\u0097\u008f Conclu\u00c3\u00addo o curso Google Cloud Platform Big Data and Machine Learning Fundamentals OU experi\u00c3\u00aancia equivalente\n \n  \u00e2\u0097\u008f Profici\u00c3\u00aancia b\u00c3\u00a1sica em linguagem de consulta comum, como SQL\n \n  \u00e2\u0097\u008f Experi\u00c3\u00aancia com atividades de extra\u00c3\u00a7\u00c3\u00a3o, transforma\u00c3\u00a7\u00c3\u00a3o, carga e modelagem de dados\n \n  \u00e2\u0097\u008f Desenvolvido aplicativos com linguagem de programa\u00c3\u00a7\u00c3\u00a3o comum, como Python\n \n  \u00e2\u0097\u008f Familiaridade com machine learning e/ou estat\u00c3\u00adsticas\n \n Observa\u00c3\u00a7\u00c3\u00b5es sobre a Conta do Google:\n \u00e2\u0080\u00a2 Para se inscrever na avalia\u00c3\u00a7\u00c3\u00a3o gratuita do Google Cloud Platform, voc\u00c3\u00aa precisa de uma Conta do Google/Gmail, al\u00c3\u00a9m de um cart\u00c3\u00a3o de cr\u00c3\u00a9dito ou uma conta banc\u00c3\u00a1ria. Os servi\u00c3\u00a7os do Google est\u00c3\u00a3o temporariamente indispon\u00c3\u00adveis na China.\n \u00e2\u0080\u00a2 Se voc\u00c3\u00aa for um cliente do Google Cloud Platform com endere\u00c3\u00a7o de faturamento na Uni\u00c3\u00a3o Europeia (UE) e na R\u00c3\u00bassia, leia a documenta\u00c3\u00a7\u00c3\u00a3o de descri\u00c3\u00a7\u00c3\u00a3o geral sobre o Imposto sobre Valor Agregado (IVA) em: https://cloud.google.com/billing/docs/resources/vat-overview\n \u00e2\u0080\u00a2 Veja mais perguntas frequentes sobre a avalia\u00c3\u00a7\u00c3\u00a3o gratuita do Google Cloud Platform no site: https://cloud.google.com/free-trial/ Este \u00c3\u00a9 o \"Serverless Machine Learning on Google Cloud Platform\" M\u00c3\u00b3dulo 1: Primeiros passos com machine learning M\u00c3\u00b3dulo 2: Cria\u00c3\u00a7\u00c3\u00a3o de modelos de ML com o TensorFlow M\u00c3\u00b3dulo 3: Escalonamento de modelos de ML com o Cloud ML Engine M\u00c3\u00b3dulo 4: Engenharia de atributos     ", "\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e7\u00b2\u00be\u00e5\u00ba\u00a6\u00e3\u0082\u0092\u00e9\u00ab\u0098\u00e3\u0082\u0081\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0084\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e3\u0081\u00ab\u00e6\u009c\u0089\u00e5\u008a\u00b9\u00e3\u0081\u00aa\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e6\u008a\u00bd\u00e5\u0087\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0097\u00e3\u0081\u00ae\u00e8\u00a6\u008b\u00e6\u00a5\u00b5\u00e3\u0082\u0081\u00e6\u0096\u00b9\u00e3\u0082\u0092\u00e7\u009f\u00a5\u00e3\u0082\u008a\u00e3\u0081\u009f\u00e3\u0081\u0084\u00e4\u00ba\u00ba\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0099\u00e3\u0081\u0099\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082Feature Engineering on Google Cloud Platform \u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e8\u0089\u00af\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00a8\u00e6\u0082\u00aa\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ae\u00e8\u00a6\u0081\u00e7\u00b4\u00a0\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e4\u00b8\u008a\u00e3\u0081\u0092\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e6\u00bc\u0094\u00e7\u00bf\u0092\u00e3\u0081\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u00bf\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u0096\u00e3\u0081\u00aa\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0080\u0081Google Cloud Platform \u00e5\u0086\u0085\u00e3\u0081\u00a7\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e9\u0081\u00b8\u00e6\u008a\u009e\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0082\u00bf\u00e3\u0083\u00bc\u00e3\u0081\u008c\u00e8\u00a7\u00a3\u00e7\u00ad\u0094\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00a7\u00a3\u00e7\u00ad\u0094\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e4\u00bb\u008a\u00e5\u00be\u008c\u00e3\u0080\u0081\u00e7\u009a\u0086\u00e3\u0081\u0095\u00e3\u0082\u0093\u00e3\u0081\u008c\u00e8\u0087\u00aa\u00e8\u00ba\u00ab\u00e3\u0081\u00ae ML \u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0082\u00a7\u00e3\u0082\u00af\u00e3\u0083\u0088\u00e3\u0081\u00ab\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e7\u00b5\u0084\u00e3\u0082\u0080\u00e9\u009a\u009b\u00e3\u0081\u00ab\u00e5\u008f\u0082\u00e7\u0085\u00a7\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0080\u0081\u00e4\u00b8\u0080\u00e8\u0088\u00ac\u00e5\u0085\u00ac\u00e9\u0096\u008b\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e4\u00ba\u0088\u00e5\u00ae\u009a\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e7\u0094\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00b8\u00e3\u0081\u00ae\u00e5\u00a4\u0089\u00e6\u008f\u009b \u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00a8\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ae\u00e4\u00bd\u009c\u00e6\u0088\u0090 \u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e6\u0096\u00ad\u00e9\u009d\u00a2 TF Transform \u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e7\u00b2\u00be\u00e5\u00ba\u00a6\u00e3\u0082\u0092\u00e9\u00ab\u0098\u00e3\u0082\u0081\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0084\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e3\u0081\u00ab\u00e6\u009c\u0089\u00e5\u008a\u00b9\u00e3\u0081\u00aa\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e6\u008a\u00bd\u00e5\u0087\u00ba\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0097\u00e3\u0081\u00ae\u00e8\u00a6\u008b\u00e6\u00a5\u00b5\u00e3\u0082\u0081\u00e6\u0096\u00b9\u00e3\u0082\u0092\u00e7\u009f\u00a5\u00e3\u0082\u008a\u00e3\u0081\u009f\u00e3\u0081\u0084\u00e6\u0096\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0099\u00e3\u0081\u0099\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082Feature Engineering\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e8\u0089\u00af\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00a8\u00e6\u0082\u00aa\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0080\u0081\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u00a4\u0089\u00e6\u008f\u009b\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e4\u00b8\u008a\u00e3\u0081\u0092\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0082\u00b8\u00e3\u0083\u008b\u00e3\u0082\u00a2\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00af\u00e3\u0080\u0081ML \u00e3\u0083\u0097\u00e3\u0083\u00ad\u00e3\u0082\u00b8\u00e3\u0082\u00a7\u00e3\u0082\u00af\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e6\u009c\u0080\u00e3\u0082\u0082\u00e9\u0095\u00b7\u00e3\u0081\u008f\u00e3\u0080\u0081\u00e5\u009b\u00b0\u00e9\u009b\u00a3\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e5\u00a4\u009a\u00e3\u0081\u0084\u00e3\u0083\u0095\u00e3\u0082\u00a7\u00e3\u0083\u00bc\u00e3\u0082\u00ba\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0082\u00b8\u00e3\u0083\u008b\u00e3\u0082\u00a2\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u00be\u00e3\u0081\u009a\u00e7\u0094\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e3\u0081\u0084\u00e3\u0080\u0081\u00e5\u00af\u00be\u00e8\u00b1\u00a1\u00e9\u00a0\u0098\u00e5\u009f\u009f\u00e3\u0081\u00ae\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0082\u0092\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u00a2\u00e3\u0083\u00ab\u00e3\u0082\u00b4\u00e3\u0083\u00aa\u00e3\u0082\u00ba\u00e3\u0083\u00a0\u00e3\u0082\u0092\u00e6\u00a9\u009f\u00e8\u0083\u00bd\u00e3\u0081\u0095\u00e3\u0081\u009b\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ae\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e8\u0089\u00af\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ae\u00e6\u009d\u00a1\u00e4\u00bb\u00b6\u00e3\u0081\u00a8\u00e3\u0080\u0081ML \u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u0092\u00e8\u00a1\u00a8\u00e7\u008f\u00be\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e6\u00a4\u009c\u00e8\u00a8\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00bb\u00e3\u0082\u00af\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00a8\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ae\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae 2 \u00e3\u0081\u00a4\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u00b7\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0083\u00a0\u00e7\u0094\u00a8\u00e3\u0081\u00ae\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e6\u00ba\u0096\u00e5\u0082\u0099\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a4\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u008b\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e5\u00be\u0093\u00e6\u009d\u00a5\u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e6\u0096\u00ad\u00e9\u009d\u00a2\u00e3\u0081\u00af\u00e3\u0081\u009d\u00e3\u0082\u008c\u00e3\u0081\u00bb\u00e3\u0081\u00a9\u00e5\u00a4\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00aa\u00e5\u00bd\u00b9\u00e5\u0089\u00b2\u00e3\u0082\u0092\u00e6\u009e\u009c\u00e3\u0081\u009f\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u009b\u00e3\u0082\u0093\u00e3\u0081\u00a7\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0080\u0082\u00e3\u0081\u0097\u00e3\u0081\u008b\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e4\u00bb\u008a\u00e6\u0097\u00a5\u00e3\u0081\u00ae ML \u00e6\u0096\u00b9\u00e5\u00bc\u008f\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e6\u0096\u00ad\u00e9\u009d\u00a2\u00e3\u0081\u00af\u00e9\u009d\u009e\u00e5\u00b8\u00b8\u00e3\u0081\u00ab\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u00ad\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae 1 \u00e3\u0081\u00a4\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00aa\u00e5\u0095\u008f\u00e9\u00a1\u008c\u00e3\u0081\u00ab\u00e3\u0081\u008a\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e6\u0096\u00ad\u00e9\u009d\u00a2\u00e3\u0081\u008c\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a4\u00e3\u0081\u008b\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 TensorFlow Transform\u00ef\u00bc\u0088tf.Transform\u00ef\u00bc\u0089\u00e3\u0081\u00af\u00e3\u0080\u0081TensorFlow \u00e3\u0081\u00a7\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00ab\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u0096\u00e3\u0083\u00a9\u00e3\u0083\u00aa\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082tf.Transform \u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0085\u00a8\u00e4\u00bd\u0093\u00e3\u0082\u0092\u00e6\u00b8\u00a1\u00e3\u0081\u0099\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086\u00e5\u00a0\u00b4\u00e5\u0090\u0088\u00e3\u0081\u00ab\u00e5\u00bd\u00b9\u00e7\u00ab\u008b\u00e3\u0081\u00a1\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u009f\u00e3\u0081\u00a8\u00e3\u0081\u0088\u00e3\u0081\u00b0\u00e3\u0080\u0081\u00e5\u00b9\u00b3\u00e5\u009d\u0087\u00e5\u0080\u00a4\u00e3\u0081\u00a8\u00e6\u00a8\u0099\u00e6\u00ba\u0096\u00e5\u0081\u008f\u00e5\u00b7\u00ae\u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e5\u0085\u00a5\u00e5\u008a\u009b\u00e5\u0080\u00a4\u00e3\u0081\u00ae\u00e6\u00ad\u00a3\u00e8\u00a6\u008f\u00e5\u008c\u0096\u00e3\u0080\u0081\u00e5\u0080\u00a4\u00e3\u0081\u00ae\u00e3\u0081\u0099\u00e3\u0081\u00b9\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e5\u0085\u00a5\u00e5\u008a\u009b\u00e4\u00be\u008b\u00e3\u0082\u0092\u00e6\u00a4\u009c\u00e6\u009f\u00bb\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0082\u008b\u00e3\u0083\u009c\u00e3\u0082\u00ad\u00e3\u0083\u00a3\u00e3\u0083\u0096\u00e3\u0083\u00a9\u00e3\u0083\u00aa\u00e3\u0081\u00ae\u00e6\u0095\u00b4\u00e6\u0095\u00b0\u00e5\u008c\u0096\u00e3\u0080\u0081\u00e8\u00a6\u00b3\u00e6\u00b8\u00ac\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u0095\u00a3\u00e3\u0081\u00ab\u00e5\u009f\u00ba\u00e3\u0081\u00a5\u00e3\u0081\u008f\u00e5\u0085\u00a5\u00e5\u008a\u009b\u00e3\u0081\u00ae\u00e3\u0083\u0090\u00e3\u0082\u00b1\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e5\u008c\u0096\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081tf.Transform \u00e3\u0081\u00ae\u00e7\u0094\u00a8\u00e9\u0080\u0094\u00e3\u0082\u0092\u00e6\u00a4\u009c\u00e8\u00a8\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0082\u00b8\u00e3\u0083\u008b\u00e3\u0082\u00a2\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u00ae\u00e5\u0090\u0084\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e4\u00b8\u00bb\u00e3\u0081\u00aa\u00e5\u0086\u0085\u00e5\u00ae\u00b9\u00e3\u0081\u00af\u00e6\u00ac\u00a1\u00e3\u0081\u00ae\u00e3\u0081\u00a8\u00e3\u0081\u008a\u00e3\u0082\u008a\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u0089\u00af\u00e3\u0081\u0084\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0081\u00ae\u00e9\u0081\u00b8\u00e6\u008a\u009e\u00e3\u0080\u0081\u00e5\u00a4\u00a7\u00e8\u00a6\u008f\u00e6\u00a8\u00a1\u00e3\u0081\u00aa\u00e5\u0089\u008d\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e6\u0096\u00ad\u00e9\u009d\u00a2\u00e3\u0081\u00ae\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0080\u0081TensorFlow \u00e3\u0081\u00ae\u00e5\u00ae\u009f\u00e8\u00b7\u00b5\u00e6\u00bc\u0094\u00e7\u00bf\u0092\u00e3\u0080\u0082", "Vous souhaitez d\u00c3\u00a9couvrir comment am\u00c3\u00a9liorer la pr\u00c3\u00a9cision de vos mod\u00c3\u00a8les de machine learning (ML) ? Vous voulez identifier les colonnes de donn\u00c3\u00a9es offrant les caract\u00c3\u00a9ristiques les plus utiles ? Bienvenue dans le cours Feature Engineering on Google Cloud Platform (Extraction de caract\u00c3\u00a9ristiques sur Google Cloud Platform). Nous vous expliquerons ce qui distingue les bonnes caract\u00c3\u00a9ristiques des mauvaises, puis nous vous montrerons comment pr\u00c3\u00a9traiter et transformer vos caract\u00c3\u00a9ristiques afin d'optimiser leur efficacit\u00c3\u00a9 dans vos mod\u00c3\u00a8les.\n\nDes ateliers interactifs vous permettront de mettre en pratique ce que vous avez appris. Vous s\u00c3\u00a9lectionnerez vous-m\u00c3\u00aame des caract\u00c3\u00a9ristiques, puis les pr\u00c3\u00a9traiterez dans Google Cloud Platform. Nos formateurs vous aideront \u00c3\u00a0 comprendre les solutions de code. Ces solutions seront accessibles \u00c3\u00a0 tous, et pourront vous servir de r\u00c3\u00a9f\u00c3\u00a9rence en cas de besoin lorsque vous travaillerez sur vos propres projets de ML. Introduction Des donn\u00c3\u00a9es brutes aux caract\u00c3\u00a9ristiques Pr\u00c3\u00a9traitement et cr\u00c3\u00a9ation des caract\u00c3\u00a9ristiques Croisement de caract\u00c3\u00a9ristiques TF\u00c2\u00a0Transform R\u00c3\u00a9sum\u00c3\u00a9 Vous souhaitez d\u00c3\u00a9couvrir comment am\u00c3\u00a9liorer la pr\u00c3\u00a9cision de vos mod\u00c3\u00a8les de machine learning ? Vous voulez identifier les colonnes de donn\u00c3\u00a9es offrant les caract\u00c3\u00a9ristiques les plus utiles ? Bienvenue dans le cours Feature Engineering (Extraction de caract\u00c3\u00a9ristiques). Tout d'abord, nous vous expliquerons comment distinguer les bonnes caract\u00c3\u00a9ristiques des mauvaises. Ensuite, nous vous montrerons comment pr\u00c3\u00a9traiter et transformer ces caract\u00c3\u00a9ristiques afin d'optimiser leur efficacit\u00c3\u00a9 dans vos mod\u00c3\u00a8les. L'extraction de caract\u00c3\u00a9ristiques est souvent la phase la plus longue et la plus complexe du d\u00c3\u00a9veloppement de votre projet de machine learning. Lors du processus d'extraction de caract\u00c3\u00a9ristiques, vous partez des donn\u00c3\u00a9es brutes, puis utilisez vos propres connaissances du domaine afin de cr\u00c3\u00a9er les caract\u00c3\u00a9ristiques appropri\u00c3\u00a9es pour vos algorithmes de machine learning. Dans ce module, nous allons \u00c3\u00a9tudier ce qui distingue les bonnes caract\u00c3\u00a9ristiques des mauvaises, et la fa\u00c3\u00a7on de les repr\u00c3\u00a9senter dans votre mod\u00c3\u00a8le de machine learning. Cette section porte sur le pr\u00c3\u00a9traitement et la cr\u00c3\u00a9ation de caract\u00c3\u00a9ristiques. Ces techniques de traitement des donn\u00c3\u00a9es vous aideront \u00c3\u00a0 pr\u00c3\u00a9parer un ensemble de caract\u00c3\u00a9ristiques pour un syst\u00c3\u00a8me de machine learning. Le croisement de caract\u00c3\u00a9ristiques ne joue pas un r\u00c3\u00b4le significatif dans le machine learning traditionnel. \u00c3\u0080 l'inverse, sa valeur est inestimable pour les m\u00c3\u00a9thodes de machine learning actuelles. Dans ce module, vous allez apprendre \u00c3\u00a0 reconna\u00c3\u00aetre les types de probl\u00c3\u00a8mes pour lesquels le croisement de caract\u00c3\u00a9ristiques joue un r\u00c3\u00b4le majeur en termes d'apprentissage. TensorFlow Transform (tf.Transform) est une biblioth\u00c3\u00a8que con\u00c3\u00a7ue pour le pr\u00c3\u00a9traitement des donn\u00c3\u00a9es avec TensorFlow. Elle est utile pour les pr\u00c3\u00a9traitements qui n\u00c3\u00a9cessitent un croisement complet des donn\u00c3\u00a9es, tels que la normalisation d'une valeur d'entr\u00c3\u00a9e selon la moyenne et l'\u00c3\u00a9cart-type, la conversion du vocabulaire en nombres entiers \u00c3\u00a0 partir des exemples d'entr\u00c3\u00a9es pour toutes les valeurs, la cr\u00c3\u00a9ation de buckets d'entr\u00c3\u00a9es \u00c3\u00a0 partir de la r\u00c3\u00a9partition observ\u00c3\u00a9e des donn\u00c3\u00a9es. Dans ce module, nous \u00c3\u00a9tudierons les cas d'utilisation de tf.Transform. Voici le r\u00c3\u00a9sum\u00c3\u00a9 des principaux points appris dans chaque module du cours Feature Engineering (Extraction des caract\u00c3\u00a9ristiques) : s\u00c3\u00a9lection des bonnes caract\u00c3\u00a9ristiques, pr\u00c3\u00a9traitement \u00c3\u00a0 grande \u00c3\u00a9chelle, utilisation des croisements de caract\u00c3\u00a9ristiques, entra\u00c3\u00aenement avec TensorFlow.\n", "Cette formation \u00c3\u00a0 la demande propose aux participants de s'initier par la pratique \u00c3\u00a0 la conception et \u00c3\u00a0 la construction de mod\u00c3\u00a8les de machine learning (ML) sur Google Cloud Platform. Il s'agit d'un cours acc\u00c3\u00a9l\u00c3\u00a9r\u00c3\u00a9, que vous pouvez effectuer en une semaine. En suivant une s\u00c3\u00a9rie de pr\u00c3\u00a9sentations, de d\u00c3\u00a9monstrations et d'ateliers, les participants d\u00c3\u00a9couvriront les concepts du ML et de TensorFlow et acquerront des comp\u00c3\u00a9tences pratiques pour d\u00c3\u00a9velopper, \u00c3\u00a9valuer et mettre en production des mod\u00c3\u00a8les de ML.\n\nOBJECTIFS\n\nAu terme de cette formation, les participants auront acquis les comp\u00c3\u00a9tences suivantes :\n\n  \u00e2\u0097\u008f Identifier les cas d'utilisation du machine learning\n  \u00e2\u0097\u008f Cr\u00c3\u00a9er un mod\u00c3\u00a8le de ML avec TensorFlow\n  \u00e2\u0097\u008f Cr\u00c3\u00a9er des mod\u00c3\u00a8les de ML \u00c3\u00a9volutifs et d\u00c3\u00a9ployables \u00c3\u00a0 l'aide de Cloud ML\n  \u00e2\u0097\u008f Reconna\u00c3\u00aetre l'importance de pr\u00c3\u00a9traiter et de combiner les caract\u00c3\u00a9ristiques\n  \u00e2\u0097\u008f Int\u00c3\u00a9grer des concepts de ML avanc\u00c3\u00a9s dans leurs mod\u00c3\u00a8les\n  \u00e2\u0097\u008f Mettre en production des mod\u00c3\u00a8les de ML entra\u00c3\u00aen\u00c3\u00a9s\n\n\nPR\u00c3\u0089REQUIS\n\nPour tirer pleinement parti de ce cours, les participants doivent remplir les pr\u00c3\u00a9requis suivants :\n\n  \u00e2\u0097\u008f Avoir suivi la formation Google Cloud Platform Big Data and Machine Learning Fundamentals OU disposer d'une exp\u00c3\u00a9rience \u00c3\u00a9quivalente\n  \u00e2\u0097\u008f Ma\u00c3\u00aetriser les principes de base des langages de requ\u00c3\u00aate courants tels que SQL\n  \u00e2\u0097\u008f Avoir de l'exp\u00c3\u00a9rience en mod\u00c3\u00a9lisation, extraction, transformation et chargement des donn\u00c3\u00a9es\n  \u00e2\u0097\u008f Savoir d\u00c3\u00a9velopper des applications \u00c3\u00a0 l'aide d'un langage de programmation courant tel que Python\n  \u00e2\u0097\u008f Savoir utiliser le machine learning et/ou les statistiques\n\n\nRemarques relatives au compte Google :\n\u00e2\u0080\u00a2 Vous avez besoin d'un compte Google/Gmail et d'une carte de cr\u00c3\u00a9dit ou d'un compte bancaire pour vous inscrire \u00c3\u00a0 l'essai gratuit de Google Cloud Platform. (Les services Google sont actuellement indisponibles en Chine.)\n\u00e2\u0080\u00a2 Si votre adresse de facturation pour les services GCP est situ\u00c3\u00a9e en Union europ\u00c3\u00a9enne (UE) ou en Russie, lisez la documentation relative \u00c3\u00a0 la TVA \u00c3\u00a0 l'adresse suivante : https://cloud.google.com/billing/docs/resources/vat-overview.\n\u00e2\u0080\u00a2 Vous trouverez d'autres questions fr\u00c3\u00a9quentes relatives \u00c3\u00a0 l'essai gratuit de GCP \u00c3\u00a0 l'adresse suivante : https://cloud.google.com/free-trial/ Introduction au machine learning sans serveur sur Google Cloud\u00c2\u00a0Platform Module\u00c2\u00a01\u00c2\u00a0: Premiers pas avec le machine learning Module\u00c2\u00a02\u00c2\u00a0: Cr\u00c3\u00a9er des mod\u00c3\u00a8les de ML avec TensorFlow Module\u00c2\u00a03\u00c2\u00a0: Faire \u00c3\u00a9voluer les mod\u00c3\u00a8les de ML avec Cloud ML\u00c2\u00a0Engine Module\u00c2\u00a04\u00c2\u00a0: Extraction de caract\u00c3\u00a9ristiques     ", "Ce cours intensif, d'une dur\u00c3\u00a9e d'une semaine, se base sur de pr\u00c3\u00a9c\u00c3\u00a9dents cours de la sp\u00c3\u00a9cialisation Data Engineering on Google Cloud Platform. \u00c3\u0080 travers un ensemble de conf\u00c3\u00a9rences vid\u00c3\u00a9o, de d\u00c3\u00a9monstrations et d'ateliers pratiques, vous allez apprendre \u00c3\u00a0 cr\u00c3\u00a9er et \u00c3\u00a0 g\u00c3\u00a9rer des clusters informatiques pour ex\u00c3\u00a9cuter des t\u00c3\u00a2ches Hadoop, Spark, Pig et Hive sur Google Cloud Platform.Vous apprendrez \u00c3\u00a9galement \u00c3\u00a0 acc\u00c3\u00a9der \u00c3\u00a0 diverses options de stockage dans le cloud \u00c3\u00a0 partir de leurs clusters de calcul et \u00c3\u00a0 int\u00c3\u00a9grer les fonctionnalit\u00c3\u00a9s de machine learning de Google \u00c3\u00a0 leurs programmes d'analyse.\n \n Lors des ateliers pratiques, vous allez cr\u00c3\u00a9er et g\u00c3\u00a9rer des clusters Dataproc via la console Web et la CLI, et vous utiliserez les clusters pour ex\u00c3\u00a9cuter des t\u00c3\u00a2ches Spark et Pig. Vous cr\u00c3\u00a9erez ensuite des notebooks iPython qui s'int\u00c3\u00a8grent \u00c3\u00a0 BigQuery et \u00c3\u00a0 l'espace de stockage, et vous utiliserez Spark. Enfin, vous int\u00c3\u00a9grerez les API de machine learning \u00c3\u00a0 votre analyse de donn\u00c3\u00a9es.\n \n Pr\u00c3\u00a9requis\n \u00e2\u0080\u00a2 Avoir suivi la formation Google Cloud Platform Big Data & Machine Learning Fundamentals (ou disposer d'une exp\u00c3\u00a9rience \u00c3\u00a9quivalente)\n \u00e2\u0080\u00a2 Disposer de quelques notions de Python Module\u00c2\u00a01\u00c2\u00a0: Pr\u00c3\u00a9sentation de Cloud\u00c2\u00a0Dataproc Module\u00c2\u00a02\u00c2\u00a0: Ex\u00c3\u00a9cuter des t\u00c3\u00a2ches Dataproc Module\u00c2\u00a03\u00c2\u00a0: Tirer parti de GCP Module\u00c2\u00a04\u00c2\u00a0: Analyser des donn\u00c3\u00a9es non structur\u00c3\u00a9es    ", "\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae 3 \u00e7\u0095\u00aa\u00e7\u009b\u00ae\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0080\u008cAchieving Advanced Insights with BigQuery\u00e3\u0080\u008d\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00b3\u00e3\u0082\u00af\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e8\u00a4\u0087\u00e9\u009b\u0091\u00e3\u0081\u00aa\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e5\u008f\u00af\u00e8\u0083\u00bd\u00e3\u0081\u00aa\u00e3\u0082\u00b9\u00e3\u0083\u0086\u00e3\u0083\u0083\u00e3\u0083\u0097\u00e3\u0081\u00ab\u00e5\u0088\u0086\u00e5\u0089\u00b2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00aa\u00e3\u0081\u008c\u00e3\u0082\u0089\u00e3\u0080\u0081SQL \u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0082\u0092\u00e6\u00b7\u00b1\u00e3\u0082\u0081\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n BigQuery \u00e3\u0081\u00ae\u00e5\u0086\u0085\u00e9\u0083\u00a8\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3\u00ef\u00bc\u0088\u00e5\u0088\u0097\u00e3\u0083\u0099\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e3\u0082\u00b7\u00e3\u0083\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u0087\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0082\u00b0 \u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00ef\u00bc\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0084\u00e3\u0080\u0081ARRAY \u00e3\u0081\u00a8 STRUCT \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0080\u0081\u00e3\u0083\u008d\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u009f\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0081\u00a8\u00e7\u00b9\u00b0\u00e3\u0082\u008a\u00e8\u00bf\u0094\u00e3\u0081\u0097\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa SQL \u00e3\u0083\u0088\u00e3\u0083\u0094\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u0080\u00e5\u00be\u008c\u00e3\u0081\u00ab\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0081\u00ae\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e6\u0089\u00bf\u00e8\u00aa\u008d\u00e6\u00b8\u0088\u00e3\u0081\u00bf\u00e3\u0083\u0093\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e4\u00bf\u009d\u00e8\u00ad\u00b7\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n >>> \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ab\u00e7\u0099\u00bb\u00e9\u008c\u00b2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0081\u008f\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e8\u00b3\u00aa\u00e5\u0095\u008f\u00e3\u0081\u00ab\u00e8\u00a8\u0098\u00e8\u00bc\u0089\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0081\u008a\u00e3\u0082\u008a Qwiklabs \u00e3\u0081\u00ae\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e8\u00a6\u008f\u00e7\u00b4\u0084\u00e3\u0081\u00ab\u00e5\u0090\u008c\u00e6\u0084\u008f\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00a9\u00b3\u00e7\u00b4\u00b0\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0081\u00af\u00e3\u0080\u0081https://qwiklabs.com/terms_of_service \u00e3\u0082\u0092\u00e3\u0081\u0094\u00e8\u00a6\u00a7\u00e3\u0081\u008f\u00e3\u0081\u00a0\u00e3\u0081\u0095\u00e3\u0081\u0084\u00e3\u0080\u0082<<< \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00b3\u00e3\u0082\u00af\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00a8\u00e5\u008f\u00a5 \u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0081\u00ae\u00e8\u00a8\u00ad\u00e8\u00a8\u0088\u00e3\u0081\u00a8\u00e3\u0083\u008d\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e6\u00a7\u008b\u00e9\u0080\u00a0 Google \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u00ae\u00e5\u00bc\u00b7\u00e5\u008c\u0096 \u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096 Cloud Datalab \u00e3\u0081\u00ae\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e6\u0083\u0085\u00e5\u00a0\u00b1 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00a2\u00e3\u0082\u00af\u00e3\u0082\u00bb\u00e3\u0082\u00b9 \u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 Data Insights \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae 3 \u00e7\u0095\u00aa\u00e7\u009b\u00ae\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u0094\u00e5\u008f\u0082\u00e5\u008a\u00a0\u00e3\u0081\u0084\u00e3\u0081\u009f\u00e3\u0081\u00a0\u00e3\u0081\u008d\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e3\u0081\u008c\u00e3\u0081\u00a8\u00e3\u0081\u0086\u00e3\u0081\u0094\u00e3\u0081\u0096\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082BigQuery \u00e3\u0081\u00ae\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00b3\u00e3\u0082\u00af\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00a8\u00e3\u0082\u00a2\u00e3\u0083\u00bc\u00e3\u0082\u00ad\u00e3\u0083\u0086\u00e3\u0082\u00af\u00e3\u0083\u0081\u00e3\u0083\u00a3\u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e5\u0086\u0085\u00e5\u00ae\u00b9\u00e3\u0082\u0092\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0080\u0082 \u00e7\u00b5\u00b1\u00e8\u00a8\u0088\u00e7\u009a\u0084\u00e8\u00bf\u0091\u00e4\u00bc\u00bc\u00e3\u0080\u0081\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0082\u00a6\u00e3\u0082\u00a3\u00e3\u0083\u00b3\u00e3\u0083\u0089\u00e3\u0082\u00a6 \u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0080\u0081\u00e3\u0083\u00a6\u00e3\u0083\u00bc\u00e3\u0082\u00b6\u00e3\u0083\u00bc\u00e5\u00ae\u009a\u00e7\u00be\u00a9\u00e9\u0096\u00a2\u00e6\u0095\u00b0\u00e3\u0080\u0081WITH \u00e5\u008f\u00a5\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e3\u0083\u0095\u00e3\u0082\u00a1\u00e3\u0083\u00b3\u00e3\u0082\u00af\u00e3\u0082\u00b7\u00e3\u0083\u00a7\u00e3\u0083\u00b3\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b6\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00a7\u00e3\u0080\u0081BigQuery \u00e3\u0081\u00a7\u00e3\u0081\u00ae SQL \u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0082\u0092\u00e6\u00b7\u00b1\u00e3\u0082\u0081\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e5\u00be\u0093\u00e6\u009d\u00a5\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u0099\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u008c\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ae\u00e9\u0080\u00b2\u00e5\u008c\u0096\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e5\u0088\u00b6\u00e7\u00b4\u0084\u00e3\u0081\u00ab\u00e5\u00af\u00be\u00e5\u0087\u00a6\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab BigQuery \u00e3\u0081\u008c\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e9\u0096\u008b\u00e7\u0099\u00ba\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u009f\u00e3\u0081\u008b\u00e3\u0082\u0092\u00e6\u00af\u0094\u00e8\u00bc\u0083\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e9\u009d\u009e\u00e6\u00ad\u00a3\u00e8\u00a6\u008f\u00e5\u008c\u0096 BigQuery \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e6\u00a7\u008b\u00e9\u0080\u00a0\u00e3\u0081\u00ae\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e3\u0081\u00aa\u00e9\u0083\u00a8\u00e5\u0088\u0086\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e3\u0080\u0081\u00e3\u0083\u008d\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u009f\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0081\u00a8\u00e7\u00b9\u00b0\u00e3\u0082\u008a\u00e8\u00bf\u0094\u00e3\u0081\u0097\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0083\u0080\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u009c\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00ae\u00e8\u00a8\u0088\u00e7\u00ae\u0097\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0080\u0081\u00e3\u0083\u0095\u00e3\u0082\u00a3\u00e3\u0083\u00ab\u00e3\u0082\u00bf\u00e3\u0080\u0081\u00e8\u00a4\u0087\u00e6\u0095\u00b0\u00e3\u0083\u009a\u00e3\u0083\u00bc\u00e3\u0082\u00b8\u00e3\u0081\u00ae\u00e3\u0083\u00ac\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0083\u0088\u00e3\u0080\u0081\u00e3\u0083\u0080\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0083\u009c\u00e3\u0083\u00bc\u00e3\u0083\u0089 \u00e3\u0082\u00ad\u00e3\u0083\u00a3\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0080\u0081\u00e9\u00ab\u0098\u00e5\u00ba\u00a6\u00e3\u0081\u00aa\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u0094\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 BigQuery \u00e3\u0081\u00ae\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e5\u00bd\u00b1\u00e9\u009f\u00bf\u00e3\u0082\u0092\u00e4\u00b8\u008e\u00e3\u0081\u0088\u00e3\u0082\u008b\u00e5\u009f\u00ba\u00e6\u009c\u00ac\u00e7\u009a\u0084\u00e3\u0081\u00aa\u00e4\u00bd\u009c\u00e6\u00a5\u00ad\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e9\u00ab\u0098\u00e9\u0080\u009f\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf \u00e3\u0082\u00b5\u00e3\u0082\u00a4\u00e3\u0082\u00a8\u00e3\u0083\u00b3\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0082\u00b9\u00e3\u0083\u0088 \u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0082\u00ad\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e4\u00b8\u00bb\u00e3\u0081\u00aa\u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u0082\u00e3\u0082\u008b Cloud Datalab \u00e3\u0081\u00ae\u00e7\u00b4\u00b9\u00e4\u00bb\u008b\u00e3\u0080\u0082\u00e3\u0082\u00a2\u00e3\u0083\u008a\u00e3\u0083\u00aa\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u00af\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00a9\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u00af\u00e3\u0083\u00a9\u00e3\u0082\u00a6\u00e3\u0083\u0089 \u00e3\u0083\u008e\u00e3\u0083\u00bc\u00e3\u0083\u0088\u00e3\u0083\u0096\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u0085\u00b1\u00e5\u0090\u008c\u00e4\u00bd\u009c\u00e6\u00a5\u00ad\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 BigQuery \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e4\u00bf\u009d\u00e8\u00ad\u00b7\u00e3\u0081\u00a8\u00e5\u0085\u00b1\u00e6\u009c\u0089\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e7\u00b5\u0084\u00e7\u00b9\u0094\u00e3\u0081\u00ab\u00e3\u0081\u00a8\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e9\u0087\u008d\u00e8\u00a6\u0081\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082Google Cloud Platform \u00e3\u0081\u00a8 BigQuery \u00e3\u0083\u0084\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e3\u0081\u0086\u00e3\u0081\u00a1\u00e3\u0080\u0081\u00e6\u00a8\u00a9\u00e9\u0099\u0090\u00e3\u0081\u00ae\u00e7\u00ae\u00a1\u00e7\u0090\u0086\u00e3\u0081\u00a8\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u0085\u00b1\u00e6\u009c\u0089\u00e3\u0081\u00ab\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0082\u0082\u00e3\u0081\u00ae\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b3\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u008a\u00e7\u0096\u00b2\u00e3\u0082\u008c\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u00a7\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0081\u00a7\u00e7\u00b5\u0082\u00e4\u00ba\u0086\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0081\u00be\u00e3\u0081\u00a7\u00e3\u0081\u00ab\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e5\u0086\u0085\u00e5\u00ae\u00b9\u00e3\u0082\u0092\u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081\u00e3\u0081\u00a6\u00e3\u0081\u00bf\u00e3\u0081\u00be\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0080\u0082", "\u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0081\u00af\u00e3\u0080\u0081Data to Insights \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae 2 \u00e7\u0095\u00aa\u00e7\u009b\u00ae\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u0093\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e5\u00a4\u0096\u00e9\u0083\u00a8\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092 BigQuery \u00e3\u0081\u00ab\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0080\u0081Google \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u00be\u00e3\u0081\u009f\u00e3\u0080\u0081\u00e8\u00a4\u0087\u00e6\u0095\u00b0\u00e3\u0083\u0086\u00e3\u0083\u00bc\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0081\u00ae JOIN \u00e3\u0081\u00a8 UNION \u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00e3\u0080\u0081\u00e4\u00b8\u00ad\u00e7\u00b4\u009a\u00e8\u0080\u0085\u00e5\u0090\u0091\u00e3\u0081\u0091\u00e3\u0081\u00ae SQL \u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e5\u00bf\u00b5\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0082\u0082\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082JOIN \u00e3\u0082\u0084 UNION \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e8\u00a4\u0087\u00e6\u0095\u00b0\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bd\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u0092\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n \u00e6\u00b3\u00a8: \u00e3\u0081\u0099\u00e3\u0081\u00a7\u00e3\u0081\u00ab SQL \u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e7\u009f\u00a5\u00e8\u00ad\u0098\u00e3\u0082\u0092\u00e3\u0081\u008a\u00e6\u008c\u0081\u00e3\u0081\u00a1\u00e3\u0081\u00ae\u00e6\u0096\u00b9\u00e3\u0082\u0082\u00e3\u0080\u0081BigQuery \u00e3\u0081\u00ab\u00e5\u009b\u00ba\u00e6\u009c\u0089\u00e3\u0081\u00ae\u00e8\u00a6\u0081\u00e7\u00b4\u00a0\u00ef\u00bc\u0088\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa \u00e3\u0082\u00ad\u00e3\u0083\u00a3\u00e3\u0083\u0083\u00e3\u0082\u00b7\u00e3\u0083\u00a5\u00e3\u0082\u0084\u00e3\u0083\u0086\u00e3\u0083\u00bc\u00e3\u0083\u0096\u00e3\u0083\u00ab \u00e3\u0083\u00af\u00e3\u0082\u00a4\u00e3\u0083\u00ab\u00e3\u0083\u0089\u00e3\u0082\u00ab\u00e3\u0083\u00bc\u00e3\u0083\u0089\u00e3\u0081\u00ae\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00ef\u00bc\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e3\u0081\u00b6\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n \n >>> \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ab\u00e7\u0099\u00bb\u00e9\u008c\u00b2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0081\u008f\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e8\u00b3\u00aa\u00e5\u0095\u008f\u00e3\u0081\u00ab\u00e8\u00a8\u0098\u00e8\u00bc\u0089\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0081\u008a\u00e3\u0082\u008a Qwiklabs \u00e3\u0081\u00ae\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e8\u00a6\u008f\u00e7\u00b4\u0084\u00e3\u0081\u00ab\u00e5\u0090\u008c\u00e6\u0084\u008f\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00a9\u00b3\u00e7\u00b4\u00b0\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e3\u0081\u00af\u00e3\u0080\u0081https://qwiklabs.com/terms_of_service \u00e3\u0082\u0092\u00e3\u0081\u0094\u00e8\u00a6\u00a7\u00e3\u0081\u008f\u00e3\u0081\u00a0\u00e3\u0081\u0095\u00e3\u0081\u0084\u00e3\u0080\u0082<<< \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e4\u00bf\u009d\u00e5\u00ad\u0098\u00e3\u0081\u00a8\u00e3\u0082\u00a8\u00e3\u0082\u00af\u00e3\u0082\u00b9\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0083\u0088 Google BigQuery \u00e3\u0081\u00b8\u00e3\u0081\u00ae\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e5\u008f\u0096\u00e3\u0082\u008a\u00e8\u00be\u00bc\u00e3\u0081\u00bf \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e7\u00b5\u0090\u00e5\u0090\u0088\u00e3\u0081\u00a8\u00e7\u00b5\u00b1\u00e5\u0090\u0088 \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096 \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e7\u00b5\u0082\u00e4\u00ba\u0086\u00e3\u0081\u00ab\u00e3\u0081\u0082\u00e3\u0081\u009f\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081  \u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e7\u00b5\u0090\u00e6\u009e\u009c\u00e3\u0081\u008b\u00e3\u0082\u0089\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e6\u00b0\u00b8\u00e7\u00b6\u009a\u00e3\u0083\u0086\u00e3\u0083\u00bc\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0081\u00a8\u00e4\u00b8\u0080\u00e6\u0099\u0082\u00e3\u0083\u0086\u00e3\u0083\u00bc\u00e3\u0083\u0096\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b BigQuery \u00e3\u0081\u00ab\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0082\u0093\u00e3\u0081\u00a7\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b SQL \u00e3\u0081\u00ae JOIN \u00e3\u0081\u00a8 UNION \u00e3\u0081\u00ae\u00e9\u0081\u0095\u00e3\u0081\u0084\u00e3\u0081\u00a8\u00e4\u00bd\u00bf\u00e3\u0081\u0084\u00e5\u0088\u0086\u00e3\u0081\u0091\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u0090\u0086\u00e8\u00a7\u00a3\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u00ae\u00e6\u00af\u0094\u00e8\u00bc\u0083\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0084\u00e3\u0080\u0081Google \u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u009d\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e4\u00bd\u00bf\u00e3\u0081\u0084\u00e6\u0096\u00b9\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e3\u0081\u00b6 ", "Dieser einw\u00c3\u00b6chige On-Demand-Schnellkurs bietet Teilnehmern eine praktische Einf\u00c3\u00bchrung in die Entwicklung und Erstellung von Modellen f\u00c3\u00bcr maschinelles Lernen (Machine Learning, ML) mithilfe der Google Cloud Platform. Mit einem Mix aus Pr\u00c3\u00a4sentationen, Demos und praxisorientierten Labs machen Sie sich mit ML- und TensorFlow-Konzepten vertraut und lernen, wie Sie ML-Modelle entwickeln, bewerten und zur Bereitstellung vorbereiten.\n\nLERNZIELE\n\nDieser Kurs vermittelt den Teilnehmern die folgenden Kompetenzen:\n\n  \u00e2\u0097\u008f Anwendungsf\u00c3\u00a4lle f\u00c3\u00bcr maschinelles Lernen bestimmen\n\n  \u00e2\u0097\u008f ML-Modelle mit TensorFlow erstellen\n\n  \u00e2\u0097\u008f Skalierbare, bereitstellbare ML-Modelle mit Cloud ML erstellen\n\n  \u00e2\u0097\u008f Bedeutung der Vorverarbeitung und Kombination von Funktionen erkennen\n\n  \u00e2\u0097\u008f Erweiterte ML-Konzepte in Modelle integrieren\n\n  \u00e2\u0097\u008f Trainierte ML-Modelle f\u00c3\u00bcr die Bereitstellung vorbereiten\n\n\nVORAUSSETZUNGEN\n\nF\u00c3\u00bcr maximale Lernerfolge sollten die Teilnehmer Folgendes mitbringen:\n\n  \u00e2\u0097\u008f Abgeschlossener Kurs \"Google Cloud Platform Big Data and Machine Learning Fundamentals\" ODER gleichwertige Kenntnisse\n\n  \u00e2\u0097\u008f Grundkenntnisse in g\u00c3\u00a4ngigen Abfragesprachen wie SQL\n\n  \u00e2\u0097\u008f Erfahrung mit Datenmodellierung, Extraktion, Transformation und Ladeaktivit\u00c3\u00a4ten\n\n  \u00e2\u0097\u008f Kenntnisse im Entwickeln von Anwendungen mit einer g\u00c3\u00a4ngigen Programmiersprache wie Python\n\n  \u00e2\u0097\u008f Vertrautheit mit maschinellem Lernen und/oder Statistik\n\nHinweise zum Google-Konto:\n\u00e2\u0080\u00a2 Wenn Sie sich f\u00c3\u00bcr die kostenlose Testversion der Google Cloud Platform registrieren m\u00c3\u00b6chten, ben\u00c3\u00b6tigen Sie ein Google-/Gmail-Konto und eine Kreditkarte oder ein Bankkonto. In China sind Google-Dienste derzeit nicht verf\u00c3\u00bcgbar.\n\u00e2\u0080\u00a2 Kunden der Google Cloud Platform mit Rechnungsadresse in der Europ\u00c3\u00a4ischen Union (EU) oder Russland finden hilfreiche Hinweise zur Mehrwertsteuer unter: https://cloud.google.com/billing/docs/resources/vat-overview\n\u00e2\u0080\u00a2 Weitere FAQs zur kostenlosen Testversion der Google Cloud Platform finden Sie unter: https://cloud.google.com/free-trial/. Willkommen zum serverlosen maschinellen Lernen mit der Google Cloud Platform Modul\u00c2\u00a01: Einf\u00c3\u00bchrung in maschinelles Lernen Modul\u00c2\u00a02: ML-Modelle mit TensorFlow erstellen Modul\u00c2\u00a03: ML-Modelle mit Cloud ML Engine skalieren Modul\u00c2\u00a04: Feature Engineering     ", "Este curso intensivo de uma semana baseia-se nos cursos anteriores da especializa\u00c3\u00a7\u00c3\u00a3o Data Engineering on Google Cloud Platform. Por meio de videoaulas, demonstra\u00c3\u00a7\u00c3\u00b5es e laborat\u00c3\u00b3rios pr\u00c3\u00a1ticos, voc\u00c3\u00aa aprender\u00c3\u00a1 a criar e gerenciar clusters de computa\u00c3\u00a7\u00c3\u00a3o para executar jobs do Hadoop, Spark, Pig e/ou Hive no Google Cloud Platform.Voc\u00c3\u00aa tamb\u00c3\u00a9m aprender\u00c3\u00a1 a acessar v\u00c3\u00a1rias op\u00c3\u00a7\u00c3\u00b5es de armazenamento em nuvem dos clusters de computa\u00c3\u00a7\u00c3\u00a3o e integrar os recursos de machine learning do Google aos respectivos programas de an\u00c3\u00a1lise.\n \n Nos laborat\u00c3\u00b3rios pr\u00c3\u00a1ticos, voc\u00c3\u00aa criar\u00c3\u00a1 e gerenciar\u00c3\u00a1 os clusters do Dataproc usando o console da Web e a CLI e usar\u00c3\u00a1 o cluster para executar jobs do Spark e Pig. Depois voc\u00c3\u00aa criar\u00c3\u00a1 notebooks iPython que s\u00c3\u00a3o integrados ao BigQuery e ao armazenamento e utilizar\u00c3\u00a1 o Spark. Por fim, voc\u00c3\u00aa integrar\u00c3\u00a1 as APIs de machine learning \u00c3\u00a0 an\u00c3\u00a1lise de dados.\n \n Pr\u00c3\u00a9-requisitos\n \u00e2\u0080\u00a2 No\u00c3\u00a7\u00c3\u00b5es b\u00c3\u00a1sicas de Big Data e Machine Learning do Google Cloud Platform (ou experi\u00c3\u00aancia equivalente)\n \u00e2\u0080\u00a2 Algum conhecimento de Python M\u00c3\u00b3dulo 1: introdu\u00c3\u00a7\u00c3\u00a3o ao Cloud Dataproc M\u00c3\u00b3dulo 2: como executar jobs do Dataproc M\u00c3\u00b3dulo 3: como usar o GCP M\u00c3\u00b3dulo 4: como analisar dados n\u00c3\u00a3o estruturados Este curso intensivo de uma semana \u00c3\u00a9 uma continua\u00c3\u00a7\u00c3\u00a3o dos cursos anteriores da especializa\u00c3\u00a7\u00c3\u00a3o Data Engineering on Google Cloud Platform. Por meio de uma combina\u00c3\u00a7\u00c3\u00a3o de palestras em v\u00c3\u00addeo, demonstra\u00c3\u00a7\u00c3\u00b5es e laborat\u00c3\u00b3rios pr\u00c3\u00a1ticos, voc\u00c3\u00aa aprender\u00c3\u00a1 a criar e gerenciar clusters de computa\u00c3\u00a7\u00c3\u00a3o para executar jobs do Hadoop, Spark, Pig e/ou Hive no Google Cloud Platform. Voc\u00c3\u00aa tamb\u00c3\u00a9m ver\u00c3\u00a1 como acessar v\u00c3\u00a1rias op\u00c3\u00a7\u00c3\u00b5es do Cloud Storage a partir dos seus clusters de computa\u00c3\u00a7\u00c3\u00a3o, al\u00c3\u00a9m de integrar recursos de aprendizado de m\u00c3\u00a1quina do Google aos seus programas de an\u00c3\u00a1lise.    ", "Dieser einw\u00c3\u00b6chige Intensivkurs baut auf bisherigen Kursen der Spezialisierung Data Engineering on Google Cloud Platform auf. Mit Videovortr\u00c3\u00a4gen, Vorf\u00c3\u00bchrungen und Labs zur praktischen Anwendung lernen Sie, wie Sie Computecluster erstellen und verwalten, um Hadoop-, Spark-, Pig- und/oder Hive-Jobs auf der Google Cloud Platform auszuf\u00c3\u00bchren.Au\u00c3\u009ferdem wird erl\u00c3\u00a4utert, wie Sie auf verschiedene Cloudspeicherl\u00c3\u00b6sungen \u00c3\u00bcber deren Computecluster zugreifen und die Google-Funktionen f\u00c3\u00bcr maschinelles Lernen in deren Analyseprogramme einbinden.\n \n In den Labs zur praktischen Anwendung erstellen und verwalten Sie Dataproc-Cluster mit der Webkonsole und der Befehlszeile und f\u00c3\u00bchren \u00c3\u00bcber die Cluster Spark- und Pig-Jobs aus. Dann erstellen Sie iPython-Notebooks, die in BigQuery und Speicherplatz eingebunden werden k\u00c3\u00b6nnen und nutzen Spark. Zuletzt binden Sie die APIs f\u00c3\u00bcr maschinelles Lernen in die Datenanalyse ein.\n \n Voraussetzungen\n \u00e2\u0080\u00a2 Google Cloud Platform Big Data & Machine Learning Fundamentals (oder \u00c3\u0084hnliches)\n \u00e2\u0080\u00a2 Grundkenntnisse in Python Modul\u00c2\u00a01: Einf\u00c3\u00bchrung in Cloud Dataproc Modul\u00c2\u00a02: Dataproc-Jobs ausf\u00c3\u00bchren Modul\u00c2\u00a03: GCP nutzen Modul\u00c2\u00a04: Unstrukturierte Daten analysieren    ", "Willkommen im Kurs zur Kunst und Wissenschaft des maschinellen Lernens. In diesem Kurs eignen Sie sich die grundlegenden Kompetenzen rund um ML, gutes Urteilsverm\u00c3\u00b6gen und Experimentierfreudigkeit an, die f\u00c3\u00bcr die Feinabstimmung und Optimierung Ihres ML-Modells f\u00c3\u00bcr bestm\u00c3\u00b6gliche Leistung erforderlich sind.  \n\n\nAu\u00c3\u009ferdem informieren wir Sie \u00c3\u00bcber die zahlreichen Optimierungsmittel, die beim Trainieren eines Modells ins Spiel kommen. Sie werden diese zuerst manuell anpassen, um ihre Auswirkung auf die Leistung zu beobachten. Sobald Sie mit den Optimierungsmitteln \u00e2\u0080\u0093 auch bekannt als Hyperparameter \u00e2\u0080\u0093 vertraut sind, lernen Sie, wie Sie diese automatisch mithilfe der Cloud Machine Learning Engine auf der Google Cloud Platform anpassen k\u00c3\u00b6nnen. Einf\u00c3\u00bchrung Die Kunst des maschinellen Lernens Hyperparameter-Abstimmung Ein bisschen Wissenschaft Die Wissenschaft neuronaler Netzwerke Einbettungen Benutzerdefinierter Estimator Zusammenfassung \u00c3\u009cberblick \u00c3\u00bcber den Kurs mit Informationen zu den wesentlichen Zielen und Modulen. Zuerst erfahren Sie mehr \u00c3\u00bcber die Aspekte des maschinellen Lernens, f\u00c3\u00bcr die eine gewisse Intuition, ein gutes Urteilsverm\u00c3\u00b6gen und Experimentierfreudigkeit erforderlich sind. Wir nennen das die Kunst des maschinellen Lernens. Au\u00c3\u009ferdem informieren wir Sie \u00c3\u00bcber die zahlreichen Optimierungsmittel, die beim Trainieren eines Modells ins Spiel kommen. Sie werden diese manuell anpassen, um ihre Auswirkung auf die Leistung zu beobachten. In diesem Kurs erhalten Sie Informationen zur Kunst des maschinellen Lernens. Wir besprechen Aspekte des maschinellen Lernens, die Intuition, Urteilsverm\u00c3\u00b6gen und Experimentierfreudigkeit erfordern, um die richtige Balance zu finden und zu ermitteln, was tauglich ist. (So viel d\u00c3\u00bcrfen wir schon verraten: Perfekt wird es niemals sein!) In diesem Modul lernen Sie, wie Sie zwischen Parametern und Hyperparametern unterscheiden. Dann besprechen wir den traditionellen Rastersuchansatz und lernen, wie wir mit intelligenteren Algorithmen \u00c3\u00bcber diesen hinausgehen k\u00c3\u00b6nnen. Abschlie\u00c3\u009fend erfahren Sie, wie Sie mit Cloud ML Engine die Hyperparameter-Abstimmung automatisieren. In diesem Modul f\u00c3\u00bchren wir langsam die Wissenschaft neben der Kunst des maschinellen Lernens ein. Zuerst sprechen wir \u00c3\u00bcber die Durchf\u00c3\u00bchrung der Regularisierung f\u00c3\u00bcr Dichte, sodass wir einfachere, pr\u00c3\u00a4gnantere Modelle erhalten. Dann sprechen wir \u00c3\u00bcber die logistische Regression und lernen, wie wir die Leistung ermitteln. In diesem Modul befassen wir uns intensiv mit der Wissenschaft, insbesondere im Hinblick auf neuronale Netzwerke. In diesem Modul lernen Sie, wie Sie Einbettungen f\u00c3\u00bcr die Verwaltung von Sparse-Daten verwenden k\u00c3\u00b6nnen, damit ML-Modelle, die Sparse-Daten nutzen, weniger Speicher verbrauchen und schneller trainiert werden k\u00c3\u00b6nnen. Einbettungen sind auch eine M\u00c3\u00b6glichkeit f\u00c3\u00bcr die Dimensionsreduktion und dementsprechend eine M\u00c3\u00b6glichkeit, Modelle einfacher und besser verallgemeinerbar zu machen. In diesem Modul gehen wir \u00c3\u00bcber die Verwendung vorgefertigter Estimatoren hinaus und schreiben einen benutzerdefinierten Estimator. Dadurch erhalten Sie mehr Kontrolle \u00c3\u00bcber die Modellfunktion selbst. Sie wiederholen die Schl\u00c3\u00bcsselkonzepte, die wir im Kurs zur Kunst und Wissenschaft des maschinellen Lernens behandelt haben.", "Quer aprender a melhorar a precis\u00c3\u00a3o dos seus modelos de aprendizado de m\u00c3\u00a1quina? Que tal descobrir quais colunas de dados criam os atributos mais \u00c3\u00bateis? Damos as boas-vindas ao curso Feature Engineering no Google Cloud Platform. Falaremos sobre a diferen\u00c3\u00a7a entre atributos bons e ruins, al\u00c3\u00a9m de como pr\u00c3\u00a9-processar e transformar essas vari\u00c3\u00a1veis para o uso ideal nos seus modelos de aprendizado de m\u00c3\u00a1quina.\n\nNesse curso, voc\u00c3\u00aa far\u00c3\u00a1 laborat\u00c3\u00b3rios interativos para ver na pr\u00c3\u00a1tica como escolher atributos e fazer o pr\u00c3\u00a9-processamento no Google Cloud Platform. Nossos instrutores apresentar\u00c3\u00a3o as solu\u00c3\u00a7\u00c3\u00b5es de c\u00c3\u00b3digo em detalhes, que tamb\u00c3\u00a9m ser\u00c3\u00a3o disponibilizadas para usar como refer\u00c3\u00aancia nos seus pr\u00c3\u00b3prios projetos de aprendizado de m\u00c3\u00a1quina. Introdu\u00c3\u00a7\u00c3\u00a3o De dados brutos a atributos Pr\u00c3\u00a9-processamento e cria\u00c3\u00a7\u00c3\u00a3o de atributos Cruzamento de atributos TF Transform Resumo Quer aprender a melhorar a precis\u00c3\u00a3o dos seus modelos de aprendizado de m\u00c3\u00a1quina? Que tal descobrir quais colunas de dados criam os atributos mais \u00c3\u00bateis? Damos as boas vindas ao curso Feature Engineering no Google Cloud Platform. Falaremos sobre a diferen\u00c3\u00a7a entre atributos bons e ruins, al\u00c3\u00a9m de como pr\u00c3\u00a9-processar e transformar essas vari\u00c3\u00a1veis para o uso ideal nos seus modelos. A engenharia de atributos \u00c3\u00a9 geralmente a fase mais longa e dif\u00c3\u00adcil da cria\u00c3\u00a7\u00c3\u00a3o de projetos de aprendizado de m\u00c3\u00a1quina. No processo de engenharia de atributos, voc\u00c3\u00aa come\u00c3\u00a7a com dados brutos e usa seu conhecimento espec\u00c3\u00adfico para criar os atributos que far\u00c3\u00a3o seus algoritmos de aprendizado de m\u00c3\u00a1quina funcionar. Neste m\u00c3\u00b3dulo, veremos como criar um bom atributo e como represent\u00c3\u00a1-lo no seu modelo. Esta se\u00c3\u00a7\u00c3\u00a3o do m\u00c3\u00b3dulo aborda o pr\u00c3\u00a9-processamento e a cria\u00c3\u00a7\u00c3\u00a3o de atributos. Essas s\u00c3\u00a3o t\u00c3\u00a9cnicas de processamento de dados que podem ajudar voc\u00c3\u00aa a preparar um conjunto de atributos para um sistema de aprendizado de m\u00c3\u00a1quina. No aprendizado de m\u00c3\u00a1quina tradicional, o cruzamento de atributos n\u00c3\u00a3o tem uma fun\u00c3\u00a7\u00c3\u00a3o. No entanto, em m\u00c3\u00a9todos modernos, esse recurso \u00c3\u00a9 um item indispens\u00c3\u00a1vel do seu kit de ferramentas. Neste m\u00c3\u00b3dulo, voc\u00c3\u00aa aprender\u00c3\u00a1 como saber em que tipos de problemas o cruzamento de atributos pode ajudar o aprendizado de m\u00c3\u00a1quina. O TensorFlow Transform (tf.Transform) \u00c3\u00a9 uma biblioteca de dados de pr\u00c3\u00a9-processamento no TensorFlow. Ele pode ser usado para casos de pr\u00c3\u00a9-processamento que necessitem de uma transmiss\u00c3\u00a3o total de dados, como: - normaliza\u00c3\u00a7\u00c3\u00a3o de valores de entrada por m\u00c3\u00a9dia e stdev; - integraliza\u00c3\u00a7\u00c3\u00a3o de vocabul\u00c3\u00a1rio a partir da an\u00c3\u00a1lise de todos os exemplos de entrada dos valores; - cria\u00c3\u00a7\u00c3\u00a3o de intervalos para entradas com base na distribui\u00c3\u00a7\u00c3\u00a3o de dados observada. Neste m\u00c3\u00b3dulo, conheceremos os casos de uso do tf.Transform. Neste m\u00c3\u00b3dulo, faremos uma recapitula\u00c3\u00a7\u00c3\u00a3o dos principais pontos aprendidos em cada m\u00c3\u00b3dulo do curso Feature Engineering: como selecionar bons atributos, como realizar pr\u00c3\u00a9-processamento em grande escala, como usar cruzamentos de atributos e treinamento pr\u00c3\u00a1tico com o TensorFlow.", "Bienvenue dans cette formation sur l'art et la science du machine learning. Dans ce cours, vous allez acqu\u00c3\u00a9rir les comp\u00c3\u00a9tences essentielles que requiert le ML : intuition, discernement et capacit\u00c3\u00a9s d'exp\u00c3\u00a9rimentation. De cette fa\u00c3\u00a7on, vous pourrez ajuster pr\u00c3\u00a9cis\u00c3\u00a9ment vos mod\u00c3\u00a8les de ML et les am\u00c3\u00a9liorer pour obtenir des performances optimales.  \n\nCe cours vous pr\u00c3\u00a9sente les nombreux m\u00c3\u00a9canismes intervenant dans l'entra\u00c3\u00aenement d'un mod\u00c3\u00a8le. Vous commencerez par les ajuster manuellement pour observer leurs effets sur les performances du mod\u00c3\u00a8le. Une fois que vous serez familiaris\u00c3\u00a9 avec ces m\u00c3\u00a9canismes, \u00c3\u00a9galement appel\u00c3\u00a9s \"hyperparam\u00c3\u00a8tres\", vous apprendrez \u00c3\u00a0 les r\u00c3\u00a9gler automatiquement avec Cloud Machine Learning Engine sur Google Cloud Platform. Introduction L'art du ML R\u00c3\u00a9glages des hyperparam\u00c3\u00a8tres Un zeste de science La science des r\u00c3\u00a9seaux de neurones Repr\u00c3\u00a9sentations vectorielles continues Instance Estimator personnalis\u00c3\u00a9e R\u00c3\u00a9capitulatif Cette pr\u00c3\u00a9sentation du cours met l'accent sur les principaux objectifs et modules. Vous commencerez par d\u00c3\u00a9couvrir les aspects du machine learning qui requi\u00c3\u00a8rent de l'intuition, du discernement et des capacit\u00c3\u00a9s d'exp\u00c3\u00a9rimentation. Nous appelons cela \"l'art du ML\". Vous d\u00c3\u00a9couvrirez les nombreux m\u00c3\u00a9canismes intervenant dans l'entra\u00c3\u00aenement d'un mod\u00c3\u00a8le. Vous les ajusterez manuellement pour observer leurs effets sur les performances du mod\u00c3\u00a8le. Ce cours va vous permettre de d\u00c3\u00a9couvrir l'art du machine learning. Nous \u00c3\u00a9tudierons les aspects du machine learning qui requi\u00c3\u00a8rent de l'intuition, du discernement et des capacit\u00c3\u00a9s d'exp\u00c3\u00a9rimentation pour trouver le compromis id\u00c3\u00a9al qui permet d'obtenir des r\u00c3\u00a9sultats satisfaisants. (Nous vous pr\u00c3\u00a9venons d'ores et d\u00c3\u00a9j\u00c3\u00a0\u00c2\u00a0: il n'existe jamais de solution parfaite.) Dans ce module, vous allez apprendre \u00c3\u00a0 diff\u00c3\u00a9rencier les param\u00c3\u00a8tres des hyperparam\u00c3\u00a8tres. Nous \u00c3\u00a9tudierons ensuite l'approche traditionnelle de la recherche par grille, et d\u00c3\u00a9couvrirons comment aller plus loin en utilisant des algorithmes plus intelligents. Enfin, nous verrons dans quelle mesure Cloud ML\u00c2\u00a0Engine facilite l'automatisation des r\u00c3\u00a9glages d'hyperparam\u00c3\u00a8tres. Dans ce module, nous allons d\u00c3\u00a9couvrir que le machine learning est non seulement un art, mais \u00c3\u00a9galement une science. Nous commencerons par \u00c3\u00a9tudier comment proc\u00c3\u00a9der \u00c3\u00a0 une r\u00c3\u00a9gularisation \u00c3\u00a0 des fins de parcimonie, de fa\u00c3\u00a7on \u00c3\u00a0 obtenir des mod\u00c3\u00a8les plus simples et plus concis. Nous aborderons ensuite la r\u00c3\u00a9gression logistique, et apprendrons \u00c3\u00a0 \u00c3\u00a9valuer les performances. Dans ce module, nous allons plonger plus profond\u00c3\u00a9ment dans l'univers de la science, en particulier avec les r\u00c3\u00a9seaux de neurones. Dans ce module, vous allez apprendre \u00c3\u00a0 utiliser les repr\u00c3\u00a9sentations vectorielles continues pour g\u00c3\u00a9rer les donn\u00c3\u00a9es \u00c3\u00a9parses, et pour faire en sorte que les mod\u00c3\u00a8les de machine learning utilisant ce type de donn\u00c3\u00a9es consomment moins de m\u00c3\u00a9moire et s'entra\u00c3\u00aenent plus rapidement. Les repr\u00c3\u00a9sentations vectorielles continues permettent \u00c3\u00a9galement de r\u00c3\u00a9duire les dimensions, ce qui contribue \u00c3\u00a0 simplifier les mod\u00c3\u00a8les et \u00c3\u00a0 les rendre plus g\u00c3\u00a9n\u00c3\u00a9ralisables. Dans ce module, nous ne nous limiterons pas aux instances Estimator standardis\u00c3\u00a9es, et nous cr\u00c3\u00a9erons une classe Estimator personnalis\u00c3\u00a9e. Vous pourrez ainsi mieux contr\u00c3\u00b4ler la fonction m\u00c3\u00aame du mod\u00c3\u00a8le. Examinez les principaux concepts abord\u00c3\u00a9s dans le cours sur l'art et la science du ML.", "In the capstone, students will engage on a real world project requiring them to apply skills from the entire data science pipeline: preparing, organizing, and transforming data, constructing a model, and evaluating results.    Through a collaboration with Coursolve, each Capstone project is associated with partner stakeholders who have a vested interest in your results and are eager to deploy them in practice.  These projects will not be straightforward and the outcome is not prescribed -- you will need to tolerate ambiguity and negative results!  But we believe the experience will be rewarding and will better prepare you for data science projects in practice. Project A: Blight Fight Week 2: Derive a list of buildings Week 3: Construct a training dataset Week 4: Train and evaluate a simple model Week 5: Feature Engineering Week 6: Final Report In this project, you will build a model to predict when a building is likely to be condemned.  The data is real, the problem is real, and the impact is real.   You are given sets of incidents with location information; you need to use some assumptions to group these incidents by location to identify specific buildings. Construct a training set by associating each of your buildings with a ground truth label derived from the permit data. Use a trivial feature set to train and evaluate a simple model Derive additional features and retrain to improve the efficacy of your model. Enter your final report for grading.", "Apresentaremos o TensorFlow de baixo n\u00c3\u00advel e trabalharemos com os conceitos e APIs necess\u00c3\u00a1rios para gravar modelos de aprendizado de m\u00c3\u00a1quina distribu\u00c3\u00addos. Levando em considera\u00c3\u00a7\u00c3\u00a3o os modelos do TensorFlow, explicaremos como fazer o escalonamento horizontal do treinamento desse modelo e oferecer previs\u00c3\u00b5es de alto desempenho com o Cloud Machine Learning Engine.\n\nObjetivos do curso:\nCriar modelos de aprendizado de m\u00c3\u00a1quina no TensorFlow\nUsar as bibliotecas do TensorFlow para solucionar problemas num\u00c3\u00a9ricos\nResolver problemas e lidar com dificuldades comuns do c\u00c3\u00b3digo do TensorFlow\nUsar o tf_estimator para criar, treinar e avaliar modelos de aprendizado de m\u00c3\u00a1quina\nTreinar, implantar e produzir modelos de aprendizado de m\u00c3\u00a1quina em escala com o Cloud ML Engine Introdu\u00c3\u00a7\u00c3\u00a3o Principais componentes do TensorFlow Estimator API  Como ampliar os modelos do TensorFlow com CMLE Resumo A ferramenta que utilizaremos para criar programas de aprendizado de m\u00c3\u00a1quina \u00c3\u00a9 o TensorFlow, que ser\u00c3\u00a1 apresentado neste curso. No primeiro curso, voc\u00c3\u00aa aprendeu a formular problemas corporativos como problemas de aprendizado de m\u00c3\u00a1quina. No segundo, viu como a m\u00c3\u00a1quina funciona na pr\u00c3\u00a1tica e como criar conjuntos de dados que podem ser usados no aprendizado de m\u00c3\u00a1quina. Agora que seus dados est\u00c3\u00a3o prontos, voc\u00c3\u00aa pode come\u00c3\u00a7ar a criar programas de aprendizado de m\u00c3\u00a1quina. Apresentaremos os principais componentes do TensorFlow. Al\u00c3\u00a9m disso, voc\u00c3\u00aa aprender\u00c3\u00a1 na pr\u00c3\u00a1tica a criar programas de aprendizado de m\u00c3\u00a1quina. Voc\u00c3\u00aa poder\u00c3\u00a1 fazer a compara\u00c3\u00a7\u00c3\u00a3o e a grava\u00c3\u00a7\u00c3\u00a3o de avalia\u00c3\u00a7\u00c3\u00b5es pregui\u00c3\u00a7osas (lazy evaluation) e programas imperativos, trabalhar com gr\u00c3\u00a1ficos, sess\u00c3\u00b5es e vari\u00c3\u00a1veis e, por fim, depurar programas do TensorFlow. Neste m\u00c3\u00b3dulo, falaremos sobre a Estimator API. Agora, vamos aprender a treinar seu modelo do TensorFlow na infraestrutura gerenciada do GCP para treinamento e implanta\u00c3\u00a7\u00c3\u00a3o de modelo de aprendizado de m\u00c3\u00a1quina. Veja o resumo dos t\u00c3\u00b3picos do TensorFlow abordados at\u00c3\u00a9 agora no curso. Relembraremos o que foi discutido sobre o c\u00c3\u00b3digo do TensorFlow e a Estimator API, al\u00c3\u00a9m do escalonamento dos seus modelos com o Cloud Machine Learning Engine.", "This course teaches you the fundamentals of transforming clinical practice using predictive models. This course examines specific challenges and methods of clinical implementation, that clinical data scientists must be aware of when developing their predictive models.", "The course will teach you how to develop deep learning models using  Pytorch. The course will start with Pytorch's  tensors and Automatic differentiation package. Then each section will cover different models starting off with fundamentals such as Linear Regression, and logistic/softmax regression. Followed by  Feedforward deep neural networks, the role of different activation functions, normalization and dropout layers. Then Convolutional Neural Networks and Transfer learning will be covered. Finally, several other Deep learning methods will be covered.\nLearning Outcomes:\nAfter completing this course, learners will be able to:\n\u00e2\u0080\u00a2\texplain and apply their knowledge of Deep Neural Networks and related machine learning methods\n\u00e2\u0080\u00a2\tknow how to use Python libraries such as PyTorch  for Deep Learning applications \n\u00e2\u0080\u00a2\tbuild Deep Neural Networks using PyTorch Tensor and Datasets  Linear Regression Linear Regression PyTorch Way Multiple Input Output Linear Regression  Logistic Regression for Classification Softmax Rergresstion  Shallow Neural Networks Deep Networks  Convolutional Neural Network Peer Review           ", "Nach einem ersten \u00c3\u009cberblick \u00c3\u00bcber die Geschichte von ML erfahren Sie in diesem Kurs, weshalb heute mithilfe neuronaler Netzwerke viele Probleme so erfolgreich gel\u00c3\u00b6st werden k\u00c3\u00b6nnen. Wir erkl\u00c3\u00a4ren anschlie\u00c3\u009fend, wie Sie \u00c3\u00bcberwachtes Lernen zur Probleml\u00c3\u00b6sung einrichten und mithilfe des Gradientenverfahrens gute Ergebnisse erzielen. Dazu sind Datasets erforderlich, mit denen die Generalisierung m\u00c3\u00b6glich ist. In diesem Kurs zeigen wir Ihnen, wie Sie Datasets auf wiederholbare Weise erstellen, um Experimente zu erm\u00c3\u00b6glichen.\n\nKursziele:\nErkennen, warum Deep Learning derzeit beliebt ist\nModelle anhand von Verlustfunktionen und Leistungsmesswerten optimieren und auswerten\nH\u00c3\u00a4ufige Probleme rund um maschinelles Lernen minimieren\nWiederholbare und skalierbare Datasets zum Trainieren, Auswerten und Testen erstellen Einf\u00c3\u00bchrung ML in der Praxis Optimierung Generalisierung und Stichprobenerhebung Zusammenfassung Dieser Kurs vermittelt Ihnen ML-Basiswissen, damit Sie die Terminologie kennenlernen, die wir w\u00c3\u00a4hrend der Spezialisierung verwenden. Sie bekommen au\u00c3\u009ferdem praktische Tipps und Hinweise zu Fallstricken von ML-Fachleuten bei Google. Am Ende nehmen Sie den Code und das Fachwissen f\u00c3\u00bcr Ihre eigenen ML-Modelle mit. In diesem Modul stellen wir einige der wichtigsten Arten maschinellen Lernens vor und sehen uns noch einmal die Entwicklung ML an. Sie k\u00c3\u00b6nnen so schneller in die ML-Praxis einsteigen. In diesem Modul gehen wir die Optimierung von ML-Modellen durch. Jetzt ist es an der Zeit, eine recht seltsam anmutende Frage zu beantworten: Wann ist das genaueste ML-Modell nicht die beste Wahl? Wie wir im letzten Modul zur Optimierung angedeutet haben, erbringt ein Modell mit einem Verlustwert von 0 mit Ihrem Trainings-Dataset nicht automatisch auch mit realen Datasets ein gutes Ergebnis.  ", "This course teaches you the fundamentals of clinical natural language processing. In this course you will learn practical techniques for extracting information stored in text-based portions of electronic medical records. Introduction: Clinical Natural Language Processing Tools: Regular Expressions Techniques: Note Sections Techniques: Keyword Windows Practical Application: Identifying Aneurysm Location from Radiology Notes     ", "Dieser einw\u00c3\u00b6chige On-Demand-Schnellkurs baut auf dem Kurs \"Google Cloud Platform Big Data and Machine Learning Fundamentals\" auf. Mit einem Mix aus Lernvideos, Demos und praxisorientierten Labs lernen Sie, wie Sie mit Google Cloud Pub/Sub und Dataflow Streamingdaten-Pipelines erstellen und die gewonnenen Erkenntnisse f\u00c3\u00bcr Entscheidungen in Echtzeit nutzen k\u00c3\u00b6nnen. Sie lernen auch, Dashboards zu erstellen, die gezielt f\u00c3\u00bcr verschiedene Stakeholder-Gruppen Informationen abbilden.\n\nVoraussetzungen:\n\u00e2\u0080\u00a2 Abgeschlossener Kurs \"Google Cloud Platform Big Data and Machine Learning Fundamentals\" (oder gleichwertige Kenntnisse)\n\u00e2\u0080\u00a2 Kenntnisse in Java\n\nLernziele:\n\u00e2\u0080\u00a2 Anwendungsm\u00c3\u00b6glichkeiten f\u00c3\u00bcr Echtzeit-Streaminganalysen kennenlernen\n\u00e2\u0080\u00a2 Den asynchronen Messaging-Dienst Google Cloud Pub/Sub zur Verwaltung von Datenereignissen verwenden\n\u00e2\u0080\u00a2 Streaming-Pipelines schreiben und Transformationen ausf\u00c3\u00bchren, falls n\u00c3\u00b6tig\n\u00e2\u0080\u00a2 Die Streaming-Pipeline von beiden Seiten kennenlernen: Produktion und Nutzung\n\u00e2\u0080\u00a2 Dataflow, BigQuery und Cloud Pub/Sub f\u00c3\u00bcr Echtzeit-Streaming und -Analysen einsetzen Modul\u00c2\u00a01: Architektur von Streaminganalyse-Pipelines Modul\u00c2\u00a02: Variable Volumes aufnehmen Modul\u00c2\u00a03: Streaming-Pipelines implementieren Modul\u00c2\u00a04: Streaminganalysen und Dashboards Modul\u00c2\u00a05: Durchsatz- und Latenzanforderungen handhaben     ", "\u00e3\u0083\u009a\u00e3\u0082\u00bf\u00e3\u0083\u0090\u00e3\u0082\u00a4\u00e3\u0083\u0088\u00e8\u00a6\u008f\u00e6\u00a8\u00a1\u00e3\u0081\u00ae\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ab\u00e5\u00af\u00be\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e6\u0095\u00b0\u00e7\u00a7\u0092\u00e3\u0081\u00a7\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0082\u0092\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e5\u0087\u00a6\u00e7\u0090\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0084\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u00a2\u0097\u00e5\u00a4\u00a7\u00e3\u0081\u00ab\u00e5\u0090\u0088\u00e3\u0082\u008f\u00e3\u0081\u009b\u00e3\u0081\u00a6\u00e8\u0087\u00aa\u00e5\u008b\u0095\u00e7\u009a\u0084\u00e3\u0081\u00ab\u00e3\u0082\u00b9\u00e3\u0082\u00b1\u00e3\u0083\u00bc\u00e3\u0083\u00aa\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0082\u008b\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00ab\u00e9\u0096\u00a2\u00e5\u00bf\u0083\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e5\u00a0\u00b4\u00e5\u0090\u0088\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u009c\u00e3\u0081\u00b2 Data Insights \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e5\u008f\u0097\u00e8\u00ac\u009b\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0081\u008f\u00e3\u0081\u00a0\u00e3\u0081\u0095\u00e3\u0081\u0084\u00e3\u0080\u0082\n\n \u00e3\u0081\u0093\u00e3\u0081\u00ae 1 \u00e9\u0080\u00b1\u00e9\u0096\u0093\u00e3\u0081\u00ae\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e9\u0080\u009f\u00e7\u00bf\u0092\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e5\u008f\u0082\u00e5\u008a\u00a0\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081Google Cloud Platform \u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00a8\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0081\u00ab\u00e3\u0082\u0088\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e6\u0083\u0085\u00e5\u00a0\u00b1\u00e3\u0082\u0092\u00e5\u00be\u0097\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u00ac\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u00af\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u00bf\u00e3\u0083\u00a9\u00e3\u0082\u00af\u00e3\u0083\u0086\u00e3\u0082\u00a3\u00e3\u0083\u0096\u00e3\u0081\u00aa\u00e3\u0082\u00b7\u00e3\u0083\u008a\u00e3\u0083\u00aa\u00e3\u0082\u00aa\u00e3\u0081\u00a8\u00e3\u0083\u008f\u00e3\u0083\u00b3\u00e3\u0082\u00ba\u00e3\u0082\u00aa\u00e3\u0083\u00b3\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0081\u008c\u00e7\u0094\u00a8\u00e6\u0084\u008f\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u008a\u00e3\u0082\u008a\u00e3\u0080\u0081\u00e5\u008f\u0082\u00e5\u008a\u00a0\u00e8\u0080\u0085\u00e3\u0081\u00af Google BigQuery \u00e3\u0081\u00ae\u00e3\u0081\u0095\u00e3\u0081\u00be\u00e3\u0081\u0096\u00e3\u0081\u00be\u00e3\u0081\u00aa\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u00a6\u00e3\u0080\u0081\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e6\u0083\u0085\u00e5\u00a0\u00b1\u00e3\u0081\u00ae\u00e6\u008e\u00a2\u00e7\u00b4\u00a2\u00e3\u0080\u0081\u00e3\u0083\u009e\u00e3\u0082\u00a4\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e3\u0080\u0081\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0080\u0081\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0080\u0081\u00e6\u008a\u00bd\u00e5\u0087\u00ba\u00e3\u0082\u0092\u00e8\u00a1\u008c\u00e3\u0081\u0086\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u008c\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u009c\u00ac\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e8\u00aa\u00ad\u00e3\u0081\u00bf\u00e8\u00be\u00bc\u00e3\u0081\u00bf\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0081\u00ae\u00e5\u00ae\u009f\u00e8\u00a1\u008c\u00e3\u0080\u0081\u00e3\u0082\u00b9\u00e3\u0082\u00ad\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e5\u008c\u0096\u00e3\u0080\u0081\u00e3\u0083\u0091\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u009e\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e6\u009c\u0080\u00e9\u0081\u00a9\u00e5\u008c\u0096\u00e3\u0080\u0081\u00e3\u0082\u00af\u00e3\u0082\u00a8\u00e3\u0083\u00aa\u00e3\u0081\u00ae\u00e6\u0096\u0099\u00e9\u0087\u0091\u00e3\u0080\u0081\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0081\u00ae\u00e5\u008f\u00af\u00e8\u00a6\u0096\u00e5\u008c\u0096\u00e3\u0082\u0092\u00e6\u0089\u00b1\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n \u00e5\u0089\u008d\u00e6\u008f\u0090\u00e6\u009d\u00a1\u00e4\u00bb\u00b6\n\n \u00e6\u009c\u00ac\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e6\u009c\u0080\u00e5\u00a4\u00a7\u00e9\u0099\u0090\u00e3\u0081\u00ab\u00e6\u00b4\u00bb\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00ab\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00a7\u00e4\u00bb\u00a5\u00e4\u00b8\u008b\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e4\u00ba\u008b\u00e5\u0089\u008d\u00e3\u0081\u00ab\u00e5\u00ae\u008c\u00e4\u00ba\u0086\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e5\u00bf\u0085\u00e8\u00a6\u0081\u00e3\u0081\u008c\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\n\n\u00e2\u0080\u00a2 Exploring and Preparing your Data\n\u00e2\u0080\u00a2 Storing and Visualizing your Data\n\u00e2\u0080\u00a2 Architecture and Performance\n\n >>> \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ab\u00e7\u0099\u00bb\u00e9\u008c\u00b2\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0082\u0088\u00e3\u0081\u008f\u00e3\u0081\u0082\u00e3\u0082\u008b\u00e8\u00b3\u00aa\u00e5\u0095\u008f\u00e3\u0081\u00ab\u00e8\u00a8\u0098\u00e8\u00bc\u0089\u00e3\u0081\u0095\u00e3\u0082\u008c\u00e3\u0081\u00a6\u00e3\u0081\u0084\u00e3\u0082\u008b\u00e3\u0081\u00a8\u00e3\u0081\u008a\u00e3\u0082\u008a Qwiklabs \u00e3\u0081\u00ae\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e8\u00a6\u008f\u00e7\u00b4\u0084\u00e3\u0081\u00ab\u00e5\u0090\u008c\u00e6\u0084\u008f\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0081\u0093\u00e3\u0081\u00a8\u00e3\u0081\u00ab\u00e3\u0081\u00aa\u00e3\u0082\u008a\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e3\u0081\u00af\u00e3\u0080\u0081https://qwiklabs.com/terms_of_service \u00e3\u0082\u0092\u00e3\u0081\u0094\u00e8\u00a6\u00a7\u00e3\u0081\u008f\u00e3\u0081\u00a0\u00e3\u0081\u0095\u00e3\u0081\u0084\u00e3\u0080\u0082 <<< \u00e3\u0081\u00af\u00e3\u0081\u0098\u00e3\u0082\u0081\u00e3\u0081\u00ab \u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081 \u00e4\u00ba\u008b\u00e5\u0089\u008d\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e6\u00b8\u0088\u00e3\u0081\u00bf\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092 API BigQuery \u00e3\u0081\u00a7\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u0087\u00e3\u0083\u00bc\u00e3\u0082\u00bf\u00e3\u0082\u00bb\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b BigQuery \u00e3\u0081\u00a7\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b \u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e7\u00b5\u0082\u00e4\u00ba\u0086\u00e3\u0081\u00ab\u00e3\u0081\u0082\u00e3\u0081\u009f\u00e3\u0081\u00a3\u00e3\u0081\u00a6\u00e3\u0081\u00ae\u00e3\u0081\u00be\u00e3\u0081\u00a8\u00e3\u0082\u0081 Data Insights \u00e5\u00b0\u0082\u00e9\u0096\u0080\u00e8\u00ac\u009b\u00e5\u00ba\u00a7\u00e3\u0081\u00ae\u00e6\u009c\u0080\u00e5\u00be\u008c\u00e3\u0081\u00ae\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0081\u0094\u00e5\u008f\u0082\u00e5\u008a\u00a0\u00e3\u0081\u0084\u00e3\u0081\u009f\u00e3\u0081\u00a0\u00e3\u0081\u008d\u00e3\u0081\u0082\u00e3\u0082\u008a\u00e3\u0081\u008c\u00e3\u0081\u00a8\u00e3\u0081\u0086\u00e3\u0081\u0094\u00e3\u0081\u0096\u00e3\u0081\u0084\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00ae\u00e3\u0083\u0088\u00e3\u0083\u0094\u00e3\u0083\u0083\u00e3\u0082\u00af\u00e3\u0081\u00a8\u00e3\u0080\u0081\u00e3\u0083\u00a9\u00e3\u0083\u009c\u00e3\u0081\u00a7\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0083\u0097\u00e3\u0083\u00a9\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0083\u0095\u00e3\u0082\u00a9\u00e3\u0083\u00bc\u00e3\u0083\u00a0\u00e3\u0081\u00ae\u00e6\u00a6\u0082\u00e8\u00a6\u0081\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ae\u009a\u00e7\u00be\u00a9\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u008c\u00e3\u0081\u00a9\u00e3\u0081\u00ae\u00e3\u0082\u0088\u00e3\u0081\u0086\u00e3\u0081\u00ab\u00e3\u0083\u0093\u00e3\u0082\u00b8\u00e3\u0083\u008d\u00e3\u0082\u00b9\u00e3\u0081\u00ab\u00e3\u0083\u00a1\u00e3\u0083\u00aa\u00e3\u0083\u0083\u00e3\u0083\u0088\u00e3\u0082\u0092\u00e3\u0082\u0082\u00e3\u0081\u009f\u00e3\u0082\u0089\u00e3\u0081\u0099\u00e3\u0081\u008b\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00aa\u00ac\u00e6\u0098\u008e\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u0092\u00e4\u00bd\u00bf\u00e7\u0094\u00a8\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e3\u0083\u0087\u00e3\u0083\u00a2\u00e3\u0082\u0092\u00e3\u0081\u0084\u00e3\u0081\u008f\u00e3\u0081\u00a4\u00e3\u0081\u008b\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u00ae\u00e4\u00b8\u00bb\u00e3\u0081\u00aa\u00e7\u0094\u00a8\u00e8\u00aa\u009e\u00ef\u00bc\u0088\u00e3\u0082\u00a4\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0082\u00bf\u00e3\u0083\u00b3\u00e3\u0082\u00b9\u00e3\u0080\u0081\u00e7\u0089\u00b9\u00e5\u00be\u00b4\u00e3\u0080\u0081\u00e3\u0083\u00a9\u00e3\u0083\u0099\u00e3\u0083\u00ab\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00ef\u00bc\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081Cloud Datalab \u00e5\u0086\u0085\u00e3\u0081\u00a7\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e4\u00ba\u008b\u00e5\u0089\u008d\u00e3\u0081\u00ab\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0081\u008a\u00e3\u0082\u0088\u00e3\u0081\u00b3\u00e3\u0083\u0088\u00e3\u0083\u00ac\u00e3\u0083\u00bc\u00e3\u0083\u008b\u00e3\u0083\u00b3\u00e3\u0082\u00b0\u00e6\u00b8\u0088\u00e3\u0081\u00bf\u00e3\u0081\u00ae\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00ef\u00bc\u0088\u00e7\u0094\u00bb\u00e5\u0083\u008f\u00e8\u00aa\u008d\u00e8\u00ad\u0098\u00e3\u0082\u0084\u00e6\u0084\u009f\u00e6\u0083\u0085\u00e5\u0088\u0086\u00e6\u009e\u0090\u00e3\u0081\u00aa\u00e3\u0081\u00a9\u00ef\u00bc\u0089\u00e3\u0081\u00ab\u00e3\u0081\u00a4\u00e3\u0081\u0084\u00e3\u0081\u00a6\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082  \u00e3\u0081\u0093\u00e3\u0081\u00ae\u00e3\u0083\u00a2\u00e3\u0082\u00b8\u00e3\u0083\u00a5\u00e3\u0083\u00bc\u00e3\u0083\u00ab\u00e3\u0081\u00a7\u00e3\u0081\u00af\u00e3\u0080\u0081BigQuery \u00e5\u0086\u0085\u00e3\u0081\u00a7\u00e7\u009b\u00b4\u00e6\u008e\u00a5\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0082\u0092\u00e4\u00bd\u009c\u00e6\u0088\u0090\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e6\u0096\u00b9\u00e6\u00b3\u0095\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e6\u0096\u00b0\u00e3\u0081\u0097\u00e3\u0081\u0084\u00e6\u00a7\u008b\u00e6\u0096\u0087\u00e3\u0082\u0092\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e6\u00a9\u009f\u00e6\u00a2\u00b0\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0083\u00a2\u00e3\u0083\u0087\u00e3\u0083\u00ab\u00e3\u0081\u00ae\u00e6\u00a7\u008b\u00e7\u00af\u0089\u00e3\u0080\u0081\u00e8\u00a9\u0095\u00e4\u00be\u00a1\u00e3\u0080\u0081\u00e3\u0083\u0086\u00e3\u0082\u00b9\u00e3\u0083\u0088\u00e3\u0081\u00ae\u00e3\u0083\u0095\u00e3\u0082\u00a7\u00e3\u0083\u00bc\u00e3\u0082\u00ba\u00e3\u0082\u0092\u00e8\u00a9\u00b3\u00e3\u0081\u0097\u00e3\u0081\u008f\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0099\u00e3\u0080\u0082 \u00e3\u0081\u0093\u00e3\u0082\u008c\u00e3\u0081\u00a7\u00e7\u00b5\u0082\u00e4\u00ba\u0086\u00e3\u0081\u00a7\u00e3\u0081\u0099\u00e3\u0080\u0082\u00e3\u0082\u00b3\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0081\u00a7\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0081\u009f\u00e5\u0086\u0085\u00e5\u00ae\u00b9\u00e3\u0082\u0092\u00e5\u00be\u00a9\u00e7\u00bf\u0092\u00e3\u0081\u0097\u00e3\u0080\u0081\u00e5\u00ad\u00a6\u00e7\u00bf\u0092\u00e3\u0082\u0092\u00e7\u00b6\u0099\u00e7\u00b6\u009a\u00e3\u0081\u0099\u00e3\u0082\u008b\u00e3\u0081\u009f\u00e3\u0082\u0081\u00e3\u0081\u00ab\u00e5\u0088\u00a9\u00e7\u0094\u00a8\u00e3\u0081\u00a7\u00e3\u0081\u008d\u00e3\u0082\u008b\u00e3\u0083\u00aa\u00e3\u0082\u00bd\u00e3\u0083\u00bc\u00e3\u0082\u00b9\u00e3\u0082\u0092\u00e7\u00a2\u00ba\u00e8\u00aa\u008d\u00e3\u0081\u0097\u00e3\u0081\u00be\u00e3\u0081\u0097\u00e3\u0082\u0087\u00e3\u0081\u0086\u00e3\u0080\u0082", "Dies ist eine Einf\u00c3\u00bchrung in die Grundlagen von TensorFlow. Darin werden die Konzepte und APIs erl\u00c3\u00a4utert, die Sie zum Schreiben verteilter Modelle f\u00c3\u00bcr maschinelles Lernen ben\u00c3\u00b6tigen. Au\u00c3\u009ferdem wird anhand eines TensorFlow-Modells erkl\u00c3\u00a4rt, wie Sie Modelle in gro\u00c3\u009fem Umfang trainieren und mit Cloud Machine Learning Engine effektive Vorhersagen treffen k\u00c3\u00b6nnen.\n\nLernziele:\nModelle f\u00c3\u00bcr maschinelles Lernen in TensorFlow erstellen\nDiverse Herausforderungen mit TensorFlow-Bibliotheken l\u00c3\u00b6sen\nG\u00c3\u00a4ngige Codefehler in TensorFlow beheben\nMit tf.estimator ein ML-Modell erstellen, trainieren und bewerten\nML-Modelle im gro\u00c3\u009fen Umfang mit Cloud ML Engine trainieren, bereitstellen und in der Produktion verwenden Einf\u00c3\u00bchrung Kernkonzept von TensorFlow Estimator API TensorFlow-Modelle mit CMLE skalieren Zusammenfassung Zum Schreiben von Programmen f\u00c3\u00bcr maschinelles Lernen verwenden wir TensorFlow. Dieser Kurs bietet daher eine Einf\u00c3\u00bchrung in das Tool. Im ersten Kurs haben Sie erfahren, wie Sie gesch\u00c3\u00a4ftliche Herausforderungen in Aufgaben f\u00c3\u00bcr das maschinelle Lernen umformulieren. Sie haben gelernt, wie maschinelles Lernen in der Praxis funktioniert und wie Sie verwertbare Datasets erstellen. Nachdem Sie die ben\u00c3\u00b6tigten Daten erfasst haben, k\u00c3\u00b6nnen Sie mit dem Schreiben von ML-Programmen beginnen. Dies ist eine Einf\u00c3\u00bchrung in die Hauptkomponenten von TensorFlow und Sie lernen in praktischen \u00c3\u009cbungen, wie Sie ein ML-Programm erstellen. Au\u00c3\u009ferdem vergleichen und schreiben Sie Programme f\u00c3\u00bcr verz\u00c3\u00b6gerte Bewertungen sowie erforderliche Programme, arbeiten mit Graphen, Sitzungen und Variablen und beheben schlie\u00c3\u009flich Fehler in TensorFlow-Programmen.\n In diesem Modul wird die Estimator\u00c2\u00a0API erl\u00c3\u00a4utert. In diesem Modul erfahren Sie, wie Sie Ihr TensorFlow-Modell in der verwalteten Infrastruktur der GCP durch maschinelles Lernen trainieren und bereitstellen. Hier fassen wir die bisher in diesem Kurs behandelten TensorFlow-Themen zusammen. Wir gehen noch einmal auf den Kerncode von TensorFlow und die Estimator\u00c2\u00a0API ein. Den Abschluss bildet die Skalierung Ihrer Modelle f\u00c3\u00bcr maschinelles Lernen mit Cloud Machine Learning Engine.", "Power and Sample Size for Longitudinal and Multilevel Study Designs, a five-week, fully online course covers innovative, research-based power and sample size methods, and software for multilevel and longitudinal studies.  The power and sample size methods and software taught in this course can be used for any health-related, or more generally, social science-related (e.g., educational research) application.  All examples in the course videos are from real-world studies on behavioral and social science employing multilevel and longitudinal designs. The course philosophy is to focus on the conceptual knowledge to conduct power and sample size methods. The goal of the course is to teach and disseminate methods for accurate sample size choice, and ultimately, the creation of a power/sample size analysis for a relevant research study in your professional context. \n\nPower and sample size selection is one of the most important ethical questions researchers face. Interventional studies that are too large expose human volunteer research participants to possible, and needless, harm from research. Interventional studies that are too small will fail to reach their scientific objective, again bringing possible harm to research participants, without the possibility of concomitant gain from the increase in knowledge. For observational studies in which there are no possible harms to the participants, such as observational studies, proper power ensures good stewardship of both time and money.\n\nMost National Institutes of Health (NIH) study sections will only fund a grant if the grantee has written a compelling and accurate power and sample size analysis. The Institute of Education Sciences (IES), the statistics, research, and evaluation arm of the U.S. Department of Education, also offers competitive grants requiring a compelling and accurate power and sample size analysis (Goal 3: Efficacy and Replication and Goal 4: Effectiveness/Scale-Up). \n\nAt the end of the online course, learners will be able to: \n\u00e2\u0080\u00a2\tUse a framework and strategy for study planning \n\u00e2\u0080\u00a2\tWrite study aims as testable hypotheses\n\u00e2\u0080\u00a2\tDescribe a longitudinal and multilevel study design\n\u00e2\u0080\u00a2\tWrite a statistical analysis plan \n\u00e2\u0080\u00a2\tPlan a sampling design for subgroups, e.g. racial and ethnic\n\u00e2\u0080\u00a2\tDemonstrate the feasibility of recruitment\n\u00e2\u0080\u00a2\tDescribe expected missing data and dropout\n\u00e2\u0080\u00a2\tWrite a power and sample size analysis that is aligned with the planned statistical analysis\n\nThis is a five-week intensive and interactive online course. We will use a mix of instructional videos, software demonstration videos, online discussion forums, online readings, quizzes, exercise assignments, and peer-review assignments. The final course project is a peer-reviewed research study you design for future power or sample size analysis. Week 1: Introduction to Multilevel and Longitudinal Designs Week 2: Foundations of Complex Multilevel and Longitudinal Designs Week 3: Model Assumptions, Alignment, Missing Data, and Dropout Week 4: Inputs to Analysis, Recruitment Feasibility, and Multiple Aims Week 5: Ethics and Using Power and Sample Size Analysis to Get Funded This first module introduces all course participants to the online course, its structure, its learning objectives, and your peers within the course. As noted, the course is composed of multiple activities to reach the learning objectives. Next, we review basic statistical concepts (e.g., hypothesis testing), and explore the fundamentals of both multi-level and longitudinal studies.  Conceptual knowledge is covered to provide a framework for analyzing and synthesizing research study designs. This module lays a foundation for subsequent learning. The module concludes with an introduction to the GLIMMPSE software for conducting your own power and sample size analyses. You will walk through a fully guided exercise problem to solve for power for a single level cluster design.  In the second module, we are going to dive into the many facets of research design, and important considerations related to power and sample size analysis. Specifically, we will examine between, within, and interactions; type 1 error, type 2 error, and power; and standard deviation, variance, and correlation structure.  We will explore the appropriate statistical tests for use in specific models, criteria for evaluating these different tests, and how to choose an appropriate test for a data analysis problem. Finally, we will note how clusters of observations or multivariate designs can induce correlation. This module provides the details for specifying research designs, and the beginning steps in aligning the research design to sample size and power analysis. The module concludes with summarizing research designs for GLIMMPSE software. You will walk through a guided exercise problem to solve for sample size analysis for a longitudinal study.  The third module includes a wide variety of topics related to power and sample size analysis. First, we examine multivariate and mixed models, their assumptions, and how this assumption impact power.  After we focus on aligning the features of data analysis and power analysis as well as the consequences of misalignment. Then we focus on missing data from sources like participant drop-out, machine failures or data entry errors; and how to account for missing data by adjusting your sample size. This module highlights several important features to consider in power and sample size analysis. To conclude the module, you will walk through an exercise problem to solve for power for a multilevel study independently.  Our emphasis in the fourth module includes the many sources of inputs for power and sample size analysis from the empirical literature, internal pilot studies, planned pilot studies, and computer simulations. Each of these approaches is discussed in detail in relation to power and sample size analysis, including the overall benefits and challenges associated with each approach.  Next we talk about recruitment feasibility and its critical importance to sample size calculations by discussing some key factors such as health, socioeconomic, and demographic factors that can be predictive of recruitment difficulty. Next, we deal with research studies that address multiple aims (e.g., hypotheses) and how to address this situation in your sample size analysis.  Lastly, you will walk through a fully independent exercise problem to solve for sample size analysis for a multilevel study with longitudinal repeated measures. The fifth and final module first introduces the ethics of sample size analysis, including overpowered and underpowered research studies and the importance of early planning. Next, we walk through the process of structuring a sample size section of a proposal in a grant application.  Then we dive into power curves again and discuss how to decide to incorporate a graphic to best tell your story. After, we explore subgroup analyses such as gender or race, and how to incorporate these design features into a power and sample size analysis. We close our last lecture on searching and applying for funding opportunities, and how a clear design and analysis plan improves your chances for funding. As this is our last module, you will walk through a fully independent exercise problem to solve for sample size analysis for a planned subgroup analysis. You will review at minimum two of your peers\u00e2\u0080\u0099 research design and sample size analyses documents, and finally, complete the final exam in the course. ", "Seja bem-vindo \u00c3\u00a0 arte e \u00c3\u00a0 ci\u00c3\u00aancia do aprendizado de m\u00c3\u00a1quina. Neste curso, voc\u00c3\u00aa aprender\u00c3\u00a1 as habilidades b\u00c3\u00a1sicas de intui\u00c3\u00a7\u00c3\u00a3o, bom senso e experimenta\u00c3\u00a7\u00c3\u00a3o em aprendizado de m\u00c3\u00a1quina para ajustar e otimizar seus modelos com o objetivo de conseguir o melhor desempenho.    \n\nAl\u00c3\u00a9m disso, voc\u00c3\u00aa conhecer\u00c3\u00a1 as ferramentas envolvidas no processo de treinamento. Primeiro, voc\u00c3\u00aa far\u00c3\u00a1 ajustes manuais para observar os efeitos no desempenho do modelo. Depois de se familiarizar com essas ferramentas, tamb\u00c3\u00a9m conhecidas como hiperpar\u00c3\u00a2metros, voc\u00c3\u00aa aprender\u00c3\u00a1 a ajust\u00c3\u00a1-las automaticamente usando o Cloud Machine Learning Engine no Google Cloud Platform. Introdu\u00c3\u00a7\u00c3\u00a3o A arte do aprendizado de m\u00c3\u00a1quina Ajuste de hiperpar\u00c3\u00a2metros Uma pitada de ci\u00c3\u00aancia A ci\u00c3\u00aancia das redes neurais Embeddings Estimator personalizado Resumo Vis\u00c3\u00a3o geral do curso destacando os principais objetivos e m\u00c3\u00b3dulos. Primeiro, voc\u00c3\u00aa conhecer\u00c3\u00a1 os aspectos do aprendizado de m\u00c3\u00a1quina que requerem alguma intui\u00c3\u00a7\u00c3\u00a3o, bom senso e experimenta\u00c3\u00a7\u00c3\u00a3o. Essa \u00c3\u00a9 a arte do aprendizado de m\u00c3\u00a1quina. Voc\u00c3\u00aa ver\u00c3\u00a1 quais s\u00c3\u00a3o as ferramentas envolvidas no processo de treinamento e far\u00c3\u00a1 ajustes manuais para observar os efeitos no desempenho do modelo. Neste curso, voc\u00c3\u00aa aprender\u00c3\u00a1 sobre a arte do aprendizado de m\u00c3\u00a1quina. Veremos os aspectos dessa tecnologia que requerem intui\u00c3\u00a7\u00c3\u00a3o, bom senso e experimenta\u00c3\u00a7\u00c3\u00a3o para encontrar o equil\u00c3\u00adbrio correto e definir o que \u00c3\u00a9 bom o bastante. Alerta de spoiler: perfei\u00c3\u00a7\u00c3\u00a3o n\u00c3\u00a3o existe! Neste m\u00c3\u00b3dulo, voc\u00c3\u00aa aprender\u00c3\u00a1 a diferen\u00c3\u00a7a entre par\u00c3\u00a2metros e hiperpar\u00c3\u00a2metros. Em seguida, discutiremos as abordagens tradicionais de pesquisa de grade e aprenderemos a ir al\u00c3\u00a9m dela, com algoritmos mais inteligentes. Por fim, voc\u00c3\u00aa ver\u00c3\u00a1 como \u00c3\u00a9 conveniente automatizar o ajuste de hiperpar\u00c3\u00a2metros no Cloud ML Engine. Neste m\u00c3\u00b3dulo, come\u00c3\u00a7aremos a introduzir a ci\u00c3\u00aancia envolvida no aprendizado de m\u00c3\u00a1quina. Primeiro, falaremos sobre como regularizar a esparsidade para ter modelos mais simples e concisos. Em seguida, falaremos sobre regress\u00c3\u00a3o log\u00c3\u00adstica e aprenderemos como determinar o desempenho. Neste m\u00c3\u00b3dulo, vamos nos aprofundar na ci\u00c3\u00aancia. Mais especificamente, nas redes neurais. Neste m\u00c3\u00b3dulo, voc\u00c3\u00aa aprender\u00c3\u00a1 a usar embeddings pra gerenciar dados esparsos, al\u00c3\u00a9m de criar modelos de aprendizado de m\u00c3\u00a1quina que usam esses dados, consomem menos mem\u00c3\u00b3ria e s\u00c3\u00a3o treinados mais rapidamente. Embeddings tamb\u00c3\u00a9m s\u00c3\u00a3o uma maneira de fazer redu\u00c3\u00a7\u00c3\u00b5es dimensionais e, dessa forma, criar modelos mais simples e generaliz\u00c3\u00a1veis. Neste m\u00c3\u00b3dulo, criaremos um Estimator personalizado para ir al\u00c3\u00a9m do uso de Estimators fixos. Ao escrever um Estimator personalizado, voc\u00c3\u00aa poder\u00c3\u00a1 ter mais controle sobre a fun\u00c3\u00a7\u00c3\u00a3o do modelo. Revise os principais conceitos abordados no curso \u00e2\u0080\u009cArte e ci\u00c3\u00aancia do aprendizado de m\u00c3\u00a1quina\u00e2\u0080\u009d.", "In this capstone, learners will apply their deep learning knowledge and expertise to a real world challenge.  They will use a library of their choice to develop and test a deep learning model. They will load and pre-process data for a real problem, build the model and validate it. Learners  will then present a project report to demonstrate the validity of their model and their proficiency in the field of Deep Learning.\nLearning Outcomes:\n\u00e2\u0080\u00a2\tdetermine what kind of deep learning method to use in which situation\n\u00e2\u0080\u00a2\tknow how to build a deep learning model to solve a real problem \n\u00e2\u0080\u00a2\tmaster the process of creating  a deep learning pipeline \n\u00e2\u0080\u00a2\tapply knowledge of deep learning to improve models using real data\n\u00e2\u0080\u00a2\tdemonstrate ability to present and communicate outcomes of deep learning projects", "Are you ready to close the loop on your Big Data skills? Do you want to apply all your knowledge you got from the previous courses in practice? Finally, in the Capstone project, you will integrate all the knowledge acquired earlier to build a real application leveraging the power of Big Data.\n\nYou will be given a task to combine data from different sources of different types (static distributed dataset, streaming data, SQL or NoSQL storage). Combined, this data will be used to build a predictive model for a financial market (as an example). First, you design a system from scratch and share it with your peers to get valuable feedback. Second, you can make it public, so get ready to receive the feedback from your service users. Real-world experience without any 3G-glasses or mock interviews. Untitled Module ", "The majority of data in the world is unlabeled and unstructured. Shallow neural networks cannot easily capture relevant structure in, for instance, images, sound, and textual data. Deep networks are capable of discovering hidden structures within this type of data. In this course you\u00e2\u0080\u0099ll use TensorFlow library to apply deep learning to different data types in order to solve real world problems.\nLearning Outcomes:\nAfter completing this course, learners will be able to:\n\u00e2\u0080\u00a2\texplain foundational TensorFlow concepts such as the main functions, operations and the execution pipelines. \n\u00e2\u0080\u00a2\tdescribe how TensorFlow can be used in curve fitting, regression, classification and minimization of error functions. \n\u00e2\u0080\u00a2\tunderstand different types of Deep Architectures, such as Convolutional Networks, Recurrent Networks and Autoencoders.\n\u00e2\u0080\u00a2\tapply TensorFlow for backpropagation to tune the weights and biases while the Neural Networks are being trained. Introduction Supervised Learning Models Supervised Learning Models (Cont'd) Unsupervised Deep Learning Models Unsupervised Deep Learning Models (Cont'd) In this module, you will learn about TensorFlow, and use it to create Linear and Logistic Regression models.\nYou will also learn about the fundamentals of Deep Learning. In this module, you will learn about about Convolutional Neural Networks, and the building blocks of a convolutional neural network, such as convolution and feature learning. You will also learn about the popular MNIST database. Finally, you will learn how to build a Multi-layer perceptron and convolutional neural networks in Python and using TensorFlow. In this module, you will learn about the recurrent neural network model, and special type of a recurrent neural network, which is the Long Short-Term Memory model. Also, you will learn about the Recursive Neural Tensor Network theory, and finally, you will apply recurrent neural networks to language modelling.  In this module, you will learn about the applications of unsupervised learning. You will learn about Restricted Boltzmann Machines (RBMs), and how to train an RBM. Finally, you will apply Restricted Boltzmann Machines to build a recommendation system. In this module, you will mainly learn about autoencoders and their architecture.\n", "In this final course, you will put together your knowledge from Courses 1, 2 and 3 to implement a complete RL solution to a problem. This capstone will let you see how each component---problem formulation, algorithm selection, parameter selection and representation design---fits together into a complete solution, and how to make appropriate choices when deploying RL in the real world. This project will require you to implement both the environment to stimulate your problem, and a control agent with Neural Network function approximation. In addition, you will conduct a scientific study of your learning system to develop your ability to assess the robustness of RL agents. To use RL in the real world, it is critical to (a) appropriately formalize the problem as an MDP, (b) select appropriate algorithms, (c ) identify what choices in your implementation will have large impacts on performance and (d) validate the expected behaviour of your algorithms. This capstone is valuable for anyone who is planning on using RL to solve real problems.\n\nTo be successful in this course, you will need to have completed Courses 1, 2, and 3 of this Specialization or the equivalent.\n\nBy the end of this course, you will be able to: \n\nComplete an RL solution to a problem, starting from problem formulation, appropriate algorithm selection and implementation and empirical study into the effectiveness of the solution. Welcome to the Final Capstone Course!  Milestone 1: Formalize Word Problem as MDP Milestone 2: Choosing The Right Algorithm Milestone 3: Identify Key Performance Parameters Milestone 4: Implement Your Agent  Milestone 5: Submit Your Parameter Study! Welcome to the final capstone course of the Reinforcement Learning Specialization!! This week you will read a description of a problem, and translate it into an MDP. You will complete skeleton code for this environment, to obtain a complete MDP for use in this capstone project.  This week you will select from three algorithms, to learn a policy for the environment. You will reflect on and discuss the appropriateness of each algorithm for this environment.   This week you will identify key parameters that affect the performance of your agent. The goal is to understand the space of options, to later enable you to choose which parameter you will investigate in-depth for your agent. This week, you will implement your agent using Expected Sarsa or Q-learning with RMSProp and Neural Networks. To use NNs, you will have to use a more careful stepsize selection strategy, which is why you will use RMSProp. You will also verify the correctness of your agent. This week you will identify a parameter to study, for your agent. Once you select the parameter to study, we will provide you with a range of values and specific values for other parameters. You will write a script to run your agent and environment on the set of parameters, to determine performance across these parameters. You will gain insight into the impact of parameters on agent performance. You will also get to visualize the agents that you learn. Your parameter study will consist of an array of values that we will check for correctness. ", "This course prepares you to deal with advanced clinical data science topics and techniques including temporal and research quality analysis.", "Sie m\u00c3\u00b6chten erfahren, wie Sie die Genauigkeit Ihrer maschinellen Lernmodelle verbessern oder wie Sie herausfinden, welche Datenspalten die n\u00c3\u00bctzlichsten Funktionen ergeben? Willkommen zum Feature Engineering mit der Google Cloud Platform. Wir er\u00c3\u00b6rtern in diesem Kurs n\u00c3\u00bctzliche und nutzlose Funktionen und wie Sie diese f\u00c3\u00bcr die optimale Nutzung in Ihren maschinellen Lernmodellen vorverarbeiten und umwandeln.\n\nIn praktischen, interaktiven Labs lernen Sie, Funktionen auszuw\u00c3\u00a4hlen und mit der Google Cloud Platform vorzuverarbeiten. Unsere Kursleiter pr\u00c3\u00a4sentieren Ihnen die Code-L\u00c3\u00b6sungen, die zu Referenzzwecken auch \u00c3\u00b6ffentlich gemacht werden, w\u00c3\u00a4hrend Sie an Ihren eigenen zuk\u00c3\u00bcnftigen ML-Projekten arbeiten. Einf\u00c3\u00bchrung Rohdaten in Funktionen umwandeln Vorverarbeitung und Funktionserstellung Kombinierte Funktionen TensorFlow Transform Zusammenfassung Sie m\u00c3\u00b6chten erfahren, wie Sie die Genauigkeit Ihrer ML-Modelle verbessern oder wie Sie herausfinden, welche Datenspalten die n\u00c3\u00bctzlichsten Funktionen ergeben? Willkommen zum Feature Engineering. Wir er\u00c3\u00b6rtern in diesem Kurs n\u00c3\u00bctzliche und nutzlose Funktionen und wie Sie diese f\u00c3\u00bcr die optimale Nutzung in Ihren ML-Modellen vorverarbeiten und umwandeln. Feature Engineering ist h\u00c3\u00a4ufig die l\u00c3\u00a4ngste und schwierigste Phase bei der Erstellung eines ML-Projekts. Beim Feature Engineering erstellen Sie aus Ihren Rohdaten und mithilfe Ihres eigenen Domainwissens Funktionen zur Optimierung der ML-Algorithmen. Wir erkunden in diesem Modul, was gute Funktionen ausmacht und wie Sie diese in Ihrem ML-Modell darstellen k\u00c3\u00b6nnen. In diesem Abschnitt des Moduls er\u00c3\u00b6rtern wir die Vorverarbeitung und die Funktionserstellung. Diese Datenverarbeitungstechniken k\u00c3\u00b6nnen Ihnen die Vorbereitung eines Funktionssatzes f\u00c3\u00bcr ein maschinelles Lernsystem erleichtern.\n Beim herk\u00c3\u00b6mmlichen maschinellen Lernen spielen kombinierte Funktionen kaum eine Rolle. Bei modernen ML-Methoden sind sie f\u00c3\u00bcr Ihr Toolkit jedoch von enormem Wert. In diesem Modul erfahren Sie, wie Sie die Arten von Problemen erkennen, bei denen kombinierte Funktionen f\u00c3\u00bcr das maschinelle Lernen \u00c3\u00a4u\u00c3\u009ferst effektiv sind. TensorFlow Transform (tf.Transform) ist eine Bibliothek f\u00c3\u00bcr die Vorverarbeitung von Daten mit TensorFlow. tf.Transform eignet sich f\u00c3\u00bcr die Vorverarbeitung, wenn eine vollst\u00c3\u00a4ndige Daten\u00c3\u00bcbergabe erforderlich ist, wie etwa beim: \u00e2\u0080\u0093 Normalisieren eines Eingabewerts mit \"mean\" und \"stdev\" \u00e2\u0080\u0093 Integrieren eines Vokabulars durch die Suche nach Werten in allen Eingabebeispielen \u00e2\u0080\u0093 Zuordnen von Eingaben zu Buckets basierend auf der beobachteten Datenverteilung \nIn diesem Modul sehen wir uns verschiedene Anwendungsf\u00c3\u00a4lle f\u00c3\u00bcr tf.Transform an. Hier eine Zusammenfassung der wichtigsten Punkte dieses Moduls zum Feature Engineering: n\u00c3\u00bctzliche Funktionen ausw\u00c3\u00a4hlen, im gro\u00c3\u009fen Ma\u00c3\u009fstab vorverarbeiten, kombinierte Funktionen verwenden und mit TensorFlow \u00c3\u00bcben."]}